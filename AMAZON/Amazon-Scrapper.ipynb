{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f315f-5bea-4900-8013-f620160f11fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Job Scraper\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job title to search for (leave empty to get all jobs):  \n",
      "Maximum number of pages to scrape (default 20):  1\n",
      "Run in headless mode? (y/n, default: n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:35:34,840 - INFO - Starting Amazon job scraper at 2025-03-18 18:35:34\n",
      "2025-03-18 18:35:34,846 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting job scraper...\n",
      "This may take several minutes depending on the number of jobs and pages.\n",
      "Progress will be logged to the console and a log file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:35:35,170 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-18 18:35:35,262 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-18 18:35:35,327 - INFO - Driver [/Users/srikar/.wdm/drivers/chromedriver/mac64/134.0.6998.88/chromedriver-mac-arm64/chromedriver] found in cache\n",
      "2025-03-18 18:35:36,659 - INFO - Scraping jobs from Amazon\n",
      "2025-03-18 18:35:38,480 - INFO - Clicked popup/cookie button with xpath: //button[contains(@id, 'accept')]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.alert import Alert\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(f\"amazon_job_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to scroll and load all jobs with improved logic\n",
    "def scroll_to_load_all(driver, max_scrolls=30, wait_time=2):\n",
    "    \"\"\"\n",
    "    Scroll the page to load all content with a maximum number of scrolls\n",
    "    For Amazon's job site, which loads content dynamically\n",
    "    \"\"\"\n",
    "    scrolls = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    last_job_count = 0\n",
    "    consecutive_no_change = 0\n",
    "    \n",
    "    logger.info(\"Starting to scroll to load all content...\")\n",
    "    \n",
    "    while scrolls < max_scrolls:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(wait_time)  # Wait time for content to load\n",
    "        \n",
    "        # Take screenshot for debugging\n",
    "        driver.save_screenshot(f\"screenshots/amazon_scroll_{scrolls+1}.png\")\n",
    "        \n",
    "        # Try to find the \"Load More\" or similar buttons and click them\n",
    "        try:\n",
    "            load_more_buttons = driver.find_elements(By.XPATH, \n",
    "                \"//button[contains(text(), 'Load More') or contains(text(), 'Show More') or contains(@aria-label, 'Load') or contains(@class, 'load-more')]\")\n",
    "            if load_more_buttons:\n",
    "                for button in load_more_buttons:\n",
    "                    if button.is_displayed() and button.is_enabled():\n",
    "                        driver.execute_script(\"arguments[0].click();\", button)\n",
    "                        logger.info(\"Clicked 'Load More' button\")\n",
    "                        time.sleep(wait_time + 1)  # Extra wait for new content\n",
    "        except Exception as e:\n",
    "            logger.info(f\"No 'Load More' button found or error clicking it: {e}\")\n",
    "        \n",
    "        # Check height and job count to determine if we've loaded all content\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Try different job card selectors to get an accurate count\n",
    "        job_selectors = [\n",
    "            \".job-tile\", \n",
    "            \".job-listing\",\n",
    "            \".job-card\",\n",
    "            \"[data-job-id]\"\n",
    "        ]\n",
    "        \n",
    "        current_job_count = 0\n",
    "        for selector in job_selectors:\n",
    "            count = len(driver.find_elements(By.CSS_SELECTOR, selector))\n",
    "            if count > current_job_count:\n",
    "                current_job_count = count\n",
    "        \n",
    "        logger.info(f\"Scroll {scrolls+1}: Height {last_height} â†’ {new_height}, Jobs found: {current_job_count}\")\n",
    "        \n",
    "        # If no change in height and job count, we might have reached the end\n",
    "        if new_height == last_height and current_job_count == last_job_count:\n",
    "            consecutive_no_change += 1\n",
    "            logger.info(f\"No change detected ({consecutive_no_change}/3)\")\n",
    "            if consecutive_no_change >= 3:  # If no change for 3 consecutive scrolls\n",
    "                logger.info(\"No more content loading after multiple scrolls. Stopping scroll operation.\")\n",
    "                break\n",
    "        else:\n",
    "            consecutive_no_change = 0\n",
    "            \n",
    "        last_height = new_height\n",
    "        last_job_count = current_job_count\n",
    "        scrolls += 1\n",
    "    \n",
    "    logger.info(f\"Completed scrolling after {scrolls} scrolls. Found approximately {last_job_count} job items.\")\n",
    "    return last_job_count\n",
    "\n",
    "# Function to handle pagination for Amazon\n",
    "def handle_pagination(driver, max_pages=20):\n",
    "    \"\"\"\n",
    "    Handle pagination by clicking through all available pages\n",
    "    \"\"\"\n",
    "    page = 1\n",
    "    all_jobs = []\n",
    "    \n",
    "    logger.info(\"Starting pagination handling...\")\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        logger.info(f\"Processing page {page}\")\n",
    "        \n",
    "        # Take screenshot for debugging\n",
    "        driver.save_screenshot(f\"screenshots/amazon_page_{page}.png\")\n",
    "        \n",
    "        # Wait for job listings to be visible \n",
    "        wait_for_job_listings(driver)\n",
    "        \n",
    "        # Extract current page's jobs\n",
    "        jobs_on_page = extract_job_listings_amazon(driver)\n",
    "        all_jobs.extend(jobs_on_page)\n",
    "        logger.info(f\"Found {len(jobs_on_page)} jobs on page {page}\")\n",
    "        \n",
    "        # Look for next page button - try multiple selectors\n",
    "        next_selectors = [\n",
    "            \"a.next\", \n",
    "            \"[data-action='pagination-next']\",\n",
    "            \"button.next\",\n",
    "            \"//a[contains(@class, 'next')]\",\n",
    "            \"//a[contains(text(), 'Next')]\",\n",
    "            \"//button[contains(text(), 'Next')]\",\n",
    "            \"//span[contains(text(), 'Next')]/parent::a\",\n",
    "            \"//span[contains(text(), 'Next')]/parent::button\"\n",
    "        ]\n",
    "        \n",
    "        next_button = None\n",
    "        for selector in next_selectors:\n",
    "            try:\n",
    "                if selector.startswith(\"//\"):\n",
    "                    elements = driver.find_elements(By.XPATH, selector)\n",
    "                else:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                \n",
    "                for elem in elements:\n",
    "                    if elem.is_displayed() and not (elem.get_attribute(\"disabled\") or \"disabled\" in elem.get_attribute(\"class\") or \"inactive\" in elem.get_attribute(\"class\")):\n",
    "                        next_button = elem\n",
    "                        break\n",
    "                        \n",
    "                if next_button:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if not next_button:\n",
    "            logger.info(\"No next page button found. Reached last page.\")\n",
    "            break\n",
    "            \n",
    "        # Check if button is disabled (end of pages)\n",
    "        if next_button.get_attribute(\"disabled\") or \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "            logger.info(\"Reached last page - Next button is disabled\")\n",
    "            break\n",
    "            \n",
    "        # Click next page using JavaScript to avoid intercept issues\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            logger.info(\"Clicked next page button\")\n",
    "            time.sleep(5)  # Wait for page to load\n",
    "            page += 1\n",
    "            \n",
    "            # Wait for job listings to reload\n",
    "            wait_for_job_listings(driver)\n",
    "            \n",
    "            # Take screenshot after page change\n",
    "            driver.save_screenshot(f\"screenshots/amazon_page_{page}_loaded.png\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error clicking next page: {e}\")\n",
    "            # Try one more time with a different approach\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                time.sleep(1)\n",
    "                next_button.click()\n",
    "                logger.info(\"Clicked next page button using alternative method\")\n",
    "                time.sleep(5)\n",
    "                page += 1\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Still failed to click next page: {e2}\")\n",
    "                break\n",
    "            \n",
    "    return all_jobs\n",
    "\n",
    "# Helper function to wait for job listings to appear\n",
    "def wait_for_job_listings(driver, timeout=15):\n",
    "    \"\"\"Wait for job listings to appear on the page using multiple possible selectors\"\"\"\n",
    "    selectors = [\n",
    "        \".job-tile\",\n",
    "        \".job-listing\",\n",
    "        \".job-card\",\n",
    "        \"[data-job-id]\",\n",
    "        \".job-title\"\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        try:\n",
    "            WebDriverWait(driver, timeout).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            logger.info(f\"Job listings found with selector: {selector}\")\n",
    "            return True\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "    \n",
    "    logger.warning(\"Could not detect job listings with any known selector\")\n",
    "    return False\n",
    "\n",
    "# Extract job information from Amazon page\n",
    "def extract_job_listings_amazon(driver):\n",
    "    \"\"\"\n",
    "    Extract job listings from Amazon jobs page\n",
    "    Handles different potential Amazon UI structures\n",
    "    \"\"\"\n",
    "    jobs_data = []\n",
    "    \n",
    "    # Try different selectors for job elements \n",
    "    selectors = [\n",
    "        {\"container\": \".job-tile\", \"type\": \"tile\"},\n",
    "        {\"container\": \".job-listing\", \"type\": \"listing\"},\n",
    "        {\"container\": \".job-card\", \"type\": \"card\"},\n",
    "        {\"container\": \"[data-job-id]\", \"type\": \"data-id\"}\n",
    "    ]\n",
    "    \n",
    "    job_elements = []\n",
    "    used_selector = None\n",
    "    \n",
    "    # Try each selector to find job elements\n",
    "    for selector in selectors:\n",
    "        try:\n",
    "            elements = driver.find_elements(By.CSS_SELECTOR, selector[\"container\"])\n",
    "            if elements and len(elements) > 0:\n",
    "                job_elements = elements\n",
    "                used_selector = selector\n",
    "                logger.info(f\"Found {len(elements)} job elements using selector: {selector['container']}\")\n",
    "                \n",
    "                # Take a screenshot of the found elements (for debugging)\n",
    "                if len(elements) > 0:\n",
    "                    try:\n",
    "                        driver.execute_script(\"arguments[0].style.border='3px solid red'\", elements[0])\n",
    "                        driver.save_screenshot(\"screenshots/amazon_job_elements_found.png\")\n",
    "                        driver.execute_script(\"arguments[0].style.border=''\", elements[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Selector {selector['container']} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not job_elements:\n",
    "        logger.warning(\"Could not find job elements with any selector. Taking screenshot for debugging.\")\n",
    "        driver.save_screenshot(\"screenshots/amazon_no_job_elements.png\")\n",
    "        return jobs_data\n",
    "    \n",
    "    # Extract detailed information for each job based on the selector type\n",
    "    for index, job in enumerate(job_elements):\n",
    "        try:\n",
    "            # Use different extraction strategies based on container type\n",
    "            if used_selector[\"type\"] in [\"tile\", \"listing\", \"card\"]:\n",
    "                # For standard job tiles\n",
    "                title_elem = job.find_element(By.CSS_SELECTOR, \".job-title, h2, h3, [data-job-title]\")\n",
    "                title = title_elem.text.strip()\n",
    "                \n",
    "                # Get link - try different approaches\n",
    "                link = None\n",
    "                link_elements = job.find_elements(By.TAG_NAME, \"a\")\n",
    "                if link_elements:\n",
    "                    for link_elem in link_elements:\n",
    "                        href = link_elem.get_attribute(\"href\")\n",
    "                        if href and (\"/jobs/\" in href or \"/job/\" in href):\n",
    "                            link = href\n",
    "                            break\n",
    "                \n",
    "                # If still no link, try to find it differently\n",
    "                if not link:\n",
    "                    try:\n",
    "                        # Try to find link in parent element\n",
    "                        parent = job\n",
    "                        for _ in range(3):  # Try up to 3 levels up\n",
    "                            parent = parent.find_element(By.XPATH, \"..\")\n",
    "                            link_elems = parent.find_elements(By.TAG_NAME, \"a\")\n",
    "                            for link_elem in link_elems:\n",
    "                                href = link_elem.get_attribute(\"href\")\n",
    "                                if href and (\"/jobs/\" in href or \"/job/\" in href):\n",
    "                                    link = href\n",
    "                                    break\n",
    "                            if link:\n",
    "                                break\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Get location\n",
    "                location = \"Not specified\"\n",
    "                location_selectors = [\n",
    "                    \".location-and-id\", \n",
    "                    \".location\", \n",
    "                    \"[data-job-location]\",\n",
    "                    \".job-meta\"\n",
    "                ]\n",
    "                \n",
    "                for loc_selector in location_selectors:\n",
    "                    try:\n",
    "                        loc_elem = job.find_element(By.CSS_SELECTOR, loc_selector)\n",
    "                        location_text = loc_elem.text.strip()\n",
    "                        if location_text:\n",
    "                            location = location_text\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "            elif used_selector[\"type\"] == \"data-id\":\n",
    "                # For jobs with data-job-id attribute\n",
    "                title = \"Unknown Title\"\n",
    "                link = None\n",
    "                location = \"Not specified\"\n",
    "                \n",
    "                try:\n",
    "                    # Try to find title\n",
    "                    title_elements = job.find_elements(By.CSS_SELECTOR, \n",
    "                        \"h2, h3, .title, [data-job-title], a\")\n",
    "                    for elem in title_elements:\n",
    "                        text = elem.text.strip()\n",
    "                        if text and len(text) > 3:  # Make sure it's substantial text\n",
    "                            title = text\n",
    "                            # If it's an anchor tag, also get the link\n",
    "                            if elem.tag_name == \"a\":\n",
    "                                link = elem.get_attribute(\"href\")\n",
    "                            break\n",
    "                    \n",
    "                    # If still no link, try to find separately\n",
    "                    if not link:\n",
    "                        link_elements = job.find_elements(By.TAG_NAME, \"a\")\n",
    "                        for link_elem in link_elements:\n",
    "                            href = link_elem.get_attribute(\"href\")\n",
    "                            if href and (\"/jobs/\" in href or \"/job/\" in href):\n",
    "                                link = href\n",
    "                                break\n",
    "                    \n",
    "                    # Try to find location\n",
    "                    location_elements = job.find_elements(By.CSS_SELECTOR, \n",
    "                        \".location, [data-location], span, div\")\n",
    "                    for elem in location_elements:\n",
    "                        text = elem.text.strip()\n",
    "                        if text and (\",\" in text or \"Remote\" in text):\n",
    "                            location = text\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error extracting data-id job info: {e}\")\n",
    "            \n",
    "            # Use job ID as fallback if no link found\n",
    "            if not link:\n",
    "                job_id = job.get_attribute(\"data-job-id\")\n",
    "                if job_id:\n",
    "                    link = f\"https://www.amazon.jobs/en/jobs/{job_id}\"\n",
    "                    logger.info(f\"Created link from job ID: {link}\")\n",
    "            \n",
    "            # Add the extracted job if we have at least a title and link\n",
    "            if title and title.strip() and link and link.strip():\n",
    "                # Check for duplicate before adding\n",
    "                is_duplicate = False\n",
    "                for existing_job in jobs_data:\n",
    "                    if existing_job[\"Title\"] == title and existing_job[\"Link\"] == link:\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                \n",
    "                if not is_duplicate:\n",
    "                    jobs_data.append({\n",
    "                        \"Title\": title,\n",
    "                        \"Location\": location,\n",
    "                        \"Link\": link,\n",
    "                        \"Description\": \"\",  # We'll get descriptions in a separate step\n",
    "                        \"Company\": \"Amazon\"\n",
    "                    })\n",
    "                    logger.info(f\"Added job: {title} at {location}\")\n",
    "            \n",
    "        except (StaleElementReferenceException, Exception) as e:\n",
    "            logger.error(f\"Error extracting job details for job {index+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return jobs_data\n",
    "\n",
    "# Function to get job descriptions\n",
    "def get_job_descriptions(driver, jobs_data, max_descriptions=100):\n",
    "    \"\"\"\n",
    "    Get job descriptions for a batch of jobs by visiting their individual pages.\n",
    "    Save full page screenshots of each job description in a separate folder.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Getting descriptions for {len(jobs_data)} jobs (up to {max_descriptions})\")\n",
    "    \n",
    "    # Create a dedicated folder for job description screenshots\n",
    "    job_desc_folder = 'amazon_job_description_screenshots'\n",
    "    os.makedirs(job_desc_folder, exist_ok=True)\n",
    "    \n",
    "    # Store current URL to return to afterward\n",
    "    original_url = driver.current_url\n",
    "    original_window = driver.current_window_handle\n",
    "    \n",
    "    # Process up to max_descriptions\n",
    "    for i, job in enumerate(jobs_data[:max_descriptions]):\n",
    "        if not job.get(\"Link\"):\n",
    "            continue\n",
    "            \n",
    "        logger.info(f\"Getting description for job {i+1}/{min(len(jobs_data), max_descriptions)}: {job['Title']}\")\n",
    "        \n",
    "        # Create a clean filename from the job title\n",
    "        clean_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", job['Title'])\n",
    "        clean_title = re.sub(r'\\s+', \"_\", clean_title)\n",
    "        clean_title = clean_title[:100] if len(clean_title) > 100 else clean_title\n",
    "        \n",
    "        # Create a new tab for each job\n",
    "        try:\n",
    "            # Open new tab\n",
    "            driver.execute_script(\"window.open('about:blank', '_blank');\")\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            \n",
    "            # Navigate to job details page\n",
    "            driver.get(job[\"Link\"])\n",
    "            time.sleep(5)  # Wait for page to load\n",
    "            \n",
    "            # Take a full page screenshot\n",
    "            # First, get the height of the entire page\n",
    "            total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            # Set window size to capture entire page\n",
    "            driver.set_window_size(1920, total_height)\n",
    "            \n",
    "            # Take screenshot and save in the dedicated folder\n",
    "            screenshot_file = f\"{job_desc_folder}/amazon_job_{i+1}_{clean_title}.png\"\n",
    "            driver.save_screenshot(screenshot_file)\n",
    "            logger.info(f\"Saved full-page screenshot to {screenshot_file}\")\n",
    "            \n",
    "            # Extract description using multiple potential selectors\n",
    "            description_selectors = [\n",
    "                \"#job-detail\",\n",
    "                \".job-description\",\n",
    "                \"#description\",\n",
    "                \".description\",\n",
    "                \".details-container\",\n",
    "                \"#job-requirements\", \n",
    "                \"[data-job-description]\",\n",
    "                \"article\"\n",
    "            ]\n",
    "            \n",
    "            description = \"\"\n",
    "            for selector in description_selectors:\n",
    "                try:\n",
    "                    desc_elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if desc_elements:\n",
    "                        description = desc_elements[0].text.strip()\n",
    "                        if description:\n",
    "                            logger.info(f\"Got description for '{job['Title']}' ({len(description)} chars)\")\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Selector {selector} failed: {e}\")\n",
    "            \n",
    "            # Update the job with the description\n",
    "            if description:\n",
    "                job[\"Description\"] = description\n",
    "            \n",
    "            # Close tab\n",
    "            driver.close()\n",
    "            driver.switch_to.window(original_window)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting description for {job['Title']}: {e}\")\n",
    "            # Make sure we're back to the original window\n",
    "            try:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(original_window)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Return to original page\n",
    "    try:\n",
    "        driver.get(original_url)\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    logger.info(f\"Completed fetching descriptions for {min(len(jobs_data), max_descriptions)} jobs\")\n",
    "    return jobs_data\n",
    "\n",
    "# Function to handle different types of popups\n",
    "def handle_popups(driver):\n",
    "    try:\n",
    "        # Common buttons for accepting cookies, terms, etc.\n",
    "        popup_selectors = [\n",
    "            \"//button[contains(text(), 'Accept')]\", \n",
    "            \"//button[contains(text(), 'I agree')]\",\n",
    "            \"//button[contains(@id, 'accept')]\",\n",
    "            \"//button[contains(@class, 'accept')]\",\n",
    "            \"//button[contains(text(), 'Continue')]\",\n",
    "            \"//button[contains(text(), 'Got it')]\",\n",
    "            \"//button[contains(text(), 'Close')]\",\n",
    "            \"//button[@aria-label='Close']\",\n",
    "            \"//div[contains(@class, 'cookie')]//button\",\n",
    "            \"//div[contains(@id, 'consent')]//button\"\n",
    "        ]\n",
    "        \n",
    "        for xpath in popup_selectors:\n",
    "            try:\n",
    "                buttons = driver.find_elements(By.XPATH, xpath)\n",
    "                for button in buttons:\n",
    "                    if button.is_displayed():\n",
    "                        button.click()\n",
    "                        logger.info(f\"Clicked popup/cookie button with xpath: {xpath}\")\n",
    "                        time.sleep(1)\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "        # Handle alerts\n",
    "        try:\n",
    "            alert = Alert(driver)\n",
    "            alert.accept()\n",
    "            logger.info(\"Accepted alert popup\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error handling popups: {e}\")\n",
    "\n",
    "# Function to validate URL\n",
    "def is_valid_link(url):\n",
    "    if not url or not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        return False\n",
    "    \n",
    "    # For Amazon links, we'll assume they're valid without checking\n",
    "    if \"amazon.jobs\" in url:\n",
    "        return True\n",
    "        \n",
    "    try:\n",
    "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
    "        return response.status_code < 400  # Accept any non-error status\n",
    "    except requests.RequestException:\n",
    "        logger.warning(f\"Invalid link: {url}\")\n",
    "        return False\n",
    "\n",
    "# Main Amazon job scraper function\n",
    "def scrape_amazon_jobs(search_keyword=\"\", max_pages=20, headless=False):\n",
    "    \"\"\"\n",
    "    Scrape Amazon jobs with comprehensive approach including pagination.\n",
    "    \n",
    "    Parameters:\n",
    "    search_keyword (str): Keyword to search for (empty string for all jobs)\n",
    "    max_pages (int): Maximum number of pages to scrape\n",
    "    headless (bool): Whether to run in headless mode\n",
    "    \n",
    "    Returns:\n",
    "    list: List of all job dictionaries found\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "        \n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36')\n",
    "    \n",
    "    # Create directories for debugging\n",
    "    os.makedirs('screenshots', exist_ok=True)\n",
    "    \n",
    "    driver = None\n",
    "    jobs_data = []\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        \n",
    "        # Construct search URL\n",
    "        search_term = search_keyword.replace(\" \", \"+\") if search_keyword else \"\"\n",
    "        if search_term:\n",
    "            base_url = f\"https://www.amazon.jobs/en/search?base_query={search_term}\"\n",
    "        else:\n",
    "            base_url = \"https://www.amazon.jobs/en/\"\n",
    "\n",
    "        # Open the Amazon careers page\n",
    "        logger.info(f\"Scraping jobs from Amazon\" + (f\", searching for '{search_keyword}'\" if search_keyword else \"\"))\n",
    "        driver.get(base_url)\n",
    "        driver.save_screenshot(\"screenshots/amazon_initial.png\")\n",
    "        \n",
    "        # Handle popups\n",
    "        handle_popups(driver)\n",
    "        \n",
    "        # Wait for results to load\n",
    "        if not wait_for_job_listings(driver, timeout=20):\n",
    "            logger.warning(\"Could not detect job search results loading. Trying manual search...\")\n",
    "            \n",
    "            # Try to search directly by submitting a search form\n",
    "            try:\n",
    "                search_box = driver.find_element(By.CSS_SELECTOR, \"input[type='search'], input[name='q'], input[name='base_query']\")\n",
    "                search_box.clear()\n",
    "                search_box.send_keys(search_keyword)\n",
    "                search_box.send_keys(Keys.RETURN)\n",
    "                time.sleep(5)\n",
    "                logger.info(\"Tried manual search submission\")\n",
    "                driver.save_screenshot(\"screenshots/amazon_manual_search.png\")\n",
    "                \n",
    "                # Wait again for results\n",
    "                wait_for_job_listings(driver, timeout=15)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Manual search also failed: {e}\")\n",
    "        \n",
    "        # Scroll to load all results on first page\n",
    "        job_count = scroll_to_load_all(driver, max_scrolls=20, wait_time=3)\n",
    "        driver.save_screenshot(\"screenshots/amazon_after_scroll.png\")\n",
    "        \n",
    "        # Handle pagination and collect all jobs\n",
    "        all_jobs = handle_pagination(driver, max_pages=max_pages)\n",
    "        logger.info(f\"Collected {len(all_jobs)} total jobs after pagination\")\n",
    "        \n",
    "        # Get job descriptions for all collected jobs\n",
    "        jobs_with_descriptions = get_job_descriptions(driver, all_jobs, max_descriptions=200)\n",
    "        \n",
    "        # Save all jobs regardless of search keyword\n",
    "        jobs_data = jobs_with_descriptions\n",
    "        logger.info(f\"Saving all {len(jobs_data)} jobs found in search results\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping from Amazon: {e}\")\n",
    "        if driver:\n",
    "            driver.save_screenshot(\"screenshots/amazon_error.png\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "    return jobs_data\n",
    "\n",
    "# Main function to scrape and save results to CSV\n",
    "def main(search_keyword=\"\", max_pages=20, headless=False):\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting Amazon job scraper at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Create screenshots directory\n",
    "    os.makedirs('screenshots', exist_ok=True)\n",
    "    \n",
    "    # Create job description screenshots directory\n",
    "    os.makedirs('amazon_job_description_screenshots', exist_ok=True)\n",
    "    \n",
    "    # Scrape Amazon jobs\n",
    "    jobs_data = scrape_amazon_jobs(search_keyword, max_pages, headless)\n",
    "    \n",
    "    # Generate filenames with timestamps\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    keyword_slug = search_keyword.replace(' ', '_') if search_keyword else 'all'\n",
    "    detailed_filename = f\"amazon_jobs_detailed_{keyword_slug}_{timestamp}.csv\"\n",
    "    simple_filename = f\"amazon_jobs_simple_{keyword_slug}_{timestamp}.csv\"\n",
    "    \n",
    "    # Save results\n",
    "    if jobs_data:\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(jobs_data)\n",
    "        \n",
    "        # Save detailed CSV with descriptions\n",
    "        df.to_csv(detailed_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Detailed jobs saved to '{detailed_filename}'\")\n",
    "        \n",
    "        # Create and save a simplified CSV without descriptions\n",
    "        simple_df = df[['Title', 'Location', 'Link', 'Company']].copy()\n",
    "        simple_df.to_csv(simple_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Simplified jobs list saved to '{simple_filename}'\")\n",
    "        \n",
    "        # Also create a filtered file if a search keyword was provided\n",
    "        if search_keyword:\n",
    "            filtered_df = df[df['Title'].str.contains(search_keyword, case=False)].copy()\n",
    "            filtered_filename = f\"amazon_jobs_filtered_{keyword_slug}_{timestamp}.csv\"\n",
    "            filtered_df.to_csv(filtered_filename, index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Jobs filtered by keyword '{search_keyword}' saved to '{filtered_filename}'\")\n",
    "            logger.info(f\"Found {len(filtered_df)} jobs matching the keyword out of {len(df)} total jobs\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Amazon Job Scraping Results:\")\n",
    "        print(f\"Total jobs found: {len(df)}\")\n",
    "        print(f\"Unique locations: {len(df['Location'].unique())}\")\n",
    "        print(f\"Sample jobs:\")\n",
    "        print(df[['Title', 'Location']].head())\n",
    "        print(\"\\nTop locations:\")\n",
    "        print(df['Location'].value_counts().head())\n",
    "        print(f\"\\nResults saved to:\")\n",
    "        print(f\"- {detailed_filename}\")\n",
    "        print(f\"- {simple_filename}\")\n",
    "        if search_keyword:\n",
    "            print(f\"- {filtered_filename}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        logger.warning(f\"No jobs found with search keyword '{search_keyword}'\")\n",
    "        print(\"\\nNo jobs found to display.\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Process completed with no results in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Amazon Job Scraper\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get user input\n",
    "    job_title = input(\"Enter job title to search for (leave empty to get all jobs): \").strip()\n",
    "    \n",
    "    try:\n",
    "        max_pages = int(input(\"Maximum number of pages to scrape (default 20): \") or \"20\")\n",
    "    except ValueError:\n",
    "        max_pages = 20\n",
    "        print(\"Invalid input. Using default of 20 pages.\")\n",
    "    \n",
    "    headless_mode = input(\"Run in headless mode? (y/n, default: n): \").strip().lower() == 'y'\n",
    "    \n",
    "    print(\"\\nStarting job scraper...\")\n",
    "    print(\"This may take several minutes depending on the number of jobs and pages.\")\n",
    "    print(\"Progress will be logged to the console and a log file.\")\n",
    "    \n",
    "    # Run the scraper\n",
    "    main(job_title, max_pages, headless_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13316404-cc54-47f8-b837-b699f3bd4fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
