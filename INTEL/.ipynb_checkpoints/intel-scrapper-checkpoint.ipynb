{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f89adf-17ed-4267-8383-52abf4d75e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel Job Scraper\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job title to search for (leave empty to get all jobs):  \n",
      "Maximum number of pages to scrape (default 10):  1\n",
      "Run in headless mode? (y/n, default: n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 23:13:21,425 - INFO - Starting Intel job scraper at 2025-03-18 23:13:21\n",
      "2025-03-18 23:13:21,428 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting job scraper...\n",
      "This may take several minutes depending on the number of jobs and pages.\n",
      "Progress will be logged to the console and a log file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 23:13:22,156 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-18 23:13:22,329 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-18 23:13:22,381 - INFO - Driver [/Users/srikar/.wdm/drivers/chromedriver/mac64/134.0.6998.88/chromedriver-mac-arm64/chromedriver] found in cache\n",
      "2025-03-18 23:13:23,784 - INFO - Scraping jobs from Intel\n",
      "2025-03-18 23:13:26,385 - INFO - Job listings found with selector: [data-automation-id='jobTitle']\n",
      "2025-03-18 23:13:26,393 - INFO - Starting to scroll to load all content...\n",
      "2025-03-18 23:13:29,894 - INFO - Scroll 1: Height 4396 → 4396, Jobs found: 20\n",
      "2025-03-18 23:13:33,379 - INFO - Scroll 2: Height 4396 → 4396, Jobs found: 20\n",
      "2025-03-18 23:13:33,381 - INFO - No change detected (1/3)\n",
      "2025-03-18 23:13:36,856 - INFO - Scroll 3: Height 4396 → 4396, Jobs found: 20\n",
      "2025-03-18 23:13:36,856 - INFO - No change detected (2/3)\n",
      "2025-03-18 23:13:40,307 - INFO - Scroll 4: Height 4396 → 4396, Jobs found: 20\n",
      "2025-03-18 23:13:40,308 - INFO - No change detected (3/3)\n",
      "2025-03-18 23:13:40,309 - INFO - No more content loading after multiple scrolls. Stopping scroll operation.\n",
      "2025-03-18 23:13:40,309 - INFO - Completed scrolling after 3 scrolls. Found approximately 20 job items.\n",
      "2025-03-18 23:13:40,709 - INFO - Starting pagination handling...\n",
      "2025-03-18 23:13:40,709 - INFO - Processing page 1\n",
      "2025-03-18 23:13:41,108 - INFO - Job listings found with selector: [data-automation-id='jobTitle']\n",
      "2025-03-18 23:13:41,113 - INFO - Found 20 job elements using selector: [data-automation-id='jobTitle']\n",
      "2025-03-18 23:13:41,594 - INFO - Added job: PDK Development Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:41,630 - INFO - Added job: Platform Validation Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:41,664 - INFO - Added job: Graduate Talent (ISCP SV ISSV) at Malaysia, Penang\n",
      "2025-03-18 23:13:41,705 - INFO - Added job: SOC Design Verification Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:41,739 - INFO - Added job: Real Estate and Facilities Expense Analyst at Malaysia, Penang\n",
      "2025-03-18 23:13:41,773 - INFO - Added job: Intern Hardware Verification Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:41,807 - INFO - Added job: Data Center Pre-Si FPGA Development Intern at Malaysia, Penang\n",
      "2025-03-18 23:13:41,841 - INFO - Added job: Pre-Silicon Validation Student Worker at Malaysia, Penang\n",
      "2025-03-18 23:13:41,876 - INFO - Added job: Firmware Development Engineering Manager at Malaysia, Penang\n",
      "2025-03-18 23:13:41,910 - INFO - Added job: Firmware Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:41,945 - INFO - Added job: E-Core Senior Staff Design Verification Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:41,980 - INFO - Added job: E-Core Staff Design Verification Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:42,014 - INFO - Added job: Post-Silicon Validation Expert – Power Management at Malaysia, Penang\n",
      "2025-03-18 23:13:42,047 - INFO - Added job: GPU Modelling Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:42,080 - INFO - Added job: GPU Modelling Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:42,113 - INFO - Added job: Senior System Integration and Validation Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:42,147 - INFO - Added job: Tactical Planner Student at Malaysia, Penang\n",
      "2025-03-18 23:13:42,181 - INFO - Added job: GPU HW IP Formal Verification Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:42,215 - INFO - Added job: GPU HW IP Functional Verification Engineer at Malaysia, Penang\n",
      "2025-03-18 23:13:42,250 - INFO - Added job: GPU HW IP Design and Verification Engineer/GPU Architect at Malaysia, Penang\n",
      "2025-03-18 23:13:42,250 - INFO - Found 20 jobs on page 1\n",
      "2025-03-18 23:13:42,344 - INFO - Clicked next page button\n",
      "2025-03-18 23:13:47,361 - INFO - Job listings found with selector: [data-automation-id='jobTitle']\n",
      "2025-03-18 23:13:47,825 - INFO - Collected 20 total jobs after pagination\n",
      "2025-03-18 23:13:47,825 - INFO - Getting descriptions for 20 jobs (up to 200)\n",
      "2025-03-18 23:13:47,830 - INFO - Getting description for job 1/20: PDK Development Engineer\n",
      "2025-03-18 23:13:54,960 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_1_PDK_Development_Engineer.png\n",
      "2025-03-18 23:14:03,986 - INFO - Getting description for job 2/20: Platform Validation Engineer\n",
      "2025-03-18 23:14:10,928 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_2_Platform_Validation_Engineer.png\n",
      "2025-03-18 23:14:19,459 - INFO - Getting description for job 3/20: Graduate Talent (ISCP SV ISSV)\n",
      "2025-03-18 23:14:26,377 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_3_Graduate_Talent_(ISCP_SV_ISSV).png\n",
      "2025-03-18 23:14:34,608 - INFO - Getting description for job 4/20: SOC Design Verification Engineer\n",
      "2025-03-18 23:14:41,794 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_4_SOC_Design_Verification_Engineer.png\n",
      "2025-03-18 23:14:49,947 - INFO - Getting description for job 5/20: Real Estate and Facilities Expense Analyst\n",
      "2025-03-18 23:14:56,803 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_5_Real_Estate_and_Facilities_Expense_Analyst.png\n",
      "2025-03-18 23:15:02,745 - INFO - Getting description for job 6/20: Intern Hardware Verification Engineer\n",
      "2025-03-18 23:15:09,814 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_6_Intern_Hardware_Verification_Engineer.png\n",
      "2025-03-18 23:15:18,496 - INFO - Getting description for job 7/20: Data Center Pre-Si FPGA Development Intern\n",
      "2025-03-18 23:15:25,566 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_7_Data_Center_Pre-Si_FPGA_Development_Intern.png\n",
      "2025-03-18 23:15:33,954 - INFO - Getting description for job 8/20: Pre-Silicon Validation Student Worker\n",
      "2025-03-18 23:15:40,827 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_8_Pre-Silicon_Validation_Student_Worker.png\n",
      "2025-03-18 23:15:48,855 - INFO - Getting description for job 9/20: Firmware Development Engineering Manager\n",
      "2025-03-18 23:15:56,070 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_9_Firmware_Development_Engineering_Manager.png\n",
      "2025-03-18 23:16:05,234 - INFO - Getting description for job 10/20: Firmware Engineer\n",
      "2025-03-18 23:16:12,337 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_10_Firmware_Engineer.png\n",
      "2025-03-18 23:16:21,023 - INFO - Getting description for job 11/20: E-Core Senior Staff Design Verification Engineer\n",
      "2025-03-18 23:16:28,185 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_11_E-Core_Senior_Staff_Design_Verification_Engineer.png\n",
      "2025-03-18 23:16:37,061 - INFO - Getting description for job 12/20: E-Core Staff Design Verification Engineer\n",
      "2025-03-18 23:16:43,990 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_12_E-Core_Staff_Design_Verification_Engineer.png\n",
      "2025-03-18 23:16:52,225 - INFO - Getting description for job 13/20: Post-Silicon Validation Expert – Power Management\n",
      "2025-03-18 23:16:59,149 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_13_Post-Silicon_Validation_Expert_–_Power_Management.png\n",
      "2025-03-18 23:17:07,208 - INFO - Getting description for job 14/20: GPU Modelling Engineer\n",
      "2025-03-18 23:17:13,975 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_14_GPU_Modelling_Engineer.png\n",
      "2025-03-18 23:17:19,838 - INFO - Getting description for job 15/20: GPU Modelling Engineer\n",
      "2025-03-18 23:17:26,704 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_15_GPU_Modelling_Engineer.png\n",
      "2025-03-18 23:17:32,647 - INFO - Getting description for job 16/20: Senior System Integration and Validation Engineer\n",
      "2025-03-18 23:17:39,399 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_16_Senior_System_Integration_and_Validation_Engineer.png\n",
      "2025-03-18 23:17:45,141 - INFO - Getting description for job 17/20: Tactical Planner Student\n",
      "2025-03-18 23:17:51,869 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_17_Tactical_Planner_Student.png\n",
      "2025-03-18 23:17:57,429 - INFO - Getting description for job 18/20: GPU HW IP Formal Verification Engineer\n",
      "2025-03-18 23:18:04,836 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_18_GPU_HW_IP_Formal_Verification_Engineer.png\n",
      "2025-03-18 23:18:14,764 - INFO - Getting description for job 19/20: GPU HW IP Functional Verification Engineer\n",
      "2025-03-18 23:18:22,251 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_19_GPU_HW_IP_Functional_Verification_Engineer.png\n",
      "2025-03-18 23:18:34,918 - INFO - Getting description for job 20/20: GPU HW IP Design and Verification Engineer/GPU Architect\n",
      "2025-03-18 23:18:42,347 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_20_GPU_HW_IP_Design_and_Verification_EngineerGPU_Architect.png\n",
      "2025-03-18 23:18:55,770 - INFO - Completed fetching descriptions for 20 jobs\n",
      "2025-03-18 23:18:55,774 - INFO - Saving all 20 jobs found in search results\n",
      "2025-03-18 23:18:56,012 - INFO - Detailed jobs saved to 'intel_jobs_detailed_all_20250318_231855.csv'\n",
      "2025-03-18 23:18:56,020 - INFO - Simplified jobs list saved to 'intel_jobs_simple_all_20250318_231855.csv'\n",
      "2025-03-18 23:18:56,026 - INFO - Completed in 334.60 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Intel Job Scraping Results:\n",
      "Total jobs found: 20\n",
      "Unique locations: 1\n",
      "Sample jobs:\n",
      "                                        Title          Location\n",
      "0                    PDK Development Engineer  Malaysia, Penang\n",
      "1                Platform Validation Engineer  Malaysia, Penang\n",
      "2              Graduate Talent (ISCP SV ISSV)  Malaysia, Penang\n",
      "3            SOC Design Verification Engineer  Malaysia, Penang\n",
      "4  Real Estate and Facilities Expense Analyst  Malaysia, Penang\n",
      "\n",
      "Top locations:\n",
      "Location\n",
      "Malaysia, Penang    20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Results saved to:\n",
      "- intel_jobs_detailed_all_20250318_231855.csv\n",
      "- intel_jobs_simple_all_20250318_231855.csv\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.alert import Alert\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(f\"intel_job_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to scroll and load all jobs with improved logic\n",
    "def scroll_to_load_all(driver, max_scrolls=30, wait_time=2):\n",
    "    \"\"\"\n",
    "    Scroll the page to load all content with a maximum number of scrolls\n",
    "    For Intel's Workday-based site, which loads content dynamically\n",
    "    \"\"\"\n",
    "    scrolls = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    last_job_count = 0\n",
    "    consecutive_no_change = 0\n",
    "    \n",
    "    logger.info(\"Starting to scroll to load all content...\")\n",
    "    \n",
    "    while scrolls < max_scrolls:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(wait_time)  # Wait time for content to load\n",
    "        \n",
    "        # Take screenshot for debugging\n",
    "        driver.save_screenshot(f\"screenshots/intel_scroll_{scrolls+1}.png\")\n",
    "        \n",
    "        # Try to find the \"Load More\" or similar buttons and click them\n",
    "        try:\n",
    "            load_more_buttons = driver.find_elements(By.XPATH, \n",
    "                \"//button[contains(text(), 'Load More') or contains(text(), 'Show More') or contains(@aria-label, 'Load') or contains(@class, 'load-more')]\")\n",
    "            if load_more_buttons:\n",
    "                for button in load_more_buttons:\n",
    "                    if button.is_displayed() and button.is_enabled():\n",
    "                        driver.execute_script(\"arguments[0].click();\", button)\n",
    "                        logger.info(\"Clicked 'Load More' button\")\n",
    "                        time.sleep(wait_time + 1)  # Extra wait for new content\n",
    "        except Exception as e:\n",
    "            logger.info(f\"No 'Load More' button found or error clicking it: {e}\")\n",
    "        \n",
    "        # Check height and job count to determine if we've loaded all content\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Try different job card selectors to get an accurate count\n",
    "        job_selectors = [\n",
    "            \"[data-automation-id='jobTitle']\", \n",
    "            \".WGDC\", \n",
    "            \"[role='listitem']\",\n",
    "            \".css-19uc56f\",\n",
    "            \".css-1q6t3sv div[role='row']\"\n",
    "        ]\n",
    "        \n",
    "        current_job_count = 0\n",
    "        for selector in job_selectors:\n",
    "            count = len(driver.find_elements(By.CSS_SELECTOR, selector))\n",
    "            if count > current_job_count:\n",
    "                current_job_count = count\n",
    "        \n",
    "        logger.info(f\"Scroll {scrolls+1}: Height {last_height} → {new_height}, Jobs found: {current_job_count}\")\n",
    "        \n",
    "        # If no change in height and job count, we might have reached the end\n",
    "        if new_height == last_height and current_job_count == last_job_count:\n",
    "            consecutive_no_change += 1\n",
    "            logger.info(f\"No change detected ({consecutive_no_change}/3)\")\n",
    "            if consecutive_no_change >= 3:  # If no change for 3 consecutive scrolls\n",
    "                logger.info(\"No more content loading after multiple scrolls. Stopping scroll operation.\")\n",
    "                break\n",
    "        else:\n",
    "            consecutive_no_change = 0\n",
    "            \n",
    "        last_height = new_height\n",
    "        last_job_count = current_job_count\n",
    "        scrolls += 1\n",
    "    \n",
    "    logger.info(f\"Completed scrolling after {scrolls} scrolls. Found approximately {last_job_count} job items.\")\n",
    "    return last_job_count\n",
    "\n",
    "# Function to handle pagination for Intel Workday\n",
    "def handle_pagination(driver, max_pages=20):\n",
    "    \"\"\"\n",
    "    Handle pagination by clicking through all available pages\n",
    "    \"\"\"\n",
    "    page = 1\n",
    "    all_jobs = []\n",
    "    \n",
    "    logger.info(\"Starting pagination handling...\")\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        logger.info(f\"Processing page {page}\")\n",
    "        \n",
    "        # Take screenshot for debugging\n",
    "        driver.save_screenshot(f\"screenshots/intel_page_{page}.png\")\n",
    "        \n",
    "        # Wait for job listings to be visible \n",
    "        wait_for_job_listings(driver)\n",
    "        \n",
    "        # Extract current page's jobs\n",
    "        jobs_on_page = extract_job_listings_intel(driver)\n",
    "        all_jobs.extend(jobs_on_page)\n",
    "        logger.info(f\"Found {len(jobs_on_page)} jobs on page {page}\")\n",
    "        \n",
    "        # Look for next page button - try multiple selectors\n",
    "        next_selectors = [\n",
    "            \"[aria-label='next page']\", \n",
    "            \"[data-automation-id='paginationNextButton']\",\n",
    "            \"button[title='Next Page']\",\n",
    "            \"button.css-1ddxsuf\",\n",
    "            \"button.next-page\",\n",
    "            \"//button[contains(@class, 'page') and contains(@class, 'next')]\",\n",
    "            \"//button[contains(@aria-label, 'next')]\"\n",
    "        ]\n",
    "        \n",
    "        next_button = None\n",
    "        for selector in next_selectors:\n",
    "            try:\n",
    "                if selector.startswith(\"//\"):\n",
    "                    elements = driver.find_elements(By.XPATH, selector)\n",
    "                else:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                \n",
    "                for elem in elements:\n",
    "                    if elem.is_displayed() and not (elem.get_attribute(\"disabled\") or \"disabled\" in elem.get_attribute(\"class\") or \"inactive\" in elem.get_attribute(\"class\")):\n",
    "                        next_button = elem\n",
    "                        break\n",
    "                        \n",
    "                if next_button:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if not next_button:\n",
    "            logger.info(\"No next page button found. Reached last page.\")\n",
    "            break\n",
    "            \n",
    "        # Check if button is disabled (end of pages)\n",
    "        if next_button.get_attribute(\"disabled\") or \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "            logger.info(\"Reached last page - Next button is disabled\")\n",
    "            break\n",
    "            \n",
    "        # Click next page using JavaScript to avoid intercept issues\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            logger.info(\"Clicked next page button\")\n",
    "            time.sleep(5)  # Wait for page to load\n",
    "            page += 1\n",
    "            \n",
    "            # Wait for job listings to reload\n",
    "            wait_for_job_listings(driver)\n",
    "            \n",
    "            # Take screenshot after page change\n",
    "            driver.save_screenshot(f\"screenshots/intel_page_{page}_loaded.png\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error clicking next page: {e}\")\n",
    "            # Try one more time with a different approach\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                time.sleep(1)\n",
    "                next_button.click()\n",
    "                logger.info(\"Clicked next page button using alternative method\")\n",
    "                time.sleep(5)\n",
    "                page += 1\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Still failed to click next page: {e2}\")\n",
    "                break\n",
    "            \n",
    "    return all_jobs\n",
    "\n",
    "# Helper function to wait for job listings to appear\n",
    "def wait_for_job_listings(driver, timeout=15):\n",
    "    \"\"\"Wait for job listings to appear on the page using multiple possible selectors\"\"\"\n",
    "    selectors = [\n",
    "        \"[data-automation-id='jobTitle']\",\n",
    "        \".WGDC\",\n",
    "        \"[role='listitem']\",\n",
    "        \".css-19uc56f\",\n",
    "        \".css-1q6t3sv div[role='row']\",\n",
    "        \"ul[role='list'] li\"\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        try:\n",
    "            WebDriverWait(driver, timeout).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            logger.info(f\"Job listings found with selector: {selector}\")\n",
    "            return True\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "    \n",
    "    logger.warning(\"Could not detect job listings with any known selector\")\n",
    "    return False\n",
    "\n",
    "# Extract job information from Intel page\n",
    "def extract_job_listings_intel(driver):\n",
    "    \"\"\"\n",
    "    Extract job listings from Intel Workday page\n",
    "    Handles different potential Workday UI structures\n",
    "    \"\"\"\n",
    "    jobs_data = []\n",
    "    \n",
    "    # Try different selectors for job elements \n",
    "    selectors = [\n",
    "        # Primary Workday selectors\n",
    "        {\"container\": \"[data-automation-id='jobTitle']\", \"type\": \"primary\"},\n",
    "        {\"container\": \"[data-automation-id='job-card']\", \"type\": \"primary\"},\n",
    "        {\"container\": \".css-1q6t3sv [role='row']\", \"type\": \"row\"},\n",
    "        {\"container\": \"ul[role='list'] > li\", \"type\": \"list\"}, \n",
    "        {\"container\": \".WGDC a[role='link']\", \"type\": \"link\"}\n",
    "    ]\n",
    "    \n",
    "    job_elements = []\n",
    "    used_selector = None\n",
    "    \n",
    "    # Try each selector to find job elements\n",
    "    for selector in selectors:\n",
    "        try:\n",
    "            elements = driver.find_elements(By.CSS_SELECTOR, selector[\"container\"])\n",
    "            if elements and len(elements) > 0:\n",
    "                job_elements = elements\n",
    "                used_selector = selector\n",
    "                logger.info(f\"Found {len(elements)} job elements using selector: {selector['container']}\")\n",
    "                \n",
    "                # Take a screenshot of the found elements (for debugging)\n",
    "                if len(elements) > 0:\n",
    "                    try:\n",
    "                        driver.execute_script(\"arguments[0].style.border='3px solid red'\", elements[0])\n",
    "                        driver.save_screenshot(\"screenshots/intel_job_elements_found.png\")\n",
    "                        driver.execute_script(\"arguments[0].style.border=''\", elements[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Selector {selector['container']} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not job_elements:\n",
    "        logger.warning(\"Could not find job elements with any selector. Taking screenshot for debugging.\")\n",
    "        driver.save_screenshot(\"screenshots/intel_no_job_elements.png\")\n",
    "        return jobs_data\n",
    "    \n",
    "    # Extract detailed information for each job based on the selector type\n",
    "    for index, job in enumerate(job_elements):\n",
    "        try:\n",
    "            # Different extraction logic based on which selector worked\n",
    "            if used_selector[\"type\"] == \"primary\":  # Standard Workday implementation\n",
    "                title_element = job if \"jobTitle\" in used_selector[\"container\"] else job.find_element(By.CSS_SELECTOR, \"[data-automation-id='jobTitle']\")\n",
    "                title = title_element.text.strip()\n",
    "                link = title_element.get_attribute(\"href\")\n",
    "                \n",
    "                # Try to find location near the job title element\n",
    "                location = \"Not specified\"\n",
    "                try:\n",
    "                    # Find parent card or container\n",
    "                    parent_card = None\n",
    "                    try:\n",
    "                        parent_card = title_element.find_element(By.XPATH, \"ancestor::div[contains(@class, 'css-') and @data-automation-id]\")\n",
    "                    except:\n",
    "                        parent_card = title_element.find_element(By.XPATH, \"./ancestor::*[3]\")  # Go up a few levels\n",
    "                    \n",
    "                    # Try multiple location selectors\n",
    "                    location_selectors = [\n",
    "                        \"[data-automation-id='location']\", \n",
    "                        \"[data-automation-id='locationLabel']\",\n",
    "                        \".css-1wzygq\",\n",
    "                        \".css-129m7dg\",\n",
    "                        \"//span[contains(text(), ',')]\",\n",
    "                        \"//div[contains(text(), ',')]\"\n",
    "                    ]\n",
    "                    \n",
    "                    for loc_selector in location_selectors:\n",
    "                        try:\n",
    "                            if loc_selector.startswith(\"//\"):\n",
    "                                location_elem = parent_card.find_element(By.XPATH, loc_selector)\n",
    "                            else:\n",
    "                                location_elem = parent_card.find_element(By.CSS_SELECTOR, loc_selector)\n",
    "                                \n",
    "                            if location_elem:\n",
    "                                location = location_elem.text.strip()\n",
    "                                if location and (\",\" in location or \"Remote\" in location):\n",
    "                                    break\n",
    "                        except:\n",
    "                            continue\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Could not find location with primary selectors: {e}\")\n",
    "                    # Fallback: look for any element that mentions location\n",
    "                    try:\n",
    "                        # Look for elements after the title with location formatting \n",
    "                        location_candidates = driver.find_elements(By.XPATH, \n",
    "                            f\"//div[contains(text(), '{title}')]/following::div[contains(text(), ',') or contains(text(), 'Remote')]\")\n",
    "                        if location_candidates:\n",
    "                            location = location_candidates[0].text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "            elif used_selector[\"type\"] in [\"row\", \"list\", \"link\"]:  # Generic extraction for other selectors\n",
    "                # Find title element\n",
    "                title_elem = None\n",
    "                title_selectors = [\"a\", \"h3\", \"[title]\", \"[aria-label]\", \"span.css-srrtrq\"]\n",
    "                \n",
    "                for t_selector in title_selectors:\n",
    "                    try:\n",
    "                        title_candidates = job.find_elements(By.CSS_SELECTOR, t_selector)\n",
    "                        for elem in title_candidates:\n",
    "                            text = elem.text.strip() or elem.get_attribute(\"title\") or elem.get_attribute(\"aria-label\")\n",
    "                            if text and len(text) > 3:  # Ensure it's substantial text\n",
    "                                title_elem = elem\n",
    "                                break\n",
    "                        if title_elem:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                if not title_elem:\n",
    "                    logger.debug(f\"Could not find title for job {index+1}\")\n",
    "                    continue\n",
    "                    \n",
    "                title = title_elem.text.strip() or title_elem.get_attribute(\"title\") or title_elem.get_attribute(\"aria-label\")\n",
    "                link = title_elem.get_attribute(\"href\")\n",
    "                \n",
    "                if not link:\n",
    "                    # Try to find a parent or sibling with a link\n",
    "                    link_containers = job.find_elements(By.CSS_SELECTOR, \"a\")\n",
    "                    if link_containers:\n",
    "                        link = link_containers[0].get_attribute(\"href\")\n",
    "                \n",
    "                # Try to find location\n",
    "                location = \"Not specified\"\n",
    "                try:\n",
    "                    # Look for location in various ways\n",
    "                    location_patterns = [\n",
    "                        \".//span[contains(text(), ',')]\",\n",
    "                        \".//div[contains(text(), ',')]\",\n",
    "                        \".//div[contains(text(), 'Location')]//following-sibling::div\",\n",
    "                        \".//span[contains(text(), 'Remote')]\",\n",
    "                        \".//div[contains(@class, 'location')]\"\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in location_patterns:\n",
    "                        location_elems = job.find_elements(By.XPATH, pattern)\n",
    "                        if location_elems:\n",
    "                            location = location_elems[0].text.strip()\n",
    "                            if location and len(location) > 2:  # Ensure it's not empty\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Could not find location with alternative selectors: {e}\")\n",
    "            \n",
    "            # Add the extracted job if we have at least a title and link\n",
    "            if title and title.strip() and link and link.strip():\n",
    "                # Check for duplicate before adding\n",
    "                is_duplicate = False\n",
    "                for existing_job in jobs_data:\n",
    "                    if existing_job[\"Title\"] == title and existing_job[\"Link\"] == link:\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                \n",
    "                if not is_duplicate:\n",
    "                    jobs_data.append({\n",
    "                        \"Title\": title,\n",
    "                        \"Location\": location,\n",
    "                        \"Link\": link,\n",
    "                        \"Description\": \"\",  # We'll get descriptions in a separate step\n",
    "                        \"Company\": \"Intel\"\n",
    "                    })\n",
    "                    logger.info(f\"Added job: {title} at {location}\")\n",
    "            \n",
    "        except (StaleElementReferenceException, Exception) as e:\n",
    "            logger.error(f\"Error extracting job details for job {index+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return jobs_data\n",
    "\n",
    "# Improved function to get job descriptions with full page screenshots saved in a separate folder\n",
    "def get_job_descriptions(driver, jobs_data, max_descriptions=100):\n",
    "    \"\"\"\n",
    "    Get job descriptions for a batch of jobs by visiting their individual pages.\n",
    "    Save full page screenshots of each job description in a separate folder.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Getting descriptions for {len(jobs_data)} jobs (up to {max_descriptions})\")\n",
    "    \n",
    "    # Create a dedicated folder for job description screenshots\n",
    "    job_desc_folder = 'job_description_screenshots_intel'\n",
    "    os.makedirs(job_desc_folder, exist_ok=True)\n",
    "    \n",
    "    # Store current URL to return to afterward\n",
    "    original_url = driver.current_url\n",
    "    original_window = driver.current_window_handle\n",
    "    \n",
    "    # Process up to max_descriptions\n",
    "    for i, job in enumerate(jobs_data[:max_descriptions]):\n",
    "        if not job.get(\"Link\"):\n",
    "            continue\n",
    "            \n",
    "        logger.info(f\"Getting description for job {i+1}/{min(len(jobs_data), max_descriptions)}: {job['Title']}\")\n",
    "        \n",
    "        # Create a clean filename from the job title\n",
    "        clean_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", job['Title'])\n",
    "        clean_title = re.sub(r'\\s+', \"_\", clean_title)\n",
    "        clean_title = clean_title[:100] if len(clean_title) > 100 else clean_title\n",
    "        \n",
    "        # Create a new tab for each job\n",
    "        try:\n",
    "            # Open new tab\n",
    "            driver.execute_script(\"window.open('about:blank', '_blank');\")\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            \n",
    "            # Navigate to job details page\n",
    "            driver.get(job[\"Link\"])\n",
    "            time.sleep(5)  # Wait for page to load\n",
    "            \n",
    "            # Take a full page screenshot\n",
    "            # First, get the height of the entire page\n",
    "            total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            # Set window size to capture entire page\n",
    "            driver.set_window_size(1920, total_height)\n",
    "            \n",
    "            # Take screenshot and save in the dedicated folder\n",
    "            screenshot_file = f\"{job_desc_folder}/job_{i+1}_{clean_title}.png\"\n",
    "            driver.save_screenshot(screenshot_file)\n",
    "            logger.info(f\"Saved full-page screenshot to {screenshot_file}\")\n",
    "            \n",
    "            # For long pages, also capture screenshots of each section\n",
    "            current_height = 0\n",
    "            viewport_height = 1080\n",
    "            section = 1\n",
    "            \n",
    "            while current_height < total_height:\n",
    "                driver.execute_script(f\"window.scrollTo(0, {current_height});\")\n",
    "                time.sleep(0.5)\n",
    "                section_screenshot = f\"{job_desc_folder}/job_{i+1}_{clean_title}_section_{section}.png\"\n",
    "                driver.save_screenshot(section_screenshot)\n",
    "                current_height += viewport_height\n",
    "                section += 1\n",
    "                \n",
    "            # Reset scroll position\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            \n",
    "            # Extract description using multiple potential selectors\n",
    "            description_selectors = [\n",
    "                \"[data-automation-id='job-description']\",\n",
    "                \".job-description\",\n",
    "                \"#job-description\",\n",
    "                \"[role='main']\",\n",
    "                \"article\",\n",
    "                \"[data-automation-id='jobPosting']\",\n",
    "                \".css-vh281m\",\n",
    "                \".css-1prfaxn\"\n",
    "            ]\n",
    "            \n",
    "            description = \"\"\n",
    "            for selector in description_selectors:\n",
    "                try:\n",
    "                    desc_elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if desc_elements:\n",
    "                        description = desc_elements[0].text.strip()\n",
    "                        if description:\n",
    "                            logger.info(f\"Got description for '{job['Title']}' ({len(description)} chars)\")\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Selector {selector} failed: {e}\")\n",
    "            \n",
    "            # Update the job with the description\n",
    "            if description:\n",
    "                job[\"Description\"] = description\n",
    "            \n",
    "            # Close tab\n",
    "            driver.close()\n",
    "            driver.switch_to.window(original_window)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting description for {job['Title']}: {e}\")\n",
    "            # Make sure we're back to the original window\n",
    "            try:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(original_window)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Return to original page\n",
    "    try:\n",
    "        driver.get(original_url)\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    logger.info(f\"Completed fetching descriptions for {min(len(jobs_data), max_descriptions)} jobs\")\n",
    "    return jobs_data\n",
    "\n",
    "# Function to handle different types of popups\n",
    "def handle_popups(driver):\n",
    "    try:\n",
    "        # Common buttons for accepting cookies, terms, etc.\n",
    "        popup_selectors = [\n",
    "            \"//button[contains(text(), 'Accept')]\", \n",
    "            \"//button[contains(text(), 'I agree')]\",\n",
    "            \"//button[contains(@id, 'accept')]\",\n",
    "            \"//button[contains(@class, 'accept')]\",\n",
    "            \"//button[contains(text(), 'Continue')]\",\n",
    "            \"//button[contains(text(), 'Got it')]\",\n",
    "            \"//button[contains(text(), 'Close')]\",\n",
    "            \"//button[@aria-label='Close']\",\n",
    "            \"//div[contains(@class, 'cookie')]//button\",\n",
    "            \"//div[contains(@id, 'consent')]//button\"\n",
    "        ]\n",
    "        \n",
    "        for xpath in popup_selectors:\n",
    "            try:\n",
    "                buttons = driver.find_elements(By.XPATH, xpath)\n",
    "                for button in buttons:\n",
    "                    if button.is_displayed():\n",
    "                        button.click()\n",
    "                        logger.info(f\"Clicked popup/cookie button with xpath: {xpath}\")\n",
    "                        time.sleep(1)\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "        # Handle alerts\n",
    "        try:\n",
    "            alert = Alert(driver)\n",
    "            alert.accept()\n",
    "            logger.info(\"Accepted alert popup\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error handling popups: {e}\")\n",
    "\n",
    "# Function to validate URL\n",
    "def is_valid_link(url):\n",
    "    if not url or not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        return False\n",
    "        \n",
    "    # For Intel Workday links, we'll assume they're valid without checking\n",
    "    if \"intel.wd1.myworkdayjobs.com\" in url:\n",
    "        return True\n",
    "        \n",
    "    try:\n",
    "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
    "        return response.status_code < 400  # Accept any non-error status\n",
    "    except requests.RequestException:\n",
    "        logger.warning(f\"Invalid link: {url}\")\n",
    "        return False\n",
    "\n",
    "# Main Intel job scraper function\n",
    "def scrape_intel_jobs(search_keyword=\"\", max_pages=20, headless=False):\n",
    "    \"\"\"\n",
    "    Scrape Intel jobs with comprehensive approach including pagination.\n",
    "    The search_keyword is used only for searching on the website.\n",
    "    All found jobs will be saved regardless of keyword match.\n",
    "    \n",
    "    Parameters:\n",
    "    search_keyword (str): Keyword to search for (empty string for all jobs)\n",
    "    max_pages (int): Maximum number of pages to scrape\n",
    "    headless (bool): Whether to run in headless mode\n",
    "    \n",
    "    Returns:\n",
    "    list: List of all job dictionaries found\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "        \n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36')\n",
    "    \n",
    "    # Create directories for debugging\n",
    "    os.makedirs('screenshots', exist_ok=True)\n",
    "    \n",
    "    driver = None\n",
    "    jobs_data = []\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        \n",
    "        # Construct search URL - Intel Workday URL\n",
    "        search_term = search_keyword.replace(\" \", \"%20\") if search_keyword else \"\"\n",
    "        if search_term:\n",
    "            base_url = f\"https://intel.wd1.myworkdayjobs.com/External?q={search_term}\"\n",
    "        else:\n",
    "            base_url = \"https://intel.wd1.myworkdayjobs.com/External\"\n",
    "\n",
    "        # Open the Intel careers page\n",
    "        logger.info(f\"Scraping jobs from Intel\" + (f\", searching for '{search_keyword}'\" if search_keyword else \"\"))\n",
    "        driver.get(base_url)\n",
    "        driver.save_screenshot(\"screenshots/intel_initial.png\")\n",
    "        \n",
    "        # Handle popups\n",
    "        handle_popups(driver)\n",
    "        \n",
    "        # Wait for results to load - trying different possible selectors\n",
    "        if not wait_for_job_listings(driver, timeout=20):\n",
    "            logger.warning(\"Could not detect job search results loading. Trying manual search...\")\n",
    "            \n",
    "            # Try to search directly by submitting a search form\n",
    "            try:\n",
    "                search_box = driver.find_element(By.CSS_SELECTOR, \"input[type='search'], input[placeholder*='Search']\")\n",
    "                search_box.clear()\n",
    "                search_box.send_keys(search_keyword)\n",
    "                search_box.send_keys(Keys.RETURN)\n",
    "                time.sleep(5)\n",
    "                logger.info(\"Tried manual search submission\")\n",
    "                driver.save_screenshot(\"screenshots/intel_manual_search.png\")\n",
    "                \n",
    "                # Wait again for results\n",
    "                wait_for_job_listings(driver, timeout=15)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Manual search also failed: {e}\")\n",
    "        \n",
    "        # Scroll to load all results on first page\n",
    "        job_count = scroll_to_load_all(driver, max_scrolls=20, wait_time=3)\n",
    "        driver.save_screenshot(\"screenshots/intel_after_scroll.png\")\n",
    "        \n",
    "        # Handle pagination and collect all jobs\n",
    "        all_jobs = handle_pagination(driver, max_pages=max_pages)\n",
    "        logger.info(f\"Collected {len(all_jobs)} total jobs after pagination\")\n",
    "        \n",
    "        # Get job descriptions for all collected jobs\n",
    "        jobs_with_descriptions = get_job_descriptions(driver, all_jobs, max_descriptions=200)\n",
    "        \n",
    "        # Save all jobs regardless of search keyword\n",
    "        jobs_data = jobs_with_descriptions\n",
    "        logger.info(f\"Saving all {len(jobs_data)} jobs found in search results\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping from Intel: {e}\")\n",
    "        if driver:\n",
    "            driver.save_screenshot(\"screenshots/intel_error.png\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "    return jobs_data\n",
    "\n",
    "# Main function to scrape and save results to CSV\n",
    "def main(search_keyword=\"\", max_pages=20, headless=False):\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting Intel job scraper at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Create screenshots directory\n",
    "    os.makedirs('screenshots', exist_ok=True)\n",
    "    \n",
    "    # Create job description screenshots directory\n",
    "    os.makedirs('job_description_screenshots_intel', exist_ok=True)\n",
    "    \n",
    "    # Scrape Intel jobs\n",
    "    jobs_data = scrape_intel_jobs(search_keyword, max_pages, headless)\n",
    "    \n",
    "    # Generate filenames with timestamps\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    keyword_slug = search_keyword.replace(' ', '_') if search_keyword else 'all'\n",
    "    detailed_filename = f\"intel_jobs_detailed_{keyword_slug}_{timestamp}.csv\"\n",
    "    simple_filename = f\"intel_jobs_simple_{keyword_slug}_{timestamp}.csv\"\n",
    "    \n",
    "    # Save results\n",
    "    if jobs_data:\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(jobs_data)\n",
    "        \n",
    "        # Save detailed CSV with descriptions\n",
    "        df.to_csv(detailed_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Detailed jobs saved to '{detailed_filename}'\")\n",
    "        \n",
    "        # Create and save a simplified CSV without descriptions\n",
    "        simple_df = df[['Title', 'Location', 'Link']].copy()\n",
    "        simple_df.to_csv(simple_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Simplified jobs list saved to '{simple_filename}'\")\n",
    "        \n",
    "        # Also create a filtered file if a search keyword was provided\n",
    "        if search_keyword:\n",
    "            filtered_df = df[df['Title'].str.contains(search_keyword, case=False)].copy()\n",
    "            filtered_filename = f\"intel_jobs_filtered_{keyword_slug}_{timestamp}.csv\"\n",
    "            filtered_df.to_csv(filtered_filename, index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Jobs filtered by keyword '{search_keyword}' saved to '{filtered_filename}'\")\n",
    "            logger.info(f\"Found {len(filtered_df)} jobs matching the keyword out of {len(df)} total jobs\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Intel Job Scraping Results:\")\n",
    "        print(f\"Total jobs found: {len(df)}\")\n",
    "        print(f\"Unique locations: {len(df['Location'].unique())}\")\n",
    "        print(f\"Sample jobs:\")\n",
    "        print(df[['Title', 'Location']].head())\n",
    "        print(\"\\nTop locations:\")\n",
    "        print(df['Location'].value_counts().head())\n",
    "        print(f\"\\nResults saved to:\")\n",
    "        print(f\"- {detailed_filename}\")\n",
    "        print(f\"- {simple_filename}\")\n",
    "        if search_keyword:\n",
    "            print(f\"- {filtered_filename}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        logger.warning(f\"No jobs found with search keyword '{search_keyword}'\")\n",
    "        print(\"\\nNo jobs found to display.\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Process completed with no results in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Intel Job Scraper\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get user input\n",
    "    job_title = input(\"Enter job title to search for (leave empty to get all jobs): \").strip()\n",
    "    \n",
    "    try:\n",
    "        max_pages = int(input(\"Maximum number of pages to scrape (default 10): \") or \"10\")\n",
    "    except ValueError:\n",
    "        max_pages = 10\n",
    "        print(\"Invalid input. Using default of 10 pages.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    headless_mode = input(\"Run in headless mode? (y/n, default: n): \").strip().lower() == 'y'\n",
    "    \n",
    "    print(\"\\nStarting job scraper...\")\n",
    "    print(\"This may take several minutes depending on the number of jobs and pages.\")\n",
    "    print(\"Progress will be logged to the console and a log file.\")\n",
    "    \n",
    "    # Run the scraper\n",
    "    main(job_title, max_pages, headless_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56009452-9b66-4dda-b88b-d3873c340ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
