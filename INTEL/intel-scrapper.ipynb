{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f89adf-17ed-4267-8383-52abf4d75e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel Job Scraper\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job title to search for (leave empty to get all jobs):  Machine learning\n",
      "Maximum number of pages to scrape (default 10):  1\n",
      "Run in headless mode? (y/n, default: n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:12:01,179 - INFO - Starting Intel job scraper at 2025-03-20 16:12:01\n",
      "2025-03-20 16:12:01,181 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting job scraper...\n",
      "This may take several minutes depending on the number of jobs and pages.\n",
      "Progress will be logged to the console and a log file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:12:01,478 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-20 16:12:01,561 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-20 16:12:01,612 - INFO - Driver [/Users/srikar/.wdm/drivers/chromedriver/mac64/134.0.6998.90/chromedriver-mac-arm64/chromedriver] found in cache\n",
      "2025-03-20 16:12:02,615 - INFO - Scraping jobs from Intel, searching for 'Machine learning'\n",
      "2025-03-20 16:12:05,103 - INFO - Job listings found with selector: [data-automation-id='jobTitle']\n",
      "2025-03-20 16:12:05,108 - INFO - Starting to scroll to load all content...\n",
      "2025-03-20 16:12:08,611 - INFO - Scroll 1: Height 4470 → 4470, Jobs found: 20\n",
      "2025-03-20 16:12:12,090 - INFO - Scroll 2: Height 4470 → 4470, Jobs found: 20\n",
      "2025-03-20 16:12:12,091 - INFO - No change detected (1/3)\n",
      "2025-03-20 16:12:15,577 - INFO - Scroll 3: Height 4470 → 4470, Jobs found: 20\n",
      "2025-03-20 16:12:15,577 - INFO - No change detected (2/3)\n",
      "2025-03-20 16:12:19,053 - INFO - Scroll 4: Height 4470 → 4470, Jobs found: 20\n",
      "2025-03-20 16:12:19,053 - INFO - No change detected (3/3)\n",
      "2025-03-20 16:12:19,054 - INFO - No more content loading after multiple scrolls. Stopping scroll operation.\n",
      "2025-03-20 16:12:19,054 - INFO - Completed scrolling after 3 scrolls. Found approximately 20 job items.\n",
      "2025-03-20 16:12:19,451 - INFO - Starting pagination handling...\n",
      "2025-03-20 16:12:19,451 - INFO - Processing page 1\n",
      "2025-03-20 16:12:19,871 - INFO - Job listings found with selector: [data-automation-id='jobTitle']\n",
      "2025-03-20 16:12:19,879 - INFO - Found 20 job elements using selector: [data-automation-id='jobTitle']\n",
      "2025-03-20 16:12:20,410 - INFO - Added job: Machine Learning Engineer at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,457 - INFO - Added job: Full Stack Software Developer & Machine Learning Engineer at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,505 - INFO - Added job: Foundational AI Research Scientist Intel contract employee at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,553 - INFO - Added job: AI SW Architect at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,600 - INFO - Added job: AI Software Tools Team Leader at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,648 - INFO - Added job: NPU Compiler Developer (C++) at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,695 - INFO - Added job: AI SW Performance Engineer at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,748 - INFO - Added job: NPU SW/HW Validation Engineer (Power Measurements) at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,797 - INFO - Added job: AI Software development engineer at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,842 - INFO - Added job: Foundational AI Research Intern at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,888 - INFO - Added job: SW CI (Embedded) Team Leader at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,934 - INFO - Added job: Senior Formal Verification Engineer at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:20,983 - INFO - Added job: Experienced DFT Engineer at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:21,033 - INFO - Added job: Research Scientist Intern – ML in Graphics at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:21,082 - INFO - Added job: AI Software Solutions Engineer at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:21,131 - INFO - Added job: AI Software Solutions Engineer at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:21,178 - INFO - Added job: Software Engineering Team Lead at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:21,225 - INFO - Added job: AI/ML Technologist at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:21,269 - INFO - Added job: Sr. Data Governance Advisor at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:21,316 - INFO - Added job: AI Software Simulation Engineer at © 2025 Workday, Inc. All rights reserved.\n",
      "2025-03-20 16:12:21,316 - INFO - Found 20 jobs on page 1\n",
      "2025-03-20 16:12:21,413 - INFO - Clicked next page button\n",
      "2025-03-20 16:12:26,432 - INFO - Job listings found with selector: [data-automation-id='jobTitle']\n",
      "2025-03-20 16:12:26,894 - INFO - Collected 20 total jobs after pagination\n",
      "2025-03-20 16:12:26,894 - INFO - Getting descriptions for 20 jobs (up to 200)\n",
      "2025-03-20 16:12:26,898 - INFO - Getting description for job 1/20: Machine Learning Engineer\n",
      "2025-03-20 16:12:33,823 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_1_Machine_Learning_Engineer.png\n",
      "2025-03-20 16:12:40,017 - INFO - Getting description for job 2/20: Full Stack Software Developer & Machine Learning Engineer\n",
      "2025-03-20 16:12:47,131 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_2_Full_Stack_Software_Developer_&_Machine_Learning_Engineer.png\n",
      "2025-03-20 16:12:55,997 - INFO - Getting description for job 3/20: Foundational AI Research Scientist Intel contract employee\n",
      "2025-03-20 16:13:03,421 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_3_Foundational_AI_Research_Scientist_Intel_contract_employee.png\n",
      "2025-03-20 16:13:13,142 - INFO - Getting description for job 4/20: AI SW Architect\n",
      "2025-03-20 16:13:20,490 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_4_AI_SW_Architect.png\n",
      "2025-03-20 16:13:30,028 - INFO - Getting description for job 5/20: AI Software Tools Team Leader\n",
      "2025-03-20 16:13:37,379 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_5_AI_Software_Tools_Team_Leader.png\n",
      "2025-03-20 16:13:46,481 - INFO - Getting description for job 6/20: NPU Compiler Developer (C++)\n",
      "2025-03-20 16:13:53,566 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_6_NPU_Compiler_Developer_(C++).png\n",
      "2025-03-20 16:14:02,603 - INFO - Getting description for job 7/20: AI SW Performance Engineer\n",
      "2025-03-20 16:14:10,439 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_7_AI_SW_Performance_Engineer.png\n",
      "2025-03-20 16:14:24,122 - INFO - Getting description for job 8/20: NPU SW/HW Validation Engineer (Power Measurements)\n",
      "2025-03-20 16:14:31,796 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_8_NPU_SWHW_Validation_Engineer_(Power_Measurements).png\n",
      "2025-03-20 16:14:45,563 - INFO - Getting description for job 9/20: AI Software development engineer\n",
      "2025-03-20 16:14:53,192 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_9_AI_Software_development_engineer.png\n",
      "2025-03-20 16:15:06,144 - INFO - Getting description for job 10/20: Foundational AI Research Intern\n",
      "2025-03-20 16:15:13,723 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_10_Foundational_AI_Research_Intern.png\n",
      "2025-03-20 16:15:26,295 - INFO - Getting description for job 11/20: SW CI (Embedded) Team Leader\n",
      "2025-03-20 16:15:33,772 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_11_SW_CI_(Embedded)_Team_Leader.png\n",
      "2025-03-20 16:15:43,395 - INFO - Getting description for job 12/20: Senior Formal Verification Engineer\n",
      "2025-03-20 16:15:50,938 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_12_Senior_Formal_Verification_Engineer.png\n",
      "2025-03-20 16:16:00,421 - INFO - Getting description for job 13/20: Experienced DFT Engineer\n",
      "2025-03-20 16:16:07,793 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_13_Experienced_DFT_Engineer.png\n",
      "2025-03-20 16:16:17,071 - INFO - Getting description for job 14/20: Research Scientist Intern – ML in Graphics\n",
      "2025-03-20 16:16:24,785 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_14_Research_Scientist_Intern_–_ML_in_Graphics.png\n",
      "2025-03-20 16:16:34,500 - INFO - Getting description for job 15/20: AI Software Solutions Engineer\n",
      "2025-03-20 16:16:41,921 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_15_AI_Software_Solutions_Engineer.png\n",
      "2025-03-20 16:16:51,466 - INFO - Getting description for job 16/20: AI Software Solutions Engineer\n",
      "2025-03-20 16:16:58,753 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_16_AI_Software_Solutions_Engineer.png\n",
      "2025-03-20 16:17:08,142 - INFO - Getting description for job 17/20: Software Engineering Team Lead\n",
      "2025-03-20 16:17:15,410 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_17_Software_Engineering_Team_Lead.png\n",
      "2025-03-20 16:17:24,413 - INFO - Getting description for job 18/20: AI/ML Technologist\n",
      "2025-03-20 16:17:31,670 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_18_AIML_Technologist.png\n",
      "2025-03-20 16:17:40,676 - INFO - Getting description for job 19/20: Sr. Data Governance Advisor\n",
      "2025-03-20 16:17:47,862 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_19_Sr._Data_Governance_Advisor.png\n",
      "2025-03-20 16:17:56,512 - INFO - Getting description for job 20/20: AI Software Simulation Engineer\n",
      "2025-03-20 16:18:03,643 - INFO - Saved full-page screenshot to job_description_screenshots_intel/job_20_AI_Software_Simulation_Engineer.png\n",
      "2025-03-20 16:18:15,549 - INFO - Completed fetching descriptions for 20 jobs\n",
      "2025-03-20 16:18:15,553 - INFO - Saving all 20 jobs found in search results\n",
      "2025-03-20 16:18:15,839 - INFO - Detailed jobs saved to 'intel_jobs_detailed_Machine_learning_20250320_161815.csv'\n",
      "2025-03-20 16:18:15,847 - INFO - Simplified jobs list saved to 'intel_jobs_simple_Machine_learning_20250320_161815.csv'\n",
      "2025-03-20 16:18:15,850 - INFO - Jobs filtered by keyword 'Machine learning' saved to 'intel_jobs_filtered_Machine_learning_20250320_161815.csv'\n",
      "2025-03-20 16:18:15,850 - INFO - Found 2 jobs matching the keyword out of 20 total jobs\n",
      "2025-03-20 16:18:15,855 - INFO - Completed in 374.68 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Intel Job Scraping Results:\n",
      "Total jobs found: 20\n",
      "Unique locations: 1\n",
      "Sample jobs:\n",
      "                                               Title  \\\n",
      "0                          Machine Learning Engineer   \n",
      "1  Full Stack Software Developer & Machine Learni...   \n",
      "2  Foundational AI Research Scientist Intel contr...   \n",
      "3                                    AI SW Architect   \n",
      "4                      AI Software Tools Team Leader   \n",
      "\n",
      "                                    Location  \n",
      "0  © 2025 Workday, Inc. All rights reserved.  \n",
      "1  © 2025 Workday, Inc. All rights reserved.  \n",
      "2  © 2025 Workday, Inc. All rights reserved.  \n",
      "3  © 2025 Workday, Inc. All rights reserved.  \n",
      "4  © 2025 Workday, Inc. All rights reserved.  \n",
      "\n",
      "Top locations:\n",
      "Location\n",
      "© 2025 Workday, Inc. All rights reserved.    20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Results saved to:\n",
      "- intel_jobs_detailed_Machine_learning_20250320_161815.csv\n",
      "- intel_jobs_simple_Machine_learning_20250320_161815.csv\n",
      "- intel_jobs_filtered_Machine_learning_20250320_161815.csv\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.alert import Alert\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(f\"intel_job_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to scroll and load all jobs with improved logic\n",
    "def scroll_to_load_all(driver, max_scrolls=30, wait_time=2):\n",
    "    \"\"\"\n",
    "    Scroll the page to load all content with a maximum number of scrolls\n",
    "    For Intel's Workday-based site, which loads content dynamically\n",
    "    \"\"\"\n",
    "    scrolls = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    last_job_count = 0\n",
    "    consecutive_no_change = 0\n",
    "    \n",
    "    logger.info(\"Starting to scroll to load all content...\")\n",
    "    \n",
    "    while scrolls < max_scrolls:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(wait_time)  # Wait time for content to load\n",
    "        \n",
    "        # Take screenshot for debugging\n",
    "        driver.save_screenshot(f\"screenshots/intel_scroll_{scrolls+1}.png\")\n",
    "        \n",
    "        # Try to find the \"Load More\" or similar buttons and click them\n",
    "        try:\n",
    "            load_more_buttons = driver.find_elements(By.XPATH, \n",
    "                \"//button[contains(text(), 'Load More') or contains(text(), 'Show More') or contains(@aria-label, 'Load') or contains(@class, 'load-more')]\")\n",
    "            if load_more_buttons:\n",
    "                for button in load_more_buttons:\n",
    "                    if button.is_displayed() and button.is_enabled():\n",
    "                        driver.execute_script(\"arguments[0].click();\", button)\n",
    "                        logger.info(\"Clicked 'Load More' button\")\n",
    "                        time.sleep(wait_time + 1)  # Extra wait for new content\n",
    "        except Exception as e:\n",
    "            logger.info(f\"No 'Load More' button found or error clicking it: {e}\")\n",
    "        \n",
    "        # Check height and job count to determine if we've loaded all content\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Try different job card selectors to get an accurate count\n",
    "        job_selectors = [\n",
    "            \"[data-automation-id='jobTitle']\", \n",
    "            \".WGDC\", \n",
    "            \"[role='listitem']\",\n",
    "            \".css-19uc56f\",\n",
    "            \".css-1q6t3sv div[role='row']\"\n",
    "        ]\n",
    "        \n",
    "        current_job_count = 0\n",
    "        for selector in job_selectors:\n",
    "            count = len(driver.find_elements(By.CSS_SELECTOR, selector))\n",
    "            if count > current_job_count:\n",
    "                current_job_count = count\n",
    "        \n",
    "        logger.info(f\"Scroll {scrolls+1}: Height {last_height} → {new_height}, Jobs found: {current_job_count}\")\n",
    "        \n",
    "        # If no change in height and job count, we might have reached the end\n",
    "        if new_height == last_height and current_job_count == last_job_count:\n",
    "            consecutive_no_change += 1\n",
    "            logger.info(f\"No change detected ({consecutive_no_change}/3)\")\n",
    "            if consecutive_no_change >= 3:  # If no change for 3 consecutive scrolls\n",
    "                logger.info(\"No more content loading after multiple scrolls. Stopping scroll operation.\")\n",
    "                break\n",
    "        else:\n",
    "            consecutive_no_change = 0\n",
    "            \n",
    "        last_height = new_height\n",
    "        last_job_count = current_job_count\n",
    "        scrolls += 1\n",
    "    \n",
    "    logger.info(f\"Completed scrolling after {scrolls} scrolls. Found approximately {last_job_count} job items.\")\n",
    "    return last_job_count\n",
    "\n",
    "# Function to handle pagination for Intel Workday\n",
    "def handle_pagination(driver, max_pages=20):\n",
    "    \"\"\"\n",
    "    Handle pagination by clicking through all available pages\n",
    "    \"\"\"\n",
    "    page = 1\n",
    "    all_jobs = []\n",
    "    \n",
    "    logger.info(\"Starting pagination handling...\")\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        logger.info(f\"Processing page {page}\")\n",
    "        \n",
    "        # Take screenshot for debugging\n",
    "        driver.save_screenshot(f\"screenshots/intel_page_{page}.png\")\n",
    "        \n",
    "        # Wait for job listings to be visible \n",
    "        wait_for_job_listings(driver)\n",
    "        \n",
    "        # Extract current page's jobs\n",
    "        jobs_on_page = extract_job_listings_intel(driver)\n",
    "        all_jobs.extend(jobs_on_page)\n",
    "        logger.info(f\"Found {len(jobs_on_page)} jobs on page {page}\")\n",
    "        \n",
    "        # Look for next page button - try multiple selectors\n",
    "        next_selectors = [\n",
    "            \"[aria-label='next page']\", \n",
    "            \"[data-automation-id='paginationNextButton']\",\n",
    "            \"button[title='Next Page']\",\n",
    "            \"button.css-1ddxsuf\",\n",
    "            \"button.next-page\",\n",
    "            \"//button[contains(@class, 'page') and contains(@class, 'next')]\",\n",
    "            \"//button[contains(@aria-label, 'next')]\"\n",
    "        ]\n",
    "        \n",
    "        next_button = None\n",
    "        for selector in next_selectors:\n",
    "            try:\n",
    "                if selector.startswith(\"//\"):\n",
    "                    elements = driver.find_elements(By.XPATH, selector)\n",
    "                else:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                \n",
    "                for elem in elements:\n",
    "                    if elem.is_displayed() and not (elem.get_attribute(\"disabled\") or \"disabled\" in elem.get_attribute(\"class\") or \"inactive\" in elem.get_attribute(\"class\")):\n",
    "                        next_button = elem\n",
    "                        break\n",
    "                        \n",
    "                if next_button:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if not next_button:\n",
    "            logger.info(\"No next page button found. Reached last page.\")\n",
    "            break\n",
    "            \n",
    "        # Check if button is disabled (end of pages)\n",
    "        if next_button.get_attribute(\"disabled\") or \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "            logger.info(\"Reached last page - Next button is disabled\")\n",
    "            break\n",
    "            \n",
    "        # Click next page using JavaScript to avoid intercept issues\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            logger.info(\"Clicked next page button\")\n",
    "            time.sleep(5)  # Wait for page to load\n",
    "            page += 1\n",
    "            \n",
    "            # Wait for job listings to reload\n",
    "            wait_for_job_listings(driver)\n",
    "            \n",
    "            # Take screenshot after page change\n",
    "            driver.save_screenshot(f\"screenshots/intel_page_{page}_loaded.png\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error clicking next page: {e}\")\n",
    "            # Try one more time with a different approach\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                time.sleep(1)\n",
    "                next_button.click()\n",
    "                logger.info(\"Clicked next page button using alternative method\")\n",
    "                time.sleep(5)\n",
    "                page += 1\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Still failed to click next page: {e2}\")\n",
    "                break\n",
    "            \n",
    "    return all_jobs\n",
    "\n",
    "# Helper function to wait for job listings to appear\n",
    "def wait_for_job_listings(driver, timeout=15):\n",
    "    \"\"\"Wait for job listings to appear on the page using multiple possible selectors\"\"\"\n",
    "    selectors = [\n",
    "        \"[data-automation-id='jobTitle']\",\n",
    "        \".WGDC\",\n",
    "        \"[role='listitem']\",\n",
    "        \".css-19uc56f\",\n",
    "        \".css-1q6t3sv div[role='row']\",\n",
    "        \"ul[role='list'] li\"\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        try:\n",
    "            WebDriverWait(driver, timeout).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            logger.info(f\"Job listings found with selector: {selector}\")\n",
    "            return True\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "    \n",
    "    logger.warning(\"Could not detect job listings with any known selector\")\n",
    "    return False\n",
    "\n",
    "# Extract job information from Intel page\n",
    "def extract_job_listings_intel(driver):\n",
    "    \"\"\"\n",
    "    Extract job listings from Intel Workday page\n",
    "    Handles different potential Workday UI structures\n",
    "    \"\"\"\n",
    "    jobs_data = []\n",
    "    \n",
    "    # Try different selectors for job elements \n",
    "    selectors = [\n",
    "        # Primary Workday selectors\n",
    "        {\"container\": \"[data-automation-id='jobTitle']\", \"type\": \"primary\"},\n",
    "        {\"container\": \"[data-automation-id='job-card']\", \"type\": \"primary\"},\n",
    "        {\"container\": \".css-1q6t3sv [role='row']\", \"type\": \"row\"},\n",
    "        {\"container\": \"ul[role='list'] > li\", \"type\": \"list\"}, \n",
    "        {\"container\": \".WGDC a[role='link']\", \"type\": \"link\"}\n",
    "    ]\n",
    "    \n",
    "    job_elements = []\n",
    "    used_selector = None\n",
    "    \n",
    "    # Try each selector to find job elements\n",
    "    for selector in selectors:\n",
    "        try:\n",
    "            elements = driver.find_elements(By.CSS_SELECTOR, selector[\"container\"])\n",
    "            if elements and len(elements) > 0:\n",
    "                job_elements = elements\n",
    "                used_selector = selector\n",
    "                logger.info(f\"Found {len(elements)} job elements using selector: {selector['container']}\")\n",
    "                \n",
    "                # Take a screenshot of the found elements (for debugging)\n",
    "                if len(elements) > 0:\n",
    "                    try:\n",
    "                        driver.execute_script(\"arguments[0].style.border='3px solid red'\", elements[0])\n",
    "                        driver.save_screenshot(\"screenshots/intel_job_elements_found.png\")\n",
    "                        driver.execute_script(\"arguments[0].style.border=''\", elements[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Selector {selector['container']} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not job_elements:\n",
    "        logger.warning(\"Could not find job elements with any selector. Taking screenshot for debugging.\")\n",
    "        driver.save_screenshot(\"screenshots/intel_no_job_elements.png\")\n",
    "        return jobs_data\n",
    "    \n",
    "    # Extract detailed information for each job based on the selector type\n",
    "    for index, job in enumerate(job_elements):\n",
    "        try:\n",
    "            # Different extraction logic based on which selector worked\n",
    "            if used_selector[\"type\"] == \"primary\":  # Standard Workday implementation\n",
    "                title_element = job if \"jobTitle\" in used_selector[\"container\"] else job.find_element(By.CSS_SELECTOR, \"[data-automation-id='jobTitle']\")\n",
    "                title = title_element.text.strip()\n",
    "                link = title_element.get_attribute(\"href\")\n",
    "                \n",
    "                # Try to find location near the job title element\n",
    "                location = \"Not specified\"\n",
    "                try:\n",
    "                    # Find parent card or container\n",
    "                    parent_card = None\n",
    "                    try:\n",
    "                        parent_card = title_element.find_element(By.XPATH, \"ancestor::div[contains(@class, 'css-') and @data-automation-id]\")\n",
    "                    except:\n",
    "                        parent_card = title_element.find_element(By.XPATH, \"./ancestor::*[3]\")  # Go up a few levels\n",
    "                    \n",
    "                    # Try multiple location selectors\n",
    "                    location_selectors = [\n",
    "                        \"[data-automation-id='location']\", \n",
    "                        \"[data-automation-id='locationLabel']\",\n",
    "                        \".css-1wzygq\",\n",
    "                        \".css-129m7dg\",\n",
    "                        \"//span[contains(text(), ',')]\",\n",
    "                        \"//div[contains(text(), ',')]\"\n",
    "                    ]\n",
    "                    \n",
    "                    for loc_selector in location_selectors:\n",
    "                        try:\n",
    "                            if loc_selector.startswith(\"//\"):\n",
    "                                location_elem = parent_card.find_element(By.XPATH, loc_selector)\n",
    "                            else:\n",
    "                                location_elem = parent_card.find_element(By.CSS_SELECTOR, loc_selector)\n",
    "                                \n",
    "                            if location_elem:\n",
    "                                location = location_elem.text.strip()\n",
    "                                if location and (\",\" in location or \"Remote\" in location):\n",
    "                                    break\n",
    "                        except:\n",
    "                            continue\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Could not find location with primary selectors: {e}\")\n",
    "                    # Fallback: look for any element that mentions location\n",
    "                    try:\n",
    "                        # Look for elements after the title with location formatting \n",
    "                        location_candidates = driver.find_elements(By.XPATH, \n",
    "                            f\"//div[contains(text(), '{title}')]/following::div[contains(text(), ',') or contains(text(), 'Remote')]\")\n",
    "                        if location_candidates:\n",
    "                            location = location_candidates[0].text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "            elif used_selector[\"type\"] in [\"row\", \"list\", \"link\"]:  # Generic extraction for other selectors\n",
    "                # Find title element\n",
    "                title_elem = None\n",
    "                title_selectors = [\"a\", \"h3\", \"[title]\", \"[aria-label]\", \"span.css-srrtrq\"]\n",
    "                \n",
    "                for t_selector in title_selectors:\n",
    "                    try:\n",
    "                        title_candidates = job.find_elements(By.CSS_SELECTOR, t_selector)\n",
    "                        for elem in title_candidates:\n",
    "                            text = elem.text.strip() or elem.get_attribute(\"title\") or elem.get_attribute(\"aria-label\")\n",
    "                            if text and len(text) > 3:  # Ensure it's substantial text\n",
    "                                title_elem = elem\n",
    "                                break\n",
    "                        if title_elem:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                if not title_elem:\n",
    "                    logger.debug(f\"Could not find title for job {index+1}\")\n",
    "                    continue\n",
    "                    \n",
    "                title = title_elem.text.strip() or title_elem.get_attribute(\"title\") or title_elem.get_attribute(\"aria-label\")\n",
    "                link = title_elem.get_attribute(\"href\")\n",
    "                \n",
    "                if not link:\n",
    "                    # Try to find a parent or sibling with a link\n",
    "                    link_containers = job.find_elements(By.CSS_SELECTOR, \"a\")\n",
    "                    if link_containers:\n",
    "                        link = link_containers[0].get_attribute(\"href\")\n",
    "                \n",
    "                # Try to find location\n",
    "                location = \"Not specified\"\n",
    "                try:\n",
    "                    # Look for location in various ways\n",
    "                    location_patterns = [\n",
    "                        \".//span[contains(text(), ',')]\",\n",
    "                        \".//div[contains(text(), ',')]\",\n",
    "                        \".//div[contains(text(), 'Location')]//following-sibling::div\",\n",
    "                        \".//span[contains(text(), 'Remote')]\",\n",
    "                        \".//div[contains(@class, 'location')]\"\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in location_patterns:\n",
    "                        location_elems = job.find_elements(By.XPATH, pattern)\n",
    "                        if location_elems:\n",
    "                            location = location_elems[0].text.strip()\n",
    "                            if location and len(location) > 2:  # Ensure it's not empty\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Could not find location with alternative selectors: {e}\")\n",
    "            \n",
    "            # Add the extracted job if we have at least a title and link\n",
    "            if title and title.strip() and link and link.strip():\n",
    "                # Check for duplicate before adding\n",
    "                is_duplicate = False\n",
    "                for existing_job in jobs_data:\n",
    "                    if existing_job[\"Title\"] == title and existing_job[\"Link\"] == link:\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                \n",
    "                if not is_duplicate:\n",
    "                    jobs_data.append({\n",
    "                        \"Title\": title,\n",
    "                        \"Location\": location,\n",
    "                        \"Link\": link,\n",
    "                        \"Description\": \"\",  # We'll get descriptions in a separate step\n",
    "                        \"Company\": \"Intel\"\n",
    "                    })\n",
    "                    logger.info(f\"Added job: {title} at {location}\")\n",
    "            \n",
    "        except (StaleElementReferenceException, Exception) as e:\n",
    "            logger.error(f\"Error extracting job details for job {index+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return jobs_data\n",
    "\n",
    "# Improved function to get job descriptions with full page screenshots saved in a separate folder\n",
    "def get_job_descriptions(driver, jobs_data, max_descriptions=100):\n",
    "    \"\"\"\n",
    "    Get job descriptions for a batch of jobs by visiting their individual pages.\n",
    "    Save full page screenshots of each job description in a separate folder.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Getting descriptions for {len(jobs_data)} jobs (up to {max_descriptions})\")\n",
    "    \n",
    "    # Create a dedicated folder for job description screenshots\n",
    "    job_desc_folder = 'job_description_screenshots_intel'\n",
    "    os.makedirs(job_desc_folder, exist_ok=True)\n",
    "    \n",
    "    # Store current URL to return to afterward\n",
    "    original_url = driver.current_url\n",
    "    original_window = driver.current_window_handle\n",
    "    \n",
    "    # Process up to max_descriptions\n",
    "    for i, job in enumerate(jobs_data[:max_descriptions]):\n",
    "        if not job.get(\"Link\"):\n",
    "            continue\n",
    "            \n",
    "        logger.info(f\"Getting description for job {i+1}/{min(len(jobs_data), max_descriptions)}: {job['Title']}\")\n",
    "        \n",
    "        # Create a clean filename from the job title\n",
    "        clean_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", job['Title'])\n",
    "        clean_title = re.sub(r'\\s+', \"_\", clean_title)\n",
    "        clean_title = clean_title[:100] if len(clean_title) > 100 else clean_title\n",
    "        \n",
    "        # Create a new tab for each job\n",
    "        try:\n",
    "            # Open new tab\n",
    "            driver.execute_script(\"window.open('about:blank', '_blank');\")\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            \n",
    "            # Navigate to job details page\n",
    "            driver.get(job[\"Link\"])\n",
    "            time.sleep(5)  # Wait for page to load\n",
    "            \n",
    "            # Take a full page screenshot\n",
    "            # First, get the height of the entire page\n",
    "            total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            # Set window size to capture entire page\n",
    "            driver.set_window_size(1920, total_height)\n",
    "            \n",
    "            # Take screenshot and save in the dedicated folder\n",
    "            screenshot_file = f\"{job_desc_folder}/job_{i+1}_{clean_title}.png\"\n",
    "            driver.save_screenshot(screenshot_file)\n",
    "            logger.info(f\"Saved full-page screenshot to {screenshot_file}\")\n",
    "            \n",
    "            # For long pages, also capture screenshots of each section\n",
    "            current_height = 0\n",
    "            viewport_height = 1080\n",
    "            section = 1\n",
    "            \n",
    "            while current_height < total_height:\n",
    "                driver.execute_script(f\"window.scrollTo(0, {current_height});\")\n",
    "                time.sleep(0.5)\n",
    "                section_screenshot = f\"{job_desc_folder}/job_{i+1}_{clean_title}_section_{section}.png\"\n",
    "                driver.save_screenshot(section_screenshot)\n",
    "                current_height += viewport_height\n",
    "                section += 1\n",
    "                \n",
    "            # Reset scroll position\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            \n",
    "            # Extract description using multiple potential selectors\n",
    "            description_selectors = [\n",
    "                \"[data-automation-id='job-description']\",\n",
    "                \".job-description\",\n",
    "                \"#job-description\",\n",
    "                \"[role='main']\",\n",
    "                \"article\",\n",
    "                \"[data-automation-id='jobPosting']\",\n",
    "                \".css-vh281m\",\n",
    "                \".css-1prfaxn\"\n",
    "            ]\n",
    "            \n",
    "            description = \"\"\n",
    "            for selector in description_selectors:\n",
    "                try:\n",
    "                    desc_elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if desc_elements:\n",
    "                        description = desc_elements[0].text.strip()\n",
    "                        if description:\n",
    "                            logger.info(f\"Got description for '{job['Title']}' ({len(description)} chars)\")\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Selector {selector} failed: {e}\")\n",
    "            \n",
    "            # Update the job with the description\n",
    "            if description:\n",
    "                job[\"Description\"] = description\n",
    "            \n",
    "            # Close tab\n",
    "            driver.close()\n",
    "            driver.switch_to.window(original_window)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting description for {job['Title']}: {e}\")\n",
    "            # Make sure we're back to the original window\n",
    "            try:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(original_window)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Return to original page\n",
    "    try:\n",
    "        driver.get(original_url)\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    logger.info(f\"Completed fetching descriptions for {min(len(jobs_data), max_descriptions)} jobs\")\n",
    "    return jobs_data\n",
    "\n",
    "# Function to handle different types of popups\n",
    "def handle_popups(driver):\n",
    "    try:\n",
    "        # Common buttons for accepting cookies, terms, etc.\n",
    "        popup_selectors = [\n",
    "            \"//button[contains(text(), 'Accept')]\", \n",
    "            \"//button[contains(text(), 'I agree')]\",\n",
    "            \"//button[contains(@id, 'accept')]\",\n",
    "            \"//button[contains(@class, 'accept')]\",\n",
    "            \"//button[contains(text(), 'Continue')]\",\n",
    "            \"//button[contains(text(), 'Got it')]\",\n",
    "            \"//button[contains(text(), 'Close')]\",\n",
    "            \"//button[@aria-label='Close']\",\n",
    "            \"//div[contains(@class, 'cookie')]//button\",\n",
    "            \"//div[contains(@id, 'consent')]//button\"\n",
    "        ]\n",
    "        \n",
    "        for xpath in popup_selectors:\n",
    "            try:\n",
    "                buttons = driver.find_elements(By.XPATH, xpath)\n",
    "                for button in buttons:\n",
    "                    if button.is_displayed():\n",
    "                        button.click()\n",
    "                        logger.info(f\"Clicked popup/cookie button with xpath: {xpath}\")\n",
    "                        time.sleep(1)\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "        # Handle alerts\n",
    "        try:\n",
    "            alert = Alert(driver)\n",
    "            alert.accept()\n",
    "            logger.info(\"Accepted alert popup\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error handling popups: {e}\")\n",
    "\n",
    "# Function to validate URL\n",
    "def is_valid_link(url):\n",
    "    if not url or not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        return False\n",
    "        \n",
    "    # For Intel Workday links, we'll assume they're valid without checking\n",
    "    if \"intel.wd1.myworkdayjobs.com\" in url:\n",
    "        return True\n",
    "        \n",
    "    try:\n",
    "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
    "        return response.status_code < 400  # Accept any non-error status\n",
    "    except requests.RequestException:\n",
    "        logger.warning(f\"Invalid link: {url}\")\n",
    "        return False\n",
    "\n",
    "# Main Intel job scraper function\n",
    "def scrape_intel_jobs(search_keyword=\"\", max_pages=20, headless=False):\n",
    "    \"\"\"\n",
    "    Scrape Intel jobs with comprehensive approach including pagination.\n",
    "    The search_keyword is used only for searching on the website.\n",
    "    All found jobs will be saved regardless of keyword match.\n",
    "    \n",
    "    Parameters:\n",
    "    search_keyword (str): Keyword to search for (empty string for all jobs)\n",
    "    max_pages (int): Maximum number of pages to scrape\n",
    "    headless (bool): Whether to run in headless mode\n",
    "    \n",
    "    Returns:\n",
    "    list: List of all job dictionaries found\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "        \n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36')\n",
    "    \n",
    "    # Create directories for debugging\n",
    "    os.makedirs('screenshots', exist_ok=True)\n",
    "    \n",
    "    driver = None\n",
    "    jobs_data = []\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        \n",
    "        # Construct search URL - Intel Workday URL\n",
    "        search_term = search_keyword.replace(\" \", \"%20\") if search_keyword else \"\"\n",
    "        if search_term:\n",
    "            base_url = f\"https://intel.wd1.myworkdayjobs.com/External?q={search_term}\"\n",
    "        else:\n",
    "            base_url = \"https://intel.wd1.myworkdayjobs.com/External\"\n",
    "\n",
    "        # Open the Intel careers page\n",
    "        logger.info(f\"Scraping jobs from Intel\" + (f\", searching for '{search_keyword}'\" if search_keyword else \"\"))\n",
    "        driver.get(base_url)\n",
    "        driver.save_screenshot(\"screenshots/intel_initial.png\")\n",
    "        \n",
    "        # Handle popups\n",
    "        handle_popups(driver)\n",
    "        \n",
    "        # Wait for results to load - trying different possible selectors\n",
    "        if not wait_for_job_listings(driver, timeout=20):\n",
    "            logger.warning(\"Could not detect job search results loading. Trying manual search...\")\n",
    "            \n",
    "            # Try to search directly by submitting a search form\n",
    "            try:\n",
    "                search_box = driver.find_element(By.CSS_SELECTOR, \"input[type='search'], input[placeholder*='Search']\")\n",
    "                search_box.clear()\n",
    "                search_box.send_keys(search_keyword)\n",
    "                search_box.send_keys(Keys.RETURN)\n",
    "                time.sleep(5)\n",
    "                logger.info(\"Tried manual search submission\")\n",
    "                driver.save_screenshot(\"screenshots/intel_manual_search.png\")\n",
    "                \n",
    "                # Wait again for results\n",
    "                wait_for_job_listings(driver, timeout=15)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Manual search also failed: {e}\")\n",
    "        \n",
    "        # Scroll to load all results on first page\n",
    "        job_count = scroll_to_load_all(driver, max_scrolls=20, wait_time=3)\n",
    "        driver.save_screenshot(\"screenshots/intel_after_scroll.png\")\n",
    "        \n",
    "        # Handle pagination and collect all jobs\n",
    "        all_jobs = handle_pagination(driver, max_pages=max_pages)\n",
    "        logger.info(f\"Collected {len(all_jobs)} total jobs after pagination\")\n",
    "        \n",
    "        # Get job descriptions for all collected jobs\n",
    "        jobs_with_descriptions = get_job_descriptions(driver, all_jobs, max_descriptions=200)\n",
    "        \n",
    "        # Save all jobs regardless of search keyword\n",
    "        jobs_data = jobs_with_descriptions\n",
    "        logger.info(f\"Saving all {len(jobs_data)} jobs found in search results\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping from Intel: {e}\")\n",
    "        if driver:\n",
    "            driver.save_screenshot(\"screenshots/intel_error.png\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "    return jobs_data\n",
    "\n",
    "# Main function to scrape and save results to CSV\n",
    "def main(search_keyword=\"\", max_pages=20, headless=False):\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting Intel job scraper at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Create screenshots directory\n",
    "    os.makedirs('screenshots', exist_ok=True)\n",
    "    \n",
    "    # Create job description screenshots directory\n",
    "    os.makedirs('job_description_screenshots_intel', exist_ok=True)\n",
    "    \n",
    "    # Scrape Intel jobs\n",
    "    jobs_data = scrape_intel_jobs(search_keyword, max_pages, headless)\n",
    "    \n",
    "    # Generate filenames with timestamps\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    keyword_slug = search_keyword.replace(' ', '_') if search_keyword else 'all'\n",
    "    detailed_filename = f\"intel_jobs_detailed_{keyword_slug}_{timestamp}.csv\"\n",
    "    simple_filename = f\"intel_jobs_simple_{keyword_slug}_{timestamp}.csv\"\n",
    "    \n",
    "    # Save results\n",
    "    if jobs_data:\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(jobs_data)\n",
    "        \n",
    "        # Save detailed CSV with descriptions\n",
    "        df.to_csv(detailed_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Detailed jobs saved to '{detailed_filename}'\")\n",
    "        \n",
    "        # Create and save a simplified CSV without descriptions\n",
    "        simple_df = df[['Title', 'Location', 'Link']].copy()\n",
    "        simple_df.to_csv(simple_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Simplified jobs list saved to '{simple_filename}'\")\n",
    "        \n",
    "        # Also create a filtered file if a search keyword was provided\n",
    "        if search_keyword:\n",
    "            filtered_df = df[df['Title'].str.contains(search_keyword, case=False)].copy()\n",
    "            filtered_filename = f\"intel_jobs_filtered_{keyword_slug}_{timestamp}.csv\"\n",
    "            filtered_df.to_csv(filtered_filename, index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Jobs filtered by keyword '{search_keyword}' saved to '{filtered_filename}'\")\n",
    "            logger.info(f\"Found {len(filtered_df)} jobs matching the keyword out of {len(df)} total jobs\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Intel Job Scraping Results:\")\n",
    "        print(f\"Total jobs found: {len(df)}\")\n",
    "        print(f\"Unique locations: {len(df['Location'].unique())}\")\n",
    "        print(f\"Sample jobs:\")\n",
    "        print(df[['Title', 'Location']].head())\n",
    "        print(\"\\nTop locations:\")\n",
    "        print(df['Location'].value_counts().head())\n",
    "        print(f\"\\nResults saved to:\")\n",
    "        print(f\"- {detailed_filename}\")\n",
    "        print(f\"- {simple_filename}\")\n",
    "        if search_keyword:\n",
    "            print(f\"- {filtered_filename}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        logger.warning(f\"No jobs found with search keyword '{search_keyword}'\")\n",
    "        print(\"\\nNo jobs found to display.\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Process completed with no results in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Intel Job Scraper\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get user input\n",
    "    job_title = input(\"Enter job title to search for (leave empty to get all jobs): \").strip()\n",
    "    \n",
    "    try:\n",
    "        max_pages = int(input(\"Maximum number of pages to scrape (default 10): \") or \"10\")\n",
    "    except ValueError:\n",
    "        max_pages = 10\n",
    "        print(\"Invalid input. Using default of 10 pages.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    headless_mode = input(\"Run in headless mode? (y/n, default: n): \").strip().lower() == 'y'\n",
    "    \n",
    "    print(\"\\nStarting job scraper...\")\n",
    "    print(\"This may take several minutes depending on the number of jobs and pages.\")\n",
    "    print(\"Progress will be logged to the console and a log file.\")\n",
    "    \n",
    "    # Run the scraper\n",
    "    main(job_title, max_pages, headless_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48087a7e-2768-4a4e-9785-f72748413278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel Job Detail Extractor\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter '1' to use a CSV file with job links or '2' to enter job URLs manually:  1\n",
      "Enter the path to your CSV file with job links:  intel_jobs_simple_Machine_learning_20250320_161815.csv\n",
      "Maximum number of jobs to process (leave empty for all):  \n",
      "Run in headless mode? (y/n, default: y):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:45:31,140 - INFO - Starting Intel job detail extractor at 2025-03-20 16:45:31\n",
      "2025-03-20 16:45:31,165 - INFO - Loaded 20 job links from intel_jobs_simple_Machine_learning_20250320_161815.csv\n",
      "2025-03-20 16:45:31,166 - INFO - Processing 20 job links\n",
      "2025-03-20 16:45:31,167 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting job detail extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:45:31,471 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-20 16:45:31,575 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-20 16:45:31,679 - INFO - Driver [/Users/srikar/.wdm/drivers/chromedriver/mac64/134.0.6998.90/chromedriver-mac-arm64/chromedriver] found in cache\n",
      "2025-03-20 16:45:32,746 - INFO - Processing job 1/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Malaysia-Penang/Machine-Learning-Engineer_JR0271988-1?q=Machine+learning\n",
      "2025-03-20 16:45:32,748 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Malaysia-Penang/Machine-Learning-Engineer_JR0271988-1?q=Machine+learning\n",
      "2025-03-20 16:45:37,173 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:45:39,179 - INFO - Processing job 2/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Costa-Rica-San-Jose/Full-Stack-Software-Developer---Machine-Learning-Engineer_JR0269204-1?q=Machine+learning\n",
      "2025-03-20 16:45:39,181 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Costa-Rica-San-Jose/Full-Stack-Software-Developer---Machine-Learning-Engineer_JR0269204-1?q=Machine+learning\n",
      "2025-03-20 16:45:43,293 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:45:45,299 - INFO - Processing job 3/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Virtual-US/Foundational-AI-Research-Scientist-Intel-contract-employee_JR0273082?q=Machine+learning\n",
      "2025-03-20 16:45:45,301 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Virtual-US/Foundational-AI-Research-Scientist-Intel-contract-employee_JR0273082?q=Machine+learning\n",
      "2025-03-20 16:45:48,785 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:45:50,791 - INFO - Processing job 4/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/US-Oregon-Hillsboro/AI-SW-Architect_JR0273675?q=Machine+learning\n",
      "2025-03-20 16:45:50,793 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/US-Oregon-Hillsboro/AI-SW-Architect_JR0273675?q=Machine+learning\n",
      "2025-03-20 16:45:54,031 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:45:56,037 - INFO - Processing job 5/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Tel-Aviv/AI-Software-Tools-Team-Leader_JR0270352?q=Machine+learning\n",
      "2025-03-20 16:45:56,038 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Tel-Aviv/AI-Software-Tools-Team-Leader_JR0270352?q=Machine+learning\n",
      "2025-03-20 16:45:59,244 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:46:01,250 - INFO - Processing job 6/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/NPU-Compiler-Developer--C---_JR0270522?q=Machine+learning\n",
      "2025-03-20 16:46:01,252 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/NPU-Compiler-Developer--C---_JR0270522?q=Machine+learning\n",
      "2025-03-20 16:46:17,586 - WARNING - Timeout waiting for job page to load: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/NPU-Compiler-Developer--C---_JR0270522?q=Machine+learning\n",
      "2025-03-20 16:46:17,885 - INFO - Successfully extracted details for: \n",
      "2025-03-20 16:46:19,891 - INFO - Processing job 7/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/AI-SW-Performance-Engineer_JR0269619-1?q=Machine+learning\n",
      "2025-03-20 16:46:19,892 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/AI-SW-Performance-Engineer_JR0269619-1?q=Machine+learning\n",
      "2025-03-20 16:46:24,368 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:46:26,374 - INFO - Processing job 8/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/NPU-SW-HW-Validation-Engineer--Power-Measurements-_JR0272351?q=Machine+learning\n",
      "2025-03-20 16:46:26,376 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/NPU-SW-HW-Validation-Engineer--Power-Measurements-_JR0272351?q=Machine+learning\n",
      "2025-03-20 16:46:30,528 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:46:32,534 - INFO - Processing job 9/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Romania-Timisoara/AI-Software-development-engineer_JR0273480?q=Machine+learning\n",
      "2025-03-20 16:46:32,535 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Romania-Timisoara/AI-Software-development-engineer_JR0273480?q=Machine+learning\n",
      "2025-03-20 16:46:36,003 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:46:38,008 - INFO - Processing job 10/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Virtual-US/Foundational-AI-Research-Intern_JR0271247?q=Machine+learning\n",
      "2025-03-20 16:46:38,009 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Virtual-US/Foundational-AI-Research-Intern_JR0271247?q=Machine+learning\n",
      "2025-03-20 16:46:41,185 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:46:43,191 - INFO - Processing job 11/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Caesarea/SW-Embedded--Kernel--Team-Leader_JR0267041?q=Machine+learning\n",
      "2025-03-20 16:46:43,193 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Caesarea/SW-Embedded--Kernel--Team-Leader_JR0267041?q=Machine+learning\n",
      "2025-03-20 16:46:46,397 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:46:48,403 - INFO - Processing job 12/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Tel-Aviv/Formal-Engineer_JR0266588-1?q=Machine+learning\n",
      "2025-03-20 16:46:48,405 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Tel-Aviv/Formal-Engineer_JR0266588-1?q=Machine+learning\n",
      "2025-03-20 16:46:51,890 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:46:53,896 - INFO - Processing job 13/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Tel-Aviv/Experienced-DFT-Engineer_JR0272026?q=Machine+learning\n",
      "2025-03-20 16:46:53,898 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Tel-Aviv/Experienced-DFT-Engineer_JR0272026?q=Machine+learning\n",
      "2025-03-20 16:46:57,116 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:46:59,121 - INFO - Processing job 14/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Virtual-US/Research-Scientist-Intern---ML-in-Graphics_JR0273359-1?q=Machine+learning\n",
      "2025-03-20 16:46:59,123 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Virtual-US/Research-Scientist-Intern---ML-in-Graphics_JR0273359-1?q=Machine+learning\n",
      "2025-03-20 16:47:03,126 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:47:05,132 - INFO - Processing job 15/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/AI-Software-Solutions-Engineer_JR0270874?q=Machine+learning\n",
      "2025-03-20 16:47:05,133 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/AI-Software-Solutions-Engineer_JR0270874?q=Machine+learning\n",
      "2025-03-20 16:47:08,581 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:47:10,587 - INFO - Processing job 16/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/AI-Software-Solutions-Engineer_JR0270875-1?q=Machine+learning\n",
      "2025-03-20 16:47:10,589 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Poland-Gdansk/AI-Software-Solutions-Engineer_JR0270875-1?q=Machine+learning\n",
      "2025-03-20 16:47:13,996 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:47:16,002 - INFO - Processing job 17/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Haifa/Software-Engineering-Manager_JR0271806?q=Machine+learning\n",
      "2025-03-20 16:47:16,004 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Israel-Haifa/Software-Engineering-Manager_JR0271806?q=Machine+learning\n",
      "2025-03-20 16:47:19,952 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:47:21,958 - INFO - Processing job 18/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/India-Bangalore/AI-ML-Technologist_JR0272095?q=Machine+learning\n",
      "2025-03-20 16:47:21,960 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/India-Bangalore/AI-ML-Technologist_JR0272095?q=Machine+learning\n",
      "2025-03-20 16:47:25,733 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:47:27,739 - INFO - Processing job 19/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Virtual-Costa-Rica/Sr-Data-Governance-Advisor_JR0273213?q=Machine+learning\n",
      "2025-03-20 16:47:27,741 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/Virtual-Costa-Rica/Sr-Data-Governance-Advisor_JR0273213?q=Machine+learning\n",
      "2025-03-20 16:47:31,030 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:47:33,036 - INFO - Processing job 20/20: https://intel.wd1.myworkdayjobs.com/en-US/External/job/India-Bangalore/AI-Software-Simulation-Engineer_JR0273501?q=Machine+learning\n",
      "2025-03-20 16:47:33,039 - INFO - Extracting details from: https://intel.wd1.myworkdayjobs.com/en-US/External/job/India-Bangalore/AI-Software-Simulation-Engineer_JR0273501?q=Machine+learning\n",
      "2025-03-20 16:47:36,925 - INFO - Successfully extracted details for: Welcome!\n",
      "2025-03-20 16:47:39,162 - INFO - Detailed job information saved to 'intel_detailed_jobs_20250320_164739.csv'\n",
      "2025-03-20 16:47:39,164 - INFO - Completed in 128.02 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Intel Job Detail Extraction Results:\n",
      "Total jobs processed: 20\n",
      "Jobs with titles: 20\n",
      "Jobs with descriptions: 20\n",
      "Jobs with qualifications: 20\n",
      "\n",
      "Results saved to: intel_detailed_jobs_20250320_164739.csv\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(f\"intel_job_details_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_detailed_job_info(driver, job_url):\n",
    "    \"\"\"\n",
    "    Extract detailed job information from a specific job posting URL\n",
    "    \n",
    "    Parameters:\n",
    "    driver (WebDriver): Selenium WebDriver instance\n",
    "    job_url (str): URL of the job posting\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing detailed job information\n",
    "    \"\"\"\n",
    "    job_details = {\n",
    "        \"Title\": \"\",\n",
    "        \"Location\": \"\",\n",
    "        \"Job ID\": \"\",\n",
    "        \"Job Description\": \"\",\n",
    "        \"Qualifications\": \"\",\n",
    "        \"Education\": \"\",\n",
    "        \"Additional Requirements\": \"\",\n",
    "        \"Job Category\": \"\",\n",
    "        \"Job Type\": \"\",\n",
    "        \"Posted Date\": \"\",\n",
    "        \"URL\": job_url\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Extracting details from: {job_url}\")\n",
    "        \n",
    "        # Navigate to the job URL\n",
    "        driver.get(job_url)\n",
    "        \n",
    "        # Wait for the page to load - look for job title or description\n",
    "        try:\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"[data-automation-id='jobTitle'], .job-title, h1, [data-automation-id='job-description']\"\n",
    "                ))\n",
    "            )\n",
    "            time.sleep(2)  # Additional wait for dynamic content\n",
    "        except TimeoutException:\n",
    "            logger.warning(f\"Timeout waiting for job page to load: {job_url}\")\n",
    "            # Continue anyway, we might still be able to extract some data\n",
    "        \n",
    "        # Extract job title using multiple potential selectors\n",
    "        title_selectors = [\n",
    "            \"[data-automation-id='jobTitle']\",\n",
    "            \".job-title\", \n",
    "            \"h1.css-sods2i\", \n",
    "            \"h1.css-8xbvbj\",\n",
    "            \"h1\"\n",
    "        ]\n",
    "        \n",
    "        for selector in title_selectors:\n",
    "            try:\n",
    "                title_element = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                if title_element and title_element.text.strip():\n",
    "                    job_details[\"Title\"] = title_element.text.strip()\n",
    "                    logger.debug(f\"Found job title: {job_details['Title']}\")\n",
    "                    break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        \n",
    "        # Extract location information\n",
    "        location_selectors = [\n",
    "            \"[data-automation-id='location']\",\n",
    "            \"[data-automation-id='locationLabel']\",\n",
    "            \".css-11derj\",\n",
    "            \".css-13syd4k\",\n",
    "            \"[data-automation-id='jobLocation']\",\n",
    "            \"//span[contains(text(), 'Location')]/following-sibling::*\",\n",
    "            \"//div[contains(text(), 'Location')]/following-sibling::*\"\n",
    "        ]\n",
    "        \n",
    "        for selector in location_selectors:\n",
    "            try:\n",
    "                if selector.startswith(\"//\"):\n",
    "                    location_element = driver.find_element(By.XPATH, selector)\n",
    "                else:\n",
    "                    location_element = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    \n",
    "                if location_element and location_element.text.strip():\n",
    "                    job_details[\"Location\"] = location_element.text.strip()\n",
    "                    logger.debug(f\"Found location: {job_details['Location']}\")\n",
    "                    break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        \n",
    "        # Extract job ID\n",
    "        job_id_selectors = [\n",
    "            \"[data-automation-id='requisitionNumber']\",\n",
    "            \".job-id\",\n",
    "            \"//span[contains(text(), 'Job ID')]/following-sibling::*\",\n",
    "            \"//div[contains(text(), 'Job ID')]/following-sibling::*\",\n",
    "            \"//span[contains(text(), 'Req ID')]/following-sibling::*\"\n",
    "        ]\n",
    "        \n",
    "        for selector in job_id_selectors:\n",
    "            try:\n",
    "                if selector.startswith(\"//\"):\n",
    "                    job_id_element = driver.find_element(By.XPATH, selector)\n",
    "                else:\n",
    "                    job_id_element = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    \n",
    "                if job_id_element and job_id_element.text.strip():\n",
    "                    job_details[\"Job ID\"] = job_id_element.text.strip()\n",
    "                    logger.debug(f\"Found job ID: {job_details['Job ID']}\")\n",
    "                    break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        \n",
    "        # Extract job description - try multiple selectors\n",
    "        description_selectors = [\n",
    "            \"[data-automation-id='job-description']\",\n",
    "            \".job-description\",\n",
    "            \"#job-description\",\n",
    "            \"[data-automation-id='jobDescription']\",\n",
    "            \".css-1prfaxn\",\n",
    "            \".css-vh281m\"\n",
    "        ]\n",
    "        \n",
    "        for selector in description_selectors:\n",
    "            try:\n",
    "                description_element = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                if description_element and description_element.text.strip():\n",
    "                    # Found the main job description container\n",
    "                    full_description = description_element.text.strip()\n",
    "                    \n",
    "                    # Now we need to parse the description into sections\n",
    "                    # First, store the full description\n",
    "                    job_details[\"Job Description\"] = full_description\n",
    "                    \n",
    "                    # Look for section titles within the description\n",
    "                    # Many Workday job postings use specific formatting for sections\n",
    "                    try:\n",
    "                        # Try to find qualifications section by looking for headings or bold elements\n",
    "                        qualification_patterns = [\n",
    "                            \"//h2[contains(text(), 'Qualifications') or contains(text(), 'Requirements')]/following-sibling::*\",\n",
    "                            \"//h3[contains(text(), 'Qualifications') or contains(text(), 'Requirements')]/following-sibling::*\",\n",
    "                            \"//strong[contains(text(), 'Qualifications') or contains(text(), 'Requirements')]/following::*\",\n",
    "                            \"//b[contains(text(), 'Qualifications') or contains(text(), 'Requirements')]/following::*\",\n",
    "                            \"//div[contains(text(), 'Qualifications') or contains(text(), 'Requirements')]/following::*\"\n",
    "                        ]\n",
    "                        \n",
    "                        for pattern in qualification_patterns:\n",
    "                            try:\n",
    "                                qual_elements = description_element.find_elements(By.XPATH, pattern)\n",
    "                                if qual_elements:\n",
    "                                    qual_text = \"\\n\".join([el.text.strip() for el in qual_elements[:5] if el.text.strip()])\n",
    "                                    if qual_text:\n",
    "                                        job_details[\"Qualifications\"] = qual_text\n",
    "                                        break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        # Try to extract education requirements similarly\n",
    "                        education_patterns = [\n",
    "                            \"//h2[contains(text(), 'Education')]/following-sibling::*\",\n",
    "                            \"//h3[contains(text(), 'Education')]/following-sibling::*\",\n",
    "                            \"//strong[contains(text(), 'Education')]/following::*\",\n",
    "                            \"//b[contains(text(), 'Education')]/following::*\",\n",
    "                            \"//div[contains(text(), 'Education')]/following::*\"\n",
    "                        ]\n",
    "                        \n",
    "                        for pattern in education_patterns:\n",
    "                            try:\n",
    "                                edu_elements = description_element.find_elements(By.XPATH, pattern)\n",
    "                                if edu_elements:\n",
    "                                    edu_text = \"\\n\".join([el.text.strip() for el in edu_elements[:3] if el.text.strip()])\n",
    "                                    if edu_text:\n",
    "                                        job_details[\"Education\"] = edu_text\n",
    "                                        break\n",
    "                            except:\n",
    "                                continue\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error parsing sections: {e}\")\n",
    "                    \n",
    "                    # If we haven't found structured sections, try to parse based on common patterns in the text\n",
    "                    if not job_details[\"Qualifications\"] and full_description:\n",
    "                        # Try to extract qualifications from the full text\n",
    "                        description_lines = full_description.split('\\n')\n",
    "                        current_section = None\n",
    "                        qualifications = []\n",
    "                        \n",
    "                        for line in description_lines:\n",
    "                            # Check if this is a section header\n",
    "                            if any(header in line.lower() for header in ['qualifications', 'requirements', 'minimum qualifications']):\n",
    "                                current_section = \"qualifications\"\n",
    "                                continue\n",
    "                            elif any(header in line.lower() for header in ['preferred qualifications', 'preferred skills']):\n",
    "                                current_section = \"additional\"\n",
    "                                continue\n",
    "                            elif any(header in line.lower() for header in ['education', 'degree']):\n",
    "                                current_section = \"education\" \n",
    "                                continue\n",
    "                            elif any(header in line.lower() for header in ['about intel', 'company', 'benefits']):\n",
    "                                current_section = None\n",
    "                                continue\n",
    "                            \n",
    "                            # Add line to the appropriate section\n",
    "                            if current_section == \"qualifications\" and line.strip():\n",
    "                                qualifications.append(line.strip())\n",
    "                        \n",
    "                        if qualifications:\n",
    "                            job_details[\"Qualifications\"] = \"\\n\".join(qualifications)\n",
    "                    \n",
    "                    break  # Exit the loop once we've processed a valid description\n",
    "                    \n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        \n",
    "        # Extract job category/type/posted date\n",
    "        metadata_selectors = [\n",
    "            \"//div[contains(@class, 'css-') and contains(@data-automation-id, 'job')]\",\n",
    "            \"//div[contains(@class, 'css-')]\"\n",
    "        ]\n",
    "        \n",
    "        for selector in metadata_selectors:\n",
    "            try:\n",
    "                metadata_containers = driver.find_elements(By.XPATH, selector)\n",
    "                for container in metadata_containers:\n",
    "                    text = container.text.strip()\n",
    "                    if not text:\n",
    "                        continue\n",
    "                        \n",
    "                    # Check for common metadata patterns\n",
    "                    if \"Job Category\" in text or \"Category\" in text:\n",
    "                        job_details[\"Job Category\"] = text.split(\":\")[-1].strip() if \":\" in text else text.replace(\"Job Category\", \"\").replace(\"Category\", \"\").strip()\n",
    "                    elif \"Job Type\" in text or \"Type\" in text:\n",
    "                        job_details[\"Job Type\"] = text.split(\":\")[-1].strip() if \":\" in text else text.replace(\"Job Type\", \"\").replace(\"Type\", \"\").strip()\n",
    "                    elif \"Posted Date\" in text or \"Date Posted\" in text:\n",
    "                        job_details[\"Posted Date\"] = text.split(\":\")[-1].strip() if \":\" in text else text.replace(\"Posted Date\", \"\").replace(\"Date Posted\", \"\").strip()\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # If we still don't have qualifications, try a more direct approach\n",
    "        if not job_details[\"Qualifications\"]:\n",
    "            try:\n",
    "                # Find any section that looks like a list of qualifications\n",
    "                list_items = driver.find_elements(By.CSS_SELECTOR, \"ul li, ol li\")\n",
    "                qualifications_items = []\n",
    "                \n",
    "                for item in list_items:\n",
    "                    text = item.text.strip()\n",
    "                    # Check if it looks like a qualification (contains keywords like experience, skills, etc.)\n",
    "                    if any(keyword in text.lower() for keyword in ['years', 'experience', 'degree', 'skill', 'proficiency', 'knowledge']):\n",
    "                        qualifications_items.append(text)\n",
    "                \n",
    "                if qualifications_items:\n",
    "                    job_details[\"Qualifications\"] = \"\\n\".join(qualifications_items)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        logger.info(f\"Successfully extracted details for: {job_details['Title']}\")\n",
    "        return job_details\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting job details from {job_url}: {e}\")\n",
    "        return job_details\n",
    "\n",
    "def process_job_links(job_links, max_jobs=None, headless=True):\n",
    "    \"\"\"\n",
    "    Process a list of job links to extract detailed information\n",
    "    \n",
    "    Parameters:\n",
    "    job_links (list): List of job URLs to process\n",
    "    max_jobs (int): Maximum number of jobs to process (None for all)\n",
    "    headless (bool): Whether to run browser in headless mode\n",
    "    \n",
    "    Returns:\n",
    "    list: List of dictionaries containing detailed job information\n",
    "    \"\"\"\n",
    "    if max_jobs:\n",
    "        job_links = job_links[:max_jobs]\n",
    "        \n",
    "    logger.info(f\"Processing {len(job_links)} job links\")\n",
    "    \n",
    "    # Set up webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "        \n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36')\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    \n",
    "    # Process each job link\n",
    "    detailed_jobs = []\n",
    "    for i, url in enumerate(job_links):\n",
    "        try:\n",
    "            logger.info(f\"Processing job {i+1}/{len(job_links)}: {url}\")\n",
    "            job_details = extract_detailed_job_info(driver, url)\n",
    "            detailed_jobs.append(job_details)\n",
    "            \n",
    "            # Brief pause between requests to avoid overloading the server\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing job link #{i+1} ({url}): {e}\")\n",
    "    \n",
    "    driver.quit()\n",
    "    return detailed_jobs\n",
    "\n",
    "def load_jobs_from_csv(csv_file):\n",
    "    \"\"\"\n",
    "    Load job links from a previously created CSV file\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    list: List of job URLs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if 'Link' in df.columns:\n",
    "            urls = df['Link'].tolist()\n",
    "            logger.info(f\"Loaded {len(urls)} job links from {csv_file}\")\n",
    "            return urls\n",
    "        else:\n",
    "            logger.error(f\"CSV file {csv_file} does not contain a 'Link' column\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading jobs from CSV {csv_file}: {e}\")\n",
    "        return []\n",
    "\n",
    "def main(input_file=None, job_links=None, max_jobs=None, headless=True):\n",
    "    \"\"\"\n",
    "    Main function to extract detailed job information\n",
    "    \n",
    "    Parameters:\n",
    "    input_file (str): Path to CSV file with job links\n",
    "    job_links (list): List of job URLs to process (alternative to input_file)\n",
    "    max_jobs (int): Maximum number of jobs to process\n",
    "    headless (bool): Whether to run browser in headless mode\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame containing detailed job information\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting Intel job detail extractor at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Get job links from either input file or provided list\n",
    "    urls_to_process = []\n",
    "    if input_file:\n",
    "        urls_to_process = load_jobs_from_csv(input_file)\n",
    "    elif job_links:\n",
    "        urls_to_process = job_links\n",
    "        logger.info(f\"Using provided list of {len(urls_to_process)} job links\")\n",
    "    \n",
    "    if not urls_to_process:\n",
    "        logger.error(\"No job links to process. Please provide a CSV file or a list of URLs.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Process jobs\n",
    "    detailed_jobs = process_job_links(urls_to_process, max_jobs, headless)\n",
    "    \n",
    "    # Save results\n",
    "    if detailed_jobs:\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(detailed_jobs)\n",
    "        \n",
    "        # Generate output filename with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_filename = f\"intel_detailed_jobs_{timestamp}.csv\"\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Detailed job information saved to '{output_filename}'\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Intel Job Detail Extraction Results:\")\n",
    "        print(f\"Total jobs processed: {len(detailed_jobs)}\")\n",
    "        print(f\"Jobs with titles: {df['Title'].notna().sum()}\")\n",
    "        print(f\"Jobs with descriptions: {df['Job Description'].notna().sum()}\")\n",
    "        print(f\"Jobs with qualifications: {df['Qualifications'].notna().sum()}\")\n",
    "        print(f\"\\nResults saved to: {output_filename}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        logger.warning(\"No detailed job information was extracted\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Process completed with no results in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Intel Job Detail Extractor\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get user input\n",
    "    input_mode = input(\"Enter '1' to use a CSV file with job links or '2' to enter job URLs manually: \").strip()\n",
    "    \n",
    "    if input_mode == '1':\n",
    "        file_path = input(\"Enter the path to your CSV file with job links: \").strip()\n",
    "        \n",
    "        try:\n",
    "            max_jobs = int(input(\"Maximum number of jobs to process (leave empty for all): \") or \"0\")\n",
    "            if max_jobs <= 0:\n",
    "                max_jobs = None\n",
    "        except ValueError:\n",
    "            max_jobs = None\n",
    "            \n",
    "        headless_mode = input(\"Run in headless mode? (y/n, default: y): \").strip().lower() != 'n'\n",
    "        \n",
    "        print(\"\\nStarting job detail extraction...\")\n",
    "        main(input_file=file_path, max_jobs=max_jobs, headless=headless_mode)\n",
    "        \n",
    "    elif input_mode == '2':\n",
    "        print(\"Enter job URLs (one per line). Enter an empty line when done:\")\n",
    "        job_urls = []\n",
    "        while True:\n",
    "            url = input().strip()\n",
    "            if not url:\n",
    "                break\n",
    "            job_urls.append(url)\n",
    "            \n",
    "        if not job_urls:\n",
    "            print(\"No URLs entered. Exiting.\")\n",
    "            exit()\n",
    "            \n",
    "        try:\n",
    "            max_jobs = int(input(\"Maximum number of jobs to process (leave empty for all): \") or \"0\")\n",
    "            if max_jobs <= 0:\n",
    "                max_jobs = None\n",
    "        except ValueError:\n",
    "            max_jobs = None\n",
    "            \n",
    "        headless_mode = input(\"Run in headless mode? (y/n, default: y): \").strip().lower() != 'n'\n",
    "        \n",
    "        print(\"\\nStarting job detail extraction...\")\n",
    "        main(job_links=job_urls, max_jobs=max_jobs, headless=headless_mode)\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid selection. Exiting.\")\n",
    "\n",
    "# Function to combine with previous job scraper \n",
    "def scrape_and_extract(search_keyword=\"\", max_pages=10, max_jobs=None, headless=True):\n",
    "  \n",
    "    from your_scraper_module import scrape_intel_jobs  # Import your existing scraper\n",
    "    \n",
    "    # First, scrape the job listings\n",
    "    jobs_list = scrape_intel_jobs(search_keyword, max_pages, headless)\n",
    "    \n",
    "    if not jobs_list:\n",
    "        logger.warning(\"No jobs found during scraping phase\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract links from the scraped jobs\n",
    "    job_links = [job[\"Link\"] for job in jobs_list if job.get(\"Link\")]\n",
    "    logger.info(f\"Extracted {len(job_links)} links from scraped jobs\")\n",
    "    \n",
    "    # Now extract detailed information\n",
    "    return main(job_links=job_links, max_jobs=max_jobs, headless=headless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46cd2f8e-6f47-4432-8cbe-057c3114b1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Qualifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Candidate must have at least 5+ years (Master'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Bachelors degree in Computer Science, Engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>PHD with 1+ years of research experience in El...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Technical Expertise: Knowledge of parallel pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>At least 8 years of experience with SW develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Optimize performance of AI models through deep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Experience dynamics of new technology developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Must be pursuing a PhD in Electrical Engineeri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Experience as a team leader\\nExperience with r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Pursuing a PhD degree.\\nRelevant research or s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>working experience with Python 3.10+,\\nexperie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>working experience with Python 3.10+,\\nexperie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Strong SW professional with at least 10 years ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Solid understanding of ML/DL techniques, algor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>Bachelor's Degree with 6+ years of relevant wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "      <td>This is for 2-7 years' experience range.\\nStro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  \\\n",
       "0   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "1   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "2   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "3   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "4   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "5   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "6   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "7   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "8   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "9   https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "10  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "11  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "12  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "13  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "14  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "15  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "16  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "17  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "18  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "19  https://intel.wd1.myworkdayjobs.com/en-US/Exte...   \n",
       "\n",
       "                                       Qualifications  \n",
       "0   Candidate must have at least 5+ years (Master'...  \n",
       "1   Bachelors degree in Computer Science, Engineer...  \n",
       "2   PHD with 1+ years of research experience in El...  \n",
       "3   Technical Expertise: Knowledge of parallel pro...  \n",
       "4   At least 8 years of experience with SW develop...  \n",
       "5                                                 NaN  \n",
       "6   Optimize performance of AI models through deep...  \n",
       "7   Experience dynamics of new technology developm...  \n",
       "8                                                 NaN  \n",
       "9   Must be pursuing a PhD in Electrical Engineeri...  \n",
       "10  Experience as a team leader\\nExperience with r...  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13  Pursuing a PhD degree.\\nRelevant research or s...  \n",
       "14  working experience with Python 3.10+,\\nexperie...  \n",
       "15  working experience with Python 3.10+,\\nexperie...  \n",
       "16  Strong SW professional with at least 10 years ...  \n",
       "17  Solid understanding of ML/DL techniques, algor...  \n",
       "18  Bachelor's Degree with 6+ years of relevant wo...  \n",
       "19  This is for 2-7 years' experience range.\\nStro...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=pd.read_csv('intel_detailed_jobs_20250320_164739.csv')\n",
    "data=dt[['URL','Qualifications']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feced2c3-f49a-4bcb-a225-dc258a726243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Stack Software Developer &amp; Machine Learni...</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foundational AI Research Scientist Intel contr...</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI SW Architect</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI Software Tools Team Leader</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NPU Compiler Developer (C++)</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AI SW Performance Engineer</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NPU SW/HW Validation Engineer (Power Measureme...</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AI Software development engineer</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Foundational AI Research Intern</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SW CI (Embedded) Team Leader</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Senior Formal Verification Engineer</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Experienced DFT Engineer</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Research Scientist Intern – ML in Graphics</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AI Software Solutions Engineer</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AI Software Solutions Engineer</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Software Engineering Team Lead</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AI/ML Technologist</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sr. Data Governance Advisor</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AI Software Simulation Engineer</td>\n",
       "      <td>© 2025 Workday, Inc. All rights reserved.</td>\n",
       "      <td>https://intel.wd1.myworkdayjobs.com/en-US/Exte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                           Machine Learning Engineer   \n",
       "1   Full Stack Software Developer & Machine Learni...   \n",
       "2   Foundational AI Research Scientist Intel contr...   \n",
       "3                                     AI SW Architect   \n",
       "4                       AI Software Tools Team Leader   \n",
       "5                        NPU Compiler Developer (C++)   \n",
       "6                          AI SW Performance Engineer   \n",
       "7   NPU SW/HW Validation Engineer (Power Measureme...   \n",
       "8                    AI Software development engineer   \n",
       "9                     Foundational AI Research Intern   \n",
       "10                       SW CI (Embedded) Team Leader   \n",
       "11                Senior Formal Verification Engineer   \n",
       "12                           Experienced DFT Engineer   \n",
       "13         Research Scientist Intern – ML in Graphics   \n",
       "14                     AI Software Solutions Engineer   \n",
       "15                     AI Software Solutions Engineer   \n",
       "16                     Software Engineering Team Lead   \n",
       "17                                 AI/ML Technologist   \n",
       "18                        Sr. Data Governance Advisor   \n",
       "19                    AI Software Simulation Engineer   \n",
       "\n",
       "                                     Location  \\\n",
       "0   © 2025 Workday, Inc. All rights reserved.   \n",
       "1   © 2025 Workday, Inc. All rights reserved.   \n",
       "2   © 2025 Workday, Inc. All rights reserved.   \n",
       "3   © 2025 Workday, Inc. All rights reserved.   \n",
       "4   © 2025 Workday, Inc. All rights reserved.   \n",
       "5   © 2025 Workday, Inc. All rights reserved.   \n",
       "6   © 2025 Workday, Inc. All rights reserved.   \n",
       "7   © 2025 Workday, Inc. All rights reserved.   \n",
       "8   © 2025 Workday, Inc. All rights reserved.   \n",
       "9   © 2025 Workday, Inc. All rights reserved.   \n",
       "10  © 2025 Workday, Inc. All rights reserved.   \n",
       "11  © 2025 Workday, Inc. All rights reserved.   \n",
       "12  © 2025 Workday, Inc. All rights reserved.   \n",
       "13  © 2025 Workday, Inc. All rights reserved.   \n",
       "14  © 2025 Workday, Inc. All rights reserved.   \n",
       "15  © 2025 Workday, Inc. All rights reserved.   \n",
       "16  © 2025 Workday, Inc. All rights reserved.   \n",
       "17  © 2025 Workday, Inc. All rights reserved.   \n",
       "18  © 2025 Workday, Inc. All rights reserved.   \n",
       "19  © 2025 Workday, Inc. All rights reserved.   \n",
       "\n",
       "                                                 Link  \n",
       "0   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "1   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "2   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "3   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "4   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "5   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "6   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "7   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "8   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "9   https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "10  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "11  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "12  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "13  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "14  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "15  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "16  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "17  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "18  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  \n",
       "19  https://intel.wd1.myworkdayjobs.com/en-US/Exte...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('intel_jobs_simple_Machine_learning_20250320_161815.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddbbae-2844-49f5-840e-09959fefdacf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
