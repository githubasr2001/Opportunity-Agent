{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2bfa8c-fe11-45d8-b75a-3b8d420bc142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.11/site-packages/fonttools-4.53.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: PyPDF2 in /opt/anaconda3/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: pdf2image in /opt/anaconda3/lib/python3.11/site-packages (1.17.0)\n",
      "Requirement already satisfied: pytesseract in /opt/anaconda3/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.11/site-packages (10.2.0)\n",
      "Requirement already satisfied: python-docx in /opt/anaconda3/lib/python3.11/site-packages (1.1.2)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.7.5)\n",
      "Collecting keras\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/lib/python3.11/site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Collecting absl-py (from keras)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras) (13.3.5)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.11/site-packages (from keras) (3.9.0)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.5.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.65.1)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading h5py-3.13.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Downloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading tensorflow-2.18.0-cp311-cp311-macosx_12_0_arm64.whl (239.5 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/239.5 MB\u001b[0m \u001b[31m538.8 kB/s\u001b[0m eta \u001b[36m0:05:39\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/http/client.py\", line 473, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/ssl.py\", line 1166, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/req_command.py\", line 245, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/commands/install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 552, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 467, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/network/download.py\", line 183, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 pdf2image pytesseract Pillow python-docx spacy keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c5d53b-4316-47e4-ae9d-f1af40c5c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.11/site-packages/fonttools-4.53.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.65.1)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.13.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-macosx_12_0_arm64.whl (239.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-macosx_10_9_universal2.whl (397 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.0/398.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp311-cp311-macosx_11_0_arm64.whl (335 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, h5py, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 h5py-3.13.0 keras-3.9.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76cafbc4-bfbf-49a1-8000-3c1368e2b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations DataFrame:\n",
      "                                 resume                   job_title  \\\n",
      "0   Anudeep_MachineLearningEngineer.pdf           Financial Analyst   \n",
      "1   Anudeep_MachineLearningEngineer.pdf              Data Scientist   \n",
      "2   Anudeep_MachineLearningEngineer.pdf  Environmental Data Analyst   \n",
      "3   Anudeep_MachineLearningEngineer.pdf       AI Research Scientist   \n",
      "4   Anudeep_MachineLearningEngineer.pdf   Sustainability Consultant   \n",
      "5                              java.png       AI Research Scientist   \n",
      "6                              java.png         Structural Engineer   \n",
      "7                              java.png    Senior Software Engineer   \n",
      "8                              java.png       Senior Civil Engineer   \n",
      "9                              java.png           Financial Analyst   \n",
      "10                           ASR_SD.pdf           Financial Analyst   \n",
      "11                           ASR_SD.pdf              Data Scientist   \n",
      "12                           ASR_SD.pdf  Environmental Data Analyst   \n",
      "13                           ASR_SD.pdf       AI Research Scientist   \n",
      "14                           ASR_SD.pdf   Sustainability Consultant   \n",
      "15                     DATAENGINEER.png              Data Scientist   \n",
      "16                     DATAENGINEER.png  Environmental Data Analyst   \n",
      "17                     DATAENGINEER.png     Healthcare Data Analyst   \n",
      "18                     DATAENGINEER.png       AI Research Scientist   \n",
      "19                     DATAENGINEER.png   Sustainability Consultant   \n",
      "20         Software_Engineer_Resume.pdf           Financial Analyst   \n",
      "21         Software_Engineer_Resume.pdf              Data Scientist   \n",
      "22         Software_Engineer_Resume.pdf  Environmental Data Analyst   \n",
      "23         Software_Engineer_Resume.pdf       AI Research Scientist   \n",
      "24         Software_Engineer_Resume.pdf   Sustainability Consultant   \n",
      "25           Environmental_engineer.png           Financial Analyst   \n",
      "26           Environmental_engineer.png  Environmental Data Analyst   \n",
      "27           Environmental_engineer.png     Healthcare Data Analyst   \n",
      "28           Environmental_engineer.png       AI Research Scientist   \n",
      "29           Environmental_engineer.png   Sustainability Consultant   \n",
      "30                           ASR_DA.pdf           Financial Analyst   \n",
      "31                           ASR_DA.pdf              Data Scientist   \n",
      "32                           ASR_DA.pdf  Environmental Data Analyst   \n",
      "33                           ASR_DA.pdf     Healthcare Data Analyst   \n",
      "34                           ASR_DA.pdf       AI Research Scientist   \n",
      "\n",
      "                            company                 domain  \\\n",
      "0                       FinanceCorp                Finance   \n",
      "1                    DataDriven Co.           Data Science   \n",
      "2          GreenTech Sustainability  Environmental Science   \n",
      "3              DeepMind Innovations    AI/Machine Learning   \n",
      "4                         EcoFuture  Environmental Science   \n",
      "5              DeepMind Innovations    AI/Machine Learning   \n",
      "6                       BuildStrong      Civil Engineering   \n",
      "7            TechInnovate Solutions                     IT   \n",
      "8   Urban Infrastructure Developers      Civil Engineering   \n",
      "9                       FinanceCorp                Finance   \n",
      "10                      FinanceCorp                Finance   \n",
      "11                   DataDriven Co.           Data Science   \n",
      "12         GreenTech Sustainability  Environmental Science   \n",
      "13             DeepMind Innovations    AI/Machine Learning   \n",
      "14                        EcoFuture  Environmental Science   \n",
      "15                   DataDriven Co.           Data Science   \n",
      "16         GreenTech Sustainability  Environmental Science   \n",
      "17             HealthTech Solutions             Healthcare   \n",
      "18             DeepMind Innovations    AI/Machine Learning   \n",
      "19                        EcoFuture  Environmental Science   \n",
      "20                      FinanceCorp                Finance   \n",
      "21                   DataDriven Co.           Data Science   \n",
      "22         GreenTech Sustainability  Environmental Science   \n",
      "23             DeepMind Innovations    AI/Machine Learning   \n",
      "24                        EcoFuture  Environmental Science   \n",
      "25                      FinanceCorp                Finance   \n",
      "26         GreenTech Sustainability  Environmental Science   \n",
      "27             HealthTech Solutions             Healthcare   \n",
      "28             DeepMind Innovations    AI/Machine Learning   \n",
      "29                        EcoFuture  Environmental Science   \n",
      "30                      FinanceCorp                Finance   \n",
      "31                   DataDriven Co.           Data Science   \n",
      "32         GreenTech Sustainability  Environmental Science   \n",
      "33             HealthTech Solutions             Healthcare   \n",
      "34             DeepMind Innovations    AI/Machine Learning   \n",
      "\n",
      "           salary_range           location  match_score  \n",
      "0     $70,000 - $90,000        Chicago, IL         46.0  \n",
      "1   $100,000 - $130,000       New York, NY         45.0  \n",
      "2    $80,000 - $110,000        Boulder, CO         45.0  \n",
      "3   $140,000 - $180,000       San Jose, CA         44.0  \n",
      "4    $90,000 - $120,000       Portland, OR         44.0  \n",
      "5   $140,000 - $180,000       San Jose, CA         44.0  \n",
      "6    $85,000 - $115,000    Los Angeles, CA         43.0  \n",
      "7   $120,000 - $150,000  San Francisco, CA         42.0  \n",
      "8    $90,000 - $120,000        Chicago, IL         41.0  \n",
      "9     $70,000 - $90,000        Chicago, IL         26.0  \n",
      "10    $70,000 - $90,000        Chicago, IL         46.0  \n",
      "11  $100,000 - $130,000       New York, NY         45.0  \n",
      "12   $80,000 - $110,000        Boulder, CO         45.0  \n",
      "13  $140,000 - $180,000       San Jose, CA         44.0  \n",
      "14   $90,000 - $120,000       Portland, OR         44.0  \n",
      "15  $100,000 - $130,000       New York, NY         45.0  \n",
      "16   $80,000 - $110,000        Boulder, CO         45.0  \n",
      "17   $80,000 - $110,000         Boston, MA         45.0  \n",
      "18  $140,000 - $180,000       San Jose, CA         44.0  \n",
      "19   $90,000 - $120,000       Portland, OR         44.0  \n",
      "20    $70,000 - $90,000        Chicago, IL         46.0  \n",
      "21  $100,000 - $130,000       New York, NY         45.0  \n",
      "22   $80,000 - $110,000        Boulder, CO         45.0  \n",
      "23  $140,000 - $180,000       San Jose, CA         44.0  \n",
      "24   $90,000 - $120,000       Portland, OR         44.0  \n",
      "25    $70,000 - $90,000        Chicago, IL         46.0  \n",
      "26   $80,000 - $110,000        Boulder, CO         45.0  \n",
      "27   $80,000 - $110,000         Boston, MA         45.0  \n",
      "28  $140,000 - $180,000       San Jose, CA         44.0  \n",
      "29   $90,000 - $120,000       Portland, OR         44.0  \n",
      "30    $70,000 - $90,000        Chicago, IL         46.0  \n",
      "31  $100,000 - $130,000       New York, NY         45.0  \n",
      "32   $80,000 - $110,000        Boulder, CO         45.0  \n",
      "33   $80,000 - $110,000         Boston, MA         45.0  \n",
      "34  $140,000 - $180,000       San Jose, CA         44.0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import docx\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class BatchResumeRecommendationSystem:\n",
    "    def __init__(self, resume_folder, log_file='resume_recommendations.log'):\n",
    "        \"\"\"Initialize the recommendation system with a folder of resumes.\"\"\"\n",
    "        # Configure logging\n",
    "        logging.basicConfig(filename=log_file, level=logging.INFO, \n",
    "                            format='%(asctime)s - %(message)s')\n",
    "        \n",
    "        # Load spaCy for NLP\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        # Resume folder path\n",
    "        self.resume_folder = resume_folder\n",
    "        \n",
    "        # Expanded job listings with certifications requirements\n",
    "        self.job_listings = [\n",
    "            # IT Jobs\n",
    "            {'title': 'Senior Software Engineer', 'company': 'TechInnovate Solutions', 'domain': 'IT',\n",
    "             'skills': ['Python', 'Java', 'Machine Learning', 'AWS', 'Docker', 'DevOps'],\n",
    "             'experience': '5-7 years', 'salary_range': '$120,000 - $150,000', 'location': 'San Francisco, CA',\n",
    "             'certifications_required': ['aws certified', 'oracle certified']},\n",
    "            {'title': 'Cloud Solutions Architect', 'company': 'CloudTech Enterprises', 'domain': 'Cloud Computing',\n",
    "             'skills': ['AWS', 'Azure', 'Kubernetes', 'Terraform', 'DevOps'],\n",
    "             'experience': '4-6 years', 'salary_range': '$130,000 - $160,000', 'location': 'Seattle, WA',\n",
    "             'certifications_required': ['aws certified', 'azure certified', 'kubernetes certified']},\n",
    "            # AI/ML Jobs\n",
    "            {'title': 'AI Research Scientist', 'company': 'DeepMind Innovations', 'domain': 'AI/Machine Learning',\n",
    "             'skills': ['Python', 'TensorFlow', 'PyTorch', 'Deep Learning', 'NLP'],\n",
    "             'experience': '3-5 years', 'salary_range': '$140,000 - $180,000', 'location': 'San Jose, CA',\n",
    "             'certifications_required': ['tensorflow developer', 'pytorch expert']},\n",
    "            {'title': 'Data Scientist', 'company': 'DataDriven Co.', 'domain': 'Data Science',\n",
    "             'skills': ['Python', 'R', 'SQL', 'Machine Learning', 'Statistics'],\n",
    "             'experience': '2-4 years', 'salary_range': '$100,000 - $130,000', 'location': 'New York, NY',\n",
    "             'certifications_required': ['certified data scientist']},\n",
    "            # Environmental Jobs\n",
    "            {'title': 'Environmental Data Analyst', 'company': 'GreenTech Sustainability', 'domain': 'Environmental Science',\n",
    "             'skills': ['R', 'Python', 'Data Analysis', 'GIS', 'Environmental Modeling'],\n",
    "             'experience': '2-4 years', 'salary_range': '$80,000 - $110,000', 'location': 'Boulder, CO',\n",
    "             'certifications_required': ['leed accredited']},\n",
    "            {'title': 'Sustainability Consultant', 'company': 'EcoFuture', 'domain': 'Environmental Science',\n",
    "             'skills': ['Sustainability', 'Environmental Policy', 'Project Management', 'Data Analysis'],\n",
    "             'experience': '3-5 years', 'salary_range': '$90,000 - $120,000', 'location': 'Portland, OR',\n",
    "             'certifications_required': ['leed accredited', 'certified sustainability professional']},\n",
    "            # Java Jobs\n",
    "            {'title': 'Java Enterprise Architect', 'company': 'Enterprise Solutions Inc.', 'domain': 'Java Development',\n",
    "             'skills': ['Java', 'Spring Boot', 'Microservices', 'RESTful APIs', 'SQL'],\n",
    "             'experience': '6-8 years', 'salary_range': '$125,000 - $155,000', 'location': 'New York, NY',\n",
    "             'certifications_required': ['java certified']},\n",
    "            {'title': 'Java Developer', 'company': 'CodeCrafters', 'domain': 'Software Development',\n",
    "             'skills': ['Java', 'Spring', 'Hibernate', 'SQL', 'Agile'],\n",
    "             'experience': '3-5 years', 'salary_range': '$90,000 - $120,000', 'location': 'Austin, TX',\n",
    "             'certifications_required': ['java certified']},\n",
    "            # Civil Engineering Jobs\n",
    "            {'title': 'Senior Civil Engineer', 'company': 'Urban Infrastructure Developers', 'domain': 'Civil Engineering',\n",
    "             'skills': ['AutoCAD', 'Structural Design', 'Project Management', 'Civil 3D', 'BIM'],\n",
    "             'experience': '6-8 years', 'salary_range': '$90,000 - $120,000', 'location': 'Chicago, IL',\n",
    "             'certifications_required': ['pe license', 'civil engineering certification']},\n",
    "            {'title': 'Structural Engineer', 'company': 'BuildStrong', 'domain': 'Civil Engineering',\n",
    "             'skills': ['Structural Analysis', 'AutoCAD', 'Revit', 'Project Management'],\n",
    "             'experience': '4-6 years', 'salary_range': '$85,000 - $115,000', 'location': 'Los Angeles, CA',\n",
    "             'certifications_required': ['pe license']},\n",
    "            # Healthcare Jobs\n",
    "            {'title': 'Healthcare Data Analyst', 'company': 'HealthTech Solutions', 'domain': 'Healthcare',\n",
    "             'skills': ['SQL', 'Python', 'Data Visualization', 'Healthcare Analytics'],\n",
    "             'experience': '2-4 years', 'salary_range': '$80,000 - $110,000', 'location': 'Boston, MA',\n",
    "             'certifications_required': ['healthcare analytics certified']},\n",
    "            # Finance Jobs\n",
    "            {'title': 'Financial Analyst', 'company': 'FinanceCorp', 'domain': 'Finance',\n",
    "             'skills': ['Excel', 'Financial Modeling', 'Data Analysis', 'SQL'],\n",
    "             'experience': '1-3 years', 'salary_range': '$70,000 - $90,000', 'location': 'Chicago, IL',\n",
    "             'certifications_required': ['cfa', 'cpa']}\n",
    "        ]\n",
    "\n",
    "        self.domain_keywords = {\n",
    "            'IT': [\n",
    "                'software', 'technology', 'programming', 'coding', 'it', 'software development',\n",
    "                'cybersecurity', 'networking', 'system administration', 'development', 'devops',\n",
    "                'database management', 'tech support', 'information systems', 'IT support',\n",
    "                'IT infrastructure', 'information security', 'system integration', 'scripting',\n",
    "                'automation', 'virtualization', 'technical troubleshooting', 'data center', 'data management'\n",
    "            ],\n",
    "            'Cloud Computing': [\n",
    "                'cloud', 'aws', 'azure', 'google cloud', 'infrastructure as a service', 'iaas',\n",
    "                'platform as a service', 'paas', 'serverless', 'multi-cloud', 'hybrid cloud',\n",
    "                'cloud migration', 'cloud security', 'cloud architecture', 'cloud orchestration',\n",
    "                'cloud native', 'cloud management', 'containerization', 'kubernetes', 'docker', 'virtualization'\n",
    "            ],\n",
    "            'AI/Machine Learning': [\n",
    "                'machine learning', 'ai', 'data science', 'deep learning', 'python', 'llms',\n",
    "                'fine tuning', 'artificial intelligence', 'predictive analytics', 'big data',\n",
    "                'neural networks', 'nlp', 'computer vision', 'reinforcement learning',\n",
    "                'algorithm development', 'automation', 'scikit-learn', 'tensorflow', 'pytorch',\n",
    "                'keras', 'supervised learning', 'unsupervised learning', 'model optimization'\n",
    "            ],\n",
    "            'Data Science': [\n",
    "                'data science', 'data mining', 'analytics', 'r programming', 'statistical modeling',\n",
    "                'data visualization', 'data wrangling', 'predictive modeling', 'big data', 'python',\n",
    "                'sql', 'data engineering', 'regression analysis', 'data analysis', 'quantitative analysis',\n",
    "                'data cleaning', 'business intelligence', 'dashboards', 'tableau', 'power bi',\n",
    "                'data storytelling'\n",
    "            ],\n",
    "            'Environmental Science': [\n",
    "                'environment', 'sustainability', 'climate', 'ecology', 'conservation', 'green technology',\n",
    "                'waste management', 'water resources', 'environmental impact', 'environmental policy',\n",
    "                'environmental compliance', 'renewable energy', 'carbon footprint', 'environmental engineering',\n",
    "                'ecosystem management', 'natural resource management', 'environmental planning',\n",
    "                'hazardous waste', 'climate change', 'sustainable development', 'gis', 'remote sensing'\n",
    "            ],\n",
    "            'Civil Engineering': [\n",
    "                'civil engineering', 'construction', 'infrastructure', 'structural', 'design',\n",
    "                'transportation engineering', 'geotechnical', 'construction management', 'urban planning',\n",
    "                'project management', 'surveying', 'bridge design', 'road design', 'hydraulics', 'building codes',\n",
    "                'cad', 'bim', 'civil drafting', 'public works', 'municipal planning', 'traffic engineering',\n",
    "                'site planning', 'construction planning', 'structural analysis', 'contract management'\n",
    "            ],\n",
    "            'Healthcare': [\n",
    "                'healthcare', 'medical', 'patient', 'clinical', 'health', 'hospital', 'patient care',\n",
    "                'health informatics', 'nursing', 'diagnostics', 'treatment', 'medical research', 'pharmacy',\n",
    "                'biomedical', 'emr', 'electronic health records', 'public health', 'health services',\n",
    "                'healthcare administration', 'health management', 'surgery', 'emergency care',\n",
    "                'healthcare compliance', 'health information technology', 'health policy', 'care management'\n",
    "            ],\n",
    "            'Finance': [\n",
    "                'finance', 'financial', 'accounting', 'banking', 'investment', 'economics', 'financial modeling',\n",
    "                'risk management', 'budgeting', 'forecasting', 'auditing', 'capital markets', 'asset management',\n",
    "                'tax', 'credit analysis', 'quantitative finance', 'financial analysis', 'compliance',\n",
    "                'investment banking', 'wealth management', 'equity research', 'valuation', 'corporate finance',\n",
    "                'hedging', 'securities', 'derivatives', 'portfolio management', 'fiscal', 'strategic planning'\n",
    "            ],\n",
    "            'Marketing': [\n",
    "                'marketing', 'digital marketing', 'seo', 'content creation', 'social media', 'branding', 'advertising',\n",
    "                'market research', 'email marketing', 'inbound marketing', 'outbound marketing', 'analytics',\n",
    "                'public relations', 'search engine marketing', 'sem', 'ppc', 'lead generation', 'content marketing',\n",
    "                'copywriting', 'influencer marketing', 'brand management', 'marketing strategy', 'marketing automation',\n",
    "                'customer engagement', 'growth hacking', 'conversion rate optimization', 'online advertising',\n",
    "                'marketing communications'\n",
    "            ],\n",
    "            'Human Resources': [\n",
    "                'human resources', 'hr', 'recruitment', 'employee relations', 'talent management', 'organizational development',\n",
    "                'benefits administration', 'compensation', 'employee engagement', 'performance management',\n",
    "                'training and development', 'labor relations', 'hr compliance', 'workforce planning', 'hris',\n",
    "                'talent acquisition', 'onboarding', 'succession planning', 'hr analytics', 'employee retention',\n",
    "                'diversity and inclusion', 'staffing', 'recruitment strategy', 'conflict resolution', 'employment law'\n",
    "            ],\n",
    "            'ATS Keywords': [\n",
    "                'communication', 'teamwork', 'leadership', 'project management', 'strategic planning', 'problem solving',\n",
    "                'critical thinking', 'collaboration', 'innovation', 'results-driven', 'detail-oriented', 'self-motivated',\n",
    "                'adaptability', 'time management', 'multitasking', 'analytical skills', 'customer service', 'interpersonal skills',\n",
    "                'decision making', 'organizational skills', 'initiative', 'accountability'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Initialize neural network model (will be built after feature vector size is determined)\n",
    "        self.input_shape = None  \n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def extract_text_from_resume(self, file_path):\n",
    "        \"\"\"Extract text from resume files (PDF, DOCX, TXT, PNG, JPG).\"\"\"\n",
    "        try:\n",
    "            file_extension = os.path.splitext(file_path)[1].lower()\n",
    "            if file_extension == '.pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = ' '.join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "            elif file_extension == '.docx':\n",
    "                doc = docx.Document(file_path)\n",
    "                text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "            elif file_extension == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "            elif file_extension in ['.png', '.jpg', '.jpeg']:\n",
    "                image = Image.open(file_path)\n",
    "                text = pytesseract.image_to_string(image)\n",
    "            else:\n",
    "                logging.warning(f\"Unsupported file type: {file_extension} for {file_path}\")\n",
    "                return None\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting text from {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_resume_features(self, resume_text):\n",
    "        \"\"\"Extract features from resume text including skills, education, experience, domains, projects, and certifications.\"\"\"\n",
    "        resume_text = resume_text.lower()\n",
    "        doc = self.nlp(resume_text)\n",
    "        \n",
    "        features = {\n",
    "            'skills': [],\n",
    "            'education_level': None,\n",
    "            'experience_years': 0,\n",
    "            'domains': [],\n",
    "            'project_keywords': [],\n",
    "            'certifications': []\n",
    "        }\n",
    "        \n",
    "        # Expanded list of skill patterns with additional skills\n",
    "        skill_patterns = [\n",
    "            'python', 'java', 'c++', 'machine learning', 'data analysis', 'aws', 'azure', 'sql', 'project management',\n",
    "            'tensorflow', 'pytorch', 'cloud computing', 'spring', 'hibernate', 'autocad', 'revit', 'gis', 'r',\n",
    "            'statistics', 'excel', 'financial modeling', 'docker', 'devops', 'microservices', 'kubernetes', 'terraform',\n",
    "            'cybersecurity', 'network administration', 'sagemaker',\n",
    "            'javascript', 'html', 'css', 'react', 'node.js', 'flask', 'django', 'php', 'sql server', 'mongodb',\n",
    "            'c#', 'ruby', 'spark', 'hadoop', 'kotlin', 'swift', 'go', 'laravel', 'express.js'\n",
    "        ]\n",
    "        features['skills'] = [skill for skill in skill_patterns if skill in resume_text]\n",
    "        \n",
    "        # Education extraction based on keywords\n",
    "        education_keywords = {\n",
    "            'PhD': ['phd', 'doctorate'],\n",
    "            'Masters': ['master', 'ms', 'ma'],\n",
    "            'Bachelors': ['bachelor', 'bs', 'ba']\n",
    "        }\n",
    "        for level, keywords in education_keywords.items():\n",
    "            if any(keyword in resume_text for keyword in keywords):\n",
    "                features['education_level'] = level\n",
    "                break\n",
    "        \n",
    "        # Extract years of experience\n",
    "        experience_match = re.search(r'(\\d+)\\s*(?:years?|yrs?)\\s*(?:of)?\\s*experience', resume_text, re.IGNORECASE)\n",
    "        if experience_match:\n",
    "            features['experience_years'] = int(experience_match.group(1))\n",
    "        \n",
    "        # Domain extraction using expanded keywords\n",
    "        for domain, keywords in self.domain_keywords.items():\n",
    "            if any(keyword in resume_text for keyword in keywords):\n",
    "                features['domains'].append(domain)\n",
    "        \n",
    "        # Extract project keywords\n",
    "        project_keywords = re.findall(r'project\\s*:\\s*([^\\n]+)', resume_text, re.IGNORECASE)\n",
    "        features['project_keywords'] = project_keywords\n",
    "        \n",
    "        # Extract certifications based on common certification keywords\n",
    "        certification_keywords = [\n",
    "            'aws certified', 'cissp', 'oracle certified', 'microsoft certified', 'comptia', 'cisco certified',\n",
    "            'tensorflow developer', 'pytorch expert', 'certified data scientist', 'leed accredited',\n",
    "            'certified sustainability professional', 'java certified', 'pe license', 'civil engineering certification',\n",
    "            'healthcare analytics certified', 'cfa', 'cpa', 'scrum master','snow pro certification',\"CompTIA A+\",\n",
    "            \"CompTIA Network+\",\n",
    "            \"CompTIA Security+\",\n",
    "            \"CompTIA Server+\",\n",
    "            \"CompTIA Cloud+\",\n",
    "            \"Cisco CCNA\",\n",
    "            \"Cisco CCNP\",\n",
    "            \"Cisco CCIE\",\n",
    "            \"Microsoft MCSA\",\n",
    "            \"Microsoft MCSE\",\n",
    "            \"Microsoft Certified: Azure Fundamentals/Administrator\",\n",
    "            \"CISSP\",\n",
    "            \"CISM\",\n",
    "            \"Certified Ethical Hacker (CEH)\",\n",
    "            \"Red Hat Certified System Administrator (RHCSA)\",\n",
    "            \"Red Hat Certified Engineer (RHCE)\",\"IBM Data Science Professional Certificate\",\n",
    "            \"DASCA Associate/Senior Data Scientist\",\n",
    "            \"Coursera Data Science Specialization (Johns Hopkins University)\",\n",
    "            \"SAS Certified Data Scientist\",\n",
    "            \"Microsoft Certified: Azure Data Scientist Associate\",\"Google Professional Machine Learning Engineer\",\n",
    "            \"AWS Certified Machine Learning – Specialty\",\n",
    "            \"Microsoft Certified: Azure AI Engineer Associate\",\n",
    "            \"TensorFlow Developer Certificate\",\"DataBricks\",\"Certified Nursing Assistant (CNA)\",\n",
    "            \"Certified Medical Assistant (CMA)\",\n",
    "            \"Registered Health Information Technician/Administrator (RHIT/RHIA)\",\n",
    "            \"Certified Professional in Healthcare Quality (CPHQ)\",\n",
    "            \"Certified Health Data Analyst (CHDA)\",\n",
    "            \"Basic Life Support (BLS)\",\n",
    "            \"Advanced Cardiovascular Life Support (ACLS)\",\n",
    "            \"Certified Professional in Healthcare Information and Management Systems (CPHIMS)\",\"Chartered Financial Analyst (CFA)\",\n",
    "            \"Certified Public Accountant (CPA)\",\n",
    "            \"Certified Financial Planner (CFP)\",\n",
    "            \"Financial Risk Manager (FRM)\",\n",
    "            \"Chartered Alternative Investment Analyst (CAIA)\",\n",
    "            \"Certified Management Accountant (CMA)\",\n",
    "            \"Certified Treasury Professional (CTP)\",\"Certified Paralegal (CP)\",\n",
    "            \"Certified Legal Manager (CLM)\",\n",
    "            \"Notary Public Certification\",\n",
    "            \"Legal Project Management Professional (LPMP)\",\n",
    "            \"Paralegal Advanced Competency Exam (PACE)\",\"AWS Solutions Architect (Associate/Professional)\",\n",
    "            \"AWS Developer\",\n",
    "            \"AWS SysOps Administrator\",\n",
    "            \"AWS DevOps Engineer\",\n",
    "            \"Microsoft Azure Fundamentals\",\n",
    "            \"Microsoft Azure Administrator\",\n",
    "            \"Microsoft Azure Developer\",\n",
    "            \"Microsoft Azure Solutions Architect\",\n",
    "            \"Google Cloud Professional Cloud Architect\",\n",
    "            \"Google Cloud Associate Cloud Engineer\",\n",
    "            \"IBM Cloud Architect\",\n",
    "            \"Oracle Cloud Infrastructure Certifications\",\"Google Cloud Professional Data Engineer\",\n",
    "            \"Microsoft Certified: Azure Data Engineer Associate\",\n",
    "            \"Cloudera Certified Professional (CCP) Data Engineer\",\n",
    "            \"IBM Certified Data Engineer – Big Data\",\n",
    "            \"SAS Certified Big Data Professional\",\"Microsoft Certified: Data Analyst Associate (Power BI)\",\n",
    "            \"Tableau Desktop Specialist/Certified Associate\",\n",
    "            \"Qlik Sense Business Analyst Certification\",\n",
    "            \"SAS Certified Specialist: Visual Business Analytics\",\n",
    "            \"Google Data Analytics Professional Certificate\",\"LEED Accredited Professional\",\n",
    "            \"Certified Environmental Professional (CEP)\",\n",
    "            \"Certified Energy Manager (CEM)\",\n",
    "            \"Certified Hazardous Materials Manager (CHMM)\",\n",
    "            \"ISO 14001 Environmental Management Certification\",\"Fundamentals of Engineering (FE) – Civil\",\n",
    "            \"Professional Engineer (PE) – Civil Engineering\",\n",
    "            \"Certified Construction Manager (CCM)\",\n",
    "            \"Project Management Professional (PMP) for construction projects\",\n",
    "            \"LEED Accredited Professional\",\"Fundamentals of Engineering (FE) – Mechanical\",\n",
    "            \"Professional Engineer (PE) – Mechanical Engineering\",\n",
    "            \"Certified Manufacturing Engineer (CMfgE)\",\n",
    "            \"ASME Certifications (e.g., Certified Reliability Engineer)\",\"FAA Airframe and Powerplant (A&P) Certification\",\n",
    "            \"Certified Aerospace Technician\",\n",
    "            \"Professional Engineer (PE) in Aerospace\",\n",
    "            \"Specialized Aerospace Quality Certifications (e.g., AS9100)\",\"Project Management Professional (PMP)\",\n",
    "            \"Six Sigma Certifications (Green Belt, Black Belt, etc.)\",\n",
    "            \"ITIL Certification\",\"AWS Certified Cloud Practitioner\",\n",
    "            \"AWS Certified Solutions Architect – Associate\",\n",
    "            \"AWS Certified Solutions Architect – Professional\",\n",
    "            \"AWS Certified Developer – Associate\",\n",
    "            \"AWS Certified SysOps Administrator – Associate\",\n",
    "            \"AWS Certified DevOps Engineer – Professional\",\n",
    "            \"AWS Certified Security – Specialty\",\n",
    "            \"AWS Certified Machine Learning – Specialty\",\n",
    "            \"AWS Certified Advanced Networking – Specialty\",\n",
    "            \"AWS Certified Database – Specialty\",\n",
    "            \"AWS Certified Data Analytics – Specialty\",\"Microsoft Certified: Azure Fundamentals (AZ-900)\",\n",
    "            \"Microsoft Certified: Azure Administrator Associate (AZ-104)\",\n",
    "            \"Microsoft Certified: Azure Developer Associate (AZ-204)\",\n",
    "            \"Microsoft Certified: Azure Security Engineer Associate (AZ-500)\",\n",
    "            \"Microsoft Certified: Azure Data Engineer Associate (DP-203)\",\n",
    "            \"Microsoft Certified: Azure Data Scientist Associate (DP-100)\",\n",
    "            \"Microsoft Certified: Azure Solutions Architect Expert (AZ-305)\",\n",
    "            \"Microsoft Certified: Azure DevOps Engineer Expert (AZ-400)\",\n",
    "            \"Microsoft Certified: Azure AI Engineer Associate (AI-102)\",\"Google Cloud Digital Leader\",\n",
    "            \"Associate Cloud Engineer\",\n",
    "            \"Professional Cloud Architect\",\n",
    "            \"Professional Data Engineer\",\n",
    "            \"Professional Cloud Developer\",\n",
    "            \"Professional Cloud Network Engineer\",\n",
    "            \"Professional Cloud Security Engineer\"\n",
    "        ]\n",
    "        features['certifications'] = [cert for cert in certification_keywords if cert in resume_text]\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def vectorize_features(self, resume_features, job_listing):\n",
    "        \"\"\"Convert resume and job features into a numerical vector for neural network.\"\"\"\n",
    "        # Vectorize skills\n",
    "        all_skills = set(resume_features['skills'] + job_listing['skills'])\n",
    "        resume_skills_vec = [1 if skill in resume_features['skills'] else 0 for skill in all_skills]\n",
    "        job_skills_vec = [1 if skill in job_listing['skills'] else 0 for skill in all_skills]\n",
    "        \n",
    "        # Vectorize certifications\n",
    "        all_certs = set(resume_features['certifications'] + job_listing['certifications_required'])\n",
    "        resume_certs_vec = [1 if cert in resume_features['certifications'] else 0 for cert in all_certs]\n",
    "        job_certs_vec = [1 if cert in job_listing['certifications_required'] else 0 for cert in all_certs]\n",
    "        \n",
    "        # Experience difference feature\n",
    "        resume_exp = resume_features['experience_years']\n",
    "        job_exp_min = int(job_listing['experience'].split('-')[0])\n",
    "        exp_diff = resume_exp - job_exp_min\n",
    "        \n",
    "        # Domain match feature\n",
    "        domain_match = 1 if any(domain in job_listing['domain'].lower() for domain in resume_features['domains']) else 0\n",
    "        \n",
    "        feature_vector = resume_skills_vec + job_skills_vec + resume_certs_vec + job_certs_vec + [exp_diff, domain_match]\n",
    "        \n",
    "        if self.input_shape is None:\n",
    "            self.input_shape = len(feature_vector)\n",
    "            self.model = self.build_model()\n",
    "        return np.array(feature_vector)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build a simple neural network model for match prediction.\"\"\"\n",
    "        if self.input_shape is None:\n",
    "            return None  # Model will be built after input shape is determined\n",
    "        model = Sequential([\n",
    "            Dense(64, activation='relu', input_shape=(self.input_shape,)),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    def calculate_job_match_score(self, resume_features, job_listing):\n",
    "        \"\"\"Calculate match score using a rule-based approach with added certifications bonus.\"\"\"\n",
    "        feature_vector = self.vectorize_features(resume_features, job_listing)\n",
    "        # Rule-based scoring:\n",
    "        score = 0\n",
    "        \n",
    "        # Skill match: up to 30 points\n",
    "        skill_match = len(set(resume_features['skills']) & set(job_listing['skills']))\n",
    "        if job_listing['skills']:\n",
    "            score += (skill_match / len(job_listing['skills'])) * 30\n",
    "        \n",
    "        # Experience match: bonus points for closeness to minimum required experience (up to 20 points)\n",
    "        exp_range = job_listing['experience'].split('-')\n",
    "        min_exp = int(exp_range[0])\n",
    "        resume_exp = resume_features['experience_years']\n",
    "        score += max(0, 20 - abs(resume_exp - min_exp))\n",
    "        \n",
    "        # Domain match: 20 points if any domain matches\n",
    "        domain_match = any(domain.lower() in job_listing['domain'].lower() for domain in resume_features['domains'])\n",
    "        score += 20 if domain_match else 0\n",
    "        \n",
    "        # Project experience: 20 points if project keywords are found\n",
    "        score += 20 if resume_features['project_keywords'] else 0\n",
    "        \n",
    "        # Education bonus\n",
    "        education_bonus = {'Bachelors': 5, 'Masters': 7, 'PhD': 10}\n",
    "        if resume_features['education_level']:\n",
    "            score += education_bonus.get(resume_features['education_level'], 0)\n",
    "        \n",
    "        # Certification match bonus: if job requires certifications, add bonus proportionally (up to 20 points)\n",
    "        if job_listing['certifications_required']:\n",
    "            matching_certs = set(resume_features['certifications']) & set(job_listing['certifications_required'])\n",
    "            cert_bonus = (len(matching_certs) / len(job_listing['certifications_required'])) * 20\n",
    "            score += cert_bonus\n",
    "        \n",
    "        # Ensure score does not exceed 100\n",
    "        return min(score, 100)\n",
    "\n",
    "    def recommend_jobs(self, resume_features):\n",
    "        \"\"\"Recommend jobs based on resume features.\"\"\"\n",
    "        job_recommendations = []\n",
    "        for job in self.job_listings:\n",
    "            match_score = self.calculate_job_match_score(resume_features, job)\n",
    "            job_recommendations.append({\n",
    "                'job_title': job['title'],\n",
    "                'company': job['company'],\n",
    "                'domain': job['domain'],\n",
    "                'salary_range': job['salary_range'],\n",
    "                'location': job['location'],\n",
    "                'match_score': match_score\n",
    "            })\n",
    "        job_recommendations.sort(key=lambda x: x['match_score'], reverse=True)\n",
    "        return job_recommendations\n",
    "\n",
    "    def process_resume_folder(self):\n",
    "        \"\"\"Process all resumes and return recommendations in a DataFrame.\"\"\"\n",
    "        all_recommendations = []\n",
    "        for filename in os.listdir(self.resume_folder):\n",
    "            file_path = os.path.join(self.resume_folder, filename)\n",
    "            if os.path.isdir(file_path):\n",
    "                continue\n",
    "            \n",
    "            resume_text = self.extract_text_from_resume(file_path)\n",
    "            if not resume_text:\n",
    "                logging.warning(f\"Could not process resume: {filename}\")\n",
    "                continue\n",
    "            \n",
    "            resume_features = self.extract_resume_features(resume_text)\n",
    "            job_recommendations = self.recommend_jobs(resume_features)\n",
    "            \n",
    "            for rec in job_recommendations[:5]:  # Top 5 recommendations per resume\n",
    "                all_recommendations.append({\n",
    "                    'resume': filename,\n",
    "                    'job_title': rec['job_title'],\n",
    "                    'company': rec['company'],\n",
    "                    'domain': rec['domain'],\n",
    "                    'salary_range': rec['salary_range'],\n",
    "                    'location': rec['location'],\n",
    "                    'match_score': rec['match_score']\n",
    "                })\n",
    "            \n",
    "            logging.info(f\"Recommendations for {filename}:\")\n",
    "            for rec in job_recommendations[:3]:\n",
    "                logging.info(f\"  - {rec['job_title']} (Match Score: {rec['match_score']:.2f})\")\n",
    "        \n",
    "        return pd.DataFrame(all_recommendations)\n",
    "\n",
    "def main():\n",
    "    resume_folder = '/Users/srikar/Desktop/KRIGNAL'\n",
    "    recommender = BatchResumeRecommendationSystem(resume_folder)\n",
    "    recommendations_df = recommender.process_resume_folder()\n",
    "    print(\"\\nRecommendations DataFrame:\")\n",
    "    print(recommendations_df)\n",
    "    # Optionally save to CSV\n",
    "    recommendations_df.to_csv('recommendations.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3600d-da32-45df-8972-417dd3cc1921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Job Recommendation Agent!\n",
      "I will help you find job matches from Zscaler and Amazon based on your interests and resume.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter your desired job title or keywords (e.g., Software Engineer):  Software Engineer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping jobs... This may take a few minutes.\n",
      "Scraping jobs from Zscaler with keywords 'Software Engineer'...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import PyPDF2\n",
    "import docx\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Load spaCy for NLP tasks\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# --- Job Scraping Functions ---\n",
    "\n",
    "def scroll_to_load_all(driver):\n",
    "    \"\"\"Scroll the page to load all job listings dynamically.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def is_valid_link(url):\n",
    "    \"\"\"Validate if a URL is accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.head(url, timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "def extract_job_features(job_description):\n",
    "    \"\"\"Extract skills, experience, and certifications from job description text.\"\"\"\n",
    "    doc = nlp(job_description.lower())\n",
    "    features = {'skills': [], 'experience': 0, 'certifications_required': []}\n",
    "\n",
    "    # Common skills list (expandable)\n",
    "    skills_list = [\n",
    "        'python', 'java', 'c++', 'machine learning', 'data analysis', 'aws', 'azure', 'sql',\n",
    "        'tensorflow', 'pytorch', 'docker', 'devops', 'kubernetes', 'javascript', 'spring', 'hibernate'\n",
    "    ]\n",
    "    features['skills'] = [skill for skill in skills_list if skill in job_description.lower()]\n",
    "\n",
    "    # Extract experience (e.g., \"3 years of experience\")\n",
    "    exp_match = re.search(r'(\\d+)\\s*years?\\s*of\\s*experience', job_description, re.IGNORECASE)\n",
    "    if exp_match:\n",
    "        features['experience'] = int(exp_match.group(1))\n",
    "\n",
    "    # Common certifications list (expandable)\n",
    "    cert_list = ['aws certified', 'cissp', 'oracle certified', 'java certified', 'pe license']\n",
    "    features['certifications_required'] = [cert for cert in cert_list if cert.lower() in job_description.lower()]\n",
    "\n",
    "    return features\n",
    "\n",
    "def scrape_zscaler_jobs(job_keywords):\n",
    "    \"\"\"Scrape job listings from Zscaler and extract detailed features.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    jobs_data = []\n",
    "    base_url = \"https://boards.greenhouse.io/zscaler\"\n",
    "    company_name = \"Zscaler\"\n",
    "\n",
    "    try:\n",
    "        driver.get(base_url)\n",
    "        print(f\"Scraping jobs from Zscaler with keywords '{job_keywords}'...\")\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"opening\")))\n",
    "        scroll_to_load_all(driver)\n",
    "        job_elements = driver.find_elements(By.CLASS_NAME, \"opening\")\n",
    "\n",
    "        for job in job_elements:\n",
    "            try:\n",
    "                title = job.find_element(By.CSS_SELECTOR, \"a\").text.strip()\n",
    "                link = job.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "                location = job.find_element(By.CLASS_NAME, \"location\").text.strip() if job.find_elements(By.CLASS_NAME, \"location\") else \"Not specified\"\n",
    "\n",
    "                if job_keywords.lower() in title.lower() and is_valid_link(link):\n",
    "                    # Visit job detail page to get description\n",
    "                    driver.get(link)\n",
    "                    try:\n",
    "                        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, \"div\")))\n",
    "                        description_elem = driver.find_element(By.CLASS_NAME, \"app-body\")  # Adjust based on Zscaler page structure\n",
    "                        description = description_elem.text\n",
    "                    except:\n",
    "                        description = \"Description not available\"\n",
    "                    driver.back()\n",
    "                    time.sleep(1)  # Prevent overwhelming the server\n",
    "\n",
    "                    # Extract features from description\n",
    "                    job_features = extract_job_features(description)\n",
    "\n",
    "                    jobs_data.append({\n",
    "                        'title': title,\n",
    "                        'company': company_name,\n",
    "                        'domain': 'Not specified',\n",
    "                        'skills': job_features['skills'],\n",
    "                        'experience': f\"{job_features['experience']} years\",\n",
    "                        'salary_range': 'Not specified',\n",
    "                        'location': location,\n",
    "                        'certifications_required': job_features['certifications_required'],\n",
    "                        'link': link\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing Zscaler job: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Found {len(jobs_data)} jobs from Zscaler.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Zscaler scraping: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return jobs_data\n",
    "\n",
    "def scrape_amazon_jobs(job_keywords):\n",
    "    \"\"\"Scrape job listings from Amazon and extract detailed features.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    jobs_data = []\n",
    "    base_url = \"https://www.amazon.jobs/en/search?base_query={}\"\n",
    "    search_url = base_url.format(job_keywords.replace(\" \", \"+\"))\n",
    "    company_name = \"Amazon\"\n",
    "\n",
    "    try:\n",
    "        driver.get(search_url)\n",
    "        print(f\"Scraping jobs from Amazon with keywords '{job_keywords}'...\")\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Accept')]\"))).click()\n",
    "        except:\n",
    "            pass\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"job-tile\")))\n",
    "        scroll_to_load_all(driver)\n",
    "        job_elements = driver.find_elements(By.CLASS_NAME, \"job-tile\")\n",
    "\n",
    "        for job in job_elements:\n",
    "            try:\n",
    "                title = job.find_element(By.CLASS_NAME, \"job-title\").text.strip()\n",
    "                link = job.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                location = job.find_element(By.CLASS_NAME, \"location-and-id\").text.strip() if job.find_elements(By.CLASS_NAME, \"location-and-id\") else \"Not specified\"\n",
    "\n",
    "                if job_keywords.lower() in title.lower() and is_valid_link(link):\n",
    "                    # Visit job detail page to get description\n",
    "                    driver.get(link)\n",
    "                    try:\n",
    "                        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, \"description\")))\n",
    "                        description_elem = driver.find_element(By.CLASS_NAME, \"description\")\n",
    "                        description = description_elem.text\n",
    "                    except:\n",
    "                        description = \"Description not available\"\n",
    "                    driver.back()\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    # Extract features from description\n",
    "                    job_features = extract_job_features(description)\n",
    "\n",
    "                    jobs_data.append({\n",
    "                        'title': title,\n",
    "                        'company': company_name,\n",
    "                        'domain': 'Not specified',\n",
    "                        'skills': job_features['skills'],\n",
    "                        'experience': f\"{job_features['experience']} years\",\n",
    "                        'salary_range': 'Not specified',\n",
    "                        'location': location,\n",
    "                        'certifications_required': job_features['certifications_required'],\n",
    "                        'link': link\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing Amazon job: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Found {len(jobs_data)} jobs from Amazon.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Amazon scraping: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return jobs_data\n",
    "\n",
    "# --- Resume Processing and Recommendation Functions ---\n",
    "\n",
    "def extract_text_from_resume(file_path):\n",
    "    \"\"\"Extract text from a resume file (PDF, DOCX, TXT, PNG, JPG).\"\"\"\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    try:\n",
    "        if file_extension == '.pdf':\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                return ' '.join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "        elif file_extension == '.docx':\n",
    "            doc = docx.Document(file_path)\n",
    "            return ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        elif file_extension == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                return file.read()\n",
    "        elif file_extension in ['.png', '.jpg', '.jpeg']:\n",
    "            image = Image.open(file_path)\n",
    "            return pytesseract.image_to_string(image)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_extension}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from resume: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_resume_features(resume_text):\n",
    "    \"\"\"Extract features from resume text.\"\"\"\n",
    "    resume_text = resume_text.lower()\n",
    "    doc = nlp(resume_text)\n",
    "    features = {\n",
    "        'skills': [],\n",
    "        'experience_years': 0,\n",
    "        'domains': [],\n",
    "        'project_keywords': [],\n",
    "        'certifications': []\n",
    "    }\n",
    "\n",
    "    # Skills extraction\n",
    "    skill_patterns = [\n",
    "        'python', 'java', 'c++', 'machine learning', 'data analysis', 'aws', 'azure', 'sql',\n",
    "        'tensorflow', 'pytorch', 'docker', 'devops', 'kubernetes', 'javascript', 'spring'\n",
    "    ]\n",
    "    features['skills'] = [skill for skill in skill_patterns if skill in resume_text]\n",
    "\n",
    "    # Experience extraction\n",
    "    exp_match = re.search(r'(\\d+)\\s*(?:years?|yrs?)\\s*(?:of)?\\s*experience', resume_text, re.IGNORECASE)\n",
    "    if exp_match:\n",
    "        features['experience_years'] = int(exp_match.group(1))\n",
    "\n",
    "    # Certifications extraction\n",
    "    cert_keywords = ['aws certified', 'cissp', 'oracle certified', 'java certified', 'pe license']\n",
    "    features['certifications'] = [cert for cert in cert_keywords if cert in resume_text]\n",
    "\n",
    "    # Placeholder for domains and project keywords (simplified)\n",
    "    features['domains'] = ['IT']  # Default domain; can be enhanced\n",
    "    features['project_keywords'] = re.findall(r'project\\s*:\\s*([^\\n]+)', resume_text, re.IGNORECASE)\n",
    "\n",
    "    return features\n",
    "\n",
    "def calculate_job_match_score(resume_features, job_listing):\n",
    "    \"\"\"Calculate a match score between resume features and job requirements.\"\"\"\n",
    "    score = 0\n",
    "\n",
    "    # Skill match (up to 30 points)\n",
    "    skill_match = len(set(resume_features['skills']) & set(job_listing['skills']))\n",
    "    if job_listing['skills']:\n",
    "        score += (skill_match / len(job_listing['skills'])) * 30\n",
    "\n",
    "    # Experience match (up to 20 points)\n",
    "    job_exp = int(job_listing['experience'].split()[0])  # Extract minimum experience\n",
    "    resume_exp = resume_features['experience_years']\n",
    "    score += max(0, 20 - abs(resume_exp - job_exp))\n",
    "\n",
    "    # Certification match (up to 20 points)\n",
    "    if job_listing['certifications_required']:\n",
    "        cert_match = len(set(resume_features['certifications']) & set(job_listing['certifications_required']))\n",
    "        score += (cert_match / len(job_listing['certifications_required'])) * 20 if job_listing['certifications_required'] else 0\n",
    "\n",
    "    # Project bonus (20 points if projects exist)\n",
    "    score += 20 if resume_features['project_keywords'] else 0\n",
    "\n",
    "    return min(score, 100)\n",
    "\n",
    "def recommend_jobs_for_resume(resume_path, job_listings):\n",
    "    \"\"\"Recommend jobs based on a single resume and a list of job listings.\"\"\"\n",
    "    resume_text = extract_text_from_resume(resume_path)\n",
    "    if not resume_text:\n",
    "        return []\n",
    "\n",
    "    resume_features = extract_resume_features(resume_text)\n",
    "    recommendations = []\n",
    "\n",
    "    for job in job_listings:\n",
    "        match_score = calculate_job_match_score(resume_features, job)\n",
    "        recommendations.append({\n",
    "            'job_title': job['title'],\n",
    "            'company': job['company'],\n",
    "            'location': job['location'],\n",
    "            'link': job['link'],\n",
    "            'match_score': match_score\n",
    "        })\n",
    "\n",
    "    recommendations.sort(key=lambda x: x['match_score'], reverse=True)\n",
    "    return recommendations\n",
    "\n",
    "# --- AI Agent Interface ---\n",
    "\n",
    "def job_recommendation_agent():\n",
    "    \"\"\"Interactive AI agent to guide the user through job scraping and recommendation.\"\"\"\n",
    "    print(\"Welcome to the Job Recommendation Agent!\")\n",
    "    print(\"I will help you find job matches from Zscaler and Amazon based on your interests and resume.\")\n",
    "\n",
    "    # Step 1: Get job keywords\n",
    "    job_keywords = input(\"\\nPlease enter your desired job title or keywords (e.g., Software Engineer): \").strip()\n",
    "    if not job_keywords:\n",
    "        print(\"No keywords provided. Using default: 'Engineer'.\")\n",
    "        job_keywords = \"Engineer\"\n",
    "\n",
    "    # Step 2: Scrape jobs\n",
    "    print(\"\\nScraping jobs... This may take a few minutes.\")\n",
    "    zscaler_jobs = scrape_zscaler_jobs(job_keywords)\n",
    "    amazon_jobs = scrape_amazon_jobs(job_keywords)\n",
    "    all_jobs = zscaler_jobs + amazon_jobs\n",
    "    print(f\"\\nTotal jobs found: {len(all_jobs)}\")\n",
    "\n",
    "    if not all_jobs:\n",
    "        print(\"No jobs found matching your keywords. Please try different keywords.\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Get resume\n",
    "    resume_path = input(\"\\nPlease enter the full path to your resume file (PDF, DOCX, TXT, PNG, JPG): \").strip()\n",
    "    if not os.path.exists(resume_path):\n",
    "        print(\"Resume file not found. Please check the path and try again.\")\n",
    "        return\n",
    "\n",
    "    # Step 4: Generate recommendations\n",
    "    print(\"\\nProcessing your resume and generating recommendations...\")\n",
    "    recommendations = recommend_jobs_for_resume(resume_path, all_jobs)\n",
    "\n",
    "    if not recommendations:\n",
    "        print(\"Could not process your resume or no matches found.\")\n",
    "        return\n",
    "\n",
    "    # Step 5: Display top recommendations\n",
    "    print(\"\\nTop 5 Job Recommendations:\")\n",
    "    for i, rec in enumerate(recommendations[:5], 1):\n",
    "        print(f\"{i}. {rec['job_title']} at {rec['company']}\")\n",
    "        print(f\"   Location: {rec['location']}\")\n",
    "        print(f\"   Match Score: {rec['match_score']:.2f}\")\n",
    "        print(f\"   Link: {rec['link']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Optional: Save to CSV\n",
    "    df = pd.DataFrame(recommendations)\n",
    "    filename = f\"job_recommendations_{job_keywords.replace(' ', '_')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"\\nAll recommendations saved to '{filename}'\")\n",
    "\n",
    "# --- Run the Agent ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    job_recommendation_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e5b18-8e9b-4033-abdb-c7a33a861cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
