{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f5ba5c-df8f-4744-8d26-994cd1ac5b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Job Scraper\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job title to search for (leave empty to get all jobs):  Machine Learning\n",
      "Maximum number of pages to scrape (default 10):  1\n",
      "Run in headless mode? (y/n, default: n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 23:54:55,047 - INFO - Starting Tesla job scraper at 2025-03-17 23:54:55\n",
      "2025-03-17 23:54:55,050 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting job scraper...\n",
      "This may take several minutes depending on the number of jobs and pages.\n",
      "Progress will be logged to the console and a log file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 23:54:55,341 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-17 23:54:55,403 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-03-17 23:54:55,449 - INFO - Driver [/Users/srikar/.wdm/drivers/chromedriver/mac64/134.0.6998.88/chromedriver-mac-arm64/chromedriver] found in cache\n",
      "2025-03-17 23:54:56,481 - INFO - Scraping jobs from Tesla, filtering by keyword 'Machine Learning'\n",
      "2025-03-17 23:55:06,790 - INFO - Searched for 'Machine Learning'\n",
      "2025-03-17 23:55:07,782 - INFO - Starting to scroll to load all content...\n",
      "2025-03-17 23:55:09,883 - INFO - Scroll 1: Height 2180 → 2180\n",
      "2025-03-17 23:55:09,883 - INFO - No change detected (1/3)\n",
      "2025-03-17 23:55:12,014 - INFO - Scroll 2: Height 2180 → 2180\n",
      "2025-03-17 23:55:12,014 - INFO - No change detected (2/3)\n",
      "2025-03-17 23:55:14,213 - INFO - Scroll 3: Height 2180 → 2180\n",
      "2025-03-17 23:55:14,213 - INFO - No change detected (3/3)\n",
      "2025-03-17 23:55:14,214 - INFO - No more content loading after multiple scrolls. Stopping scroll operation.\n",
      "2025-03-17 23:55:14,214 - INFO - Completed scrolling after 2 scrolls.\n",
      "2025-03-17 23:55:14,774 - INFO - Finding and processing job categories...\n",
      "2025-03-17 23:55:15,432 - INFO - Found 0 potential job category sections\n",
      "2025-03-17 23:55:15,433 - INFO - No specific job categories found. Getting all visible jobs.\n",
      "2025-03-17 23:55:16,086 - INFO - Found search input on job listings page\n",
      "2025-03-17 23:55:16,130 - INFO - Found 0 unique job listings after deduplication\n",
      "2025-03-17 23:55:16,130 - WARNING - No job cards found using primary selectors\n",
      "2025-03-17 23:55:16,877 - INFO - Found 14 jobs using alternative detection\n",
      "2025-03-17 23:55:16,898 - INFO - Added job: Explore Jobs at Not specified\n",
      "2025-03-17 23:55:16,917 - INFO - Added job: Machine Learning Engineer, Motion Planning, Self-Driving at Not specified\n",
      "2025-03-17 23:55:16,936 - INFO - Added job: Machine Learning Engineer, Geometric Vision, Self-Driving at Not specified\n",
      "2025-03-17 23:55:16,954 - INFO - Added job: Sr. Machine Learning Engineer, Navigation, Optimus at Not specified\n",
      "2025-03-17 23:55:16,973 - INFO - Added job: Machine Learning Engineer, Reliability Engineering at Not specified\n",
      "2025-03-17 23:55:16,992 - INFO - Added job: Machine Learning Hardware Performance Engineer, Self-Driving Hardware at Not specified\n",
      "2025-03-17 23:55:17,011 - INFO - Added job: Sr. Machine Learning Engineer, Autobidder at Not specified\n",
      "2025-03-17 23:55:17,030 - INFO - Added job: Sr. Site Reliability Engineer, Machine Learning Operations, Infrastructure at Not specified\n",
      "2025-03-17 23:55:17,049 - INFO - Added job: Fullstack Software Engineer, Machine Learning Platform, AI Infrastructure at Not specified\n",
      "2025-03-17 23:55:17,068 - INFO - Added job: Backend Software Engineer, Machine Learning Platform, AI Infrastructure at Not specified\n",
      "2025-03-17 23:55:17,086 - INFO - Added job: Software Engineer, AI Networking, Machine Learning Infrastructure at Not specified\n",
      "2025-03-17 23:55:17,105 - INFO - Added job: Machine Learning Engineer, Integration, AI Platforms at Not specified\n",
      "2025-03-17 23:55:17,124 - INFO - Added job: Staff Machine Learning Engineer, Autobidder at Not specified\n",
      "2025-03-17 23:55:17,143 - INFO - Added job: Sr. Machine Learning Infrastructure Engineer, Optimus at Not specified\n",
      "2025-03-17 23:55:17,143 - INFO - Collected 14 total jobs\n",
      "2025-03-17 23:55:17,144 - INFO - Filtered from 14 to 13 jobs matching 'Machine Learning'\n",
      "2025-03-17 23:55:17,144 - INFO - Getting descriptions for 13 jobs (up to 100)\n",
      "2025-03-17 23:55:17,148 - INFO - Getting description for job 1/13: Machine Learning Engineer, Motion Planning, Self-Driving\n",
      "2025-03-17 23:55:22,146 - INFO - Got description for 'Machine Learning Engineer, Motion Planning, Self-Driving' (20 chars)\n",
      "2025-03-17 23:55:22,208 - INFO - Getting description for job 2/13: Machine Learning Engineer, Geometric Vision, Self-Driving\n",
      "2025-03-17 23:55:27,456 - INFO - Got description for 'Machine Learning Engineer, Geometric Vision, Self-Driving' (20 chars)\n",
      "2025-03-17 23:55:27,511 - INFO - Getting description for job 3/13: Sr. Machine Learning Engineer, Navigation, Optimus\n",
      "2025-03-17 23:55:32,916 - INFO - Got description for 'Sr. Machine Learning Engineer, Navigation, Optimus' (20 chars)\n",
      "2025-03-17 23:55:32,975 - INFO - Getting description for job 4/13: Machine Learning Engineer, Reliability Engineering\n",
      "2025-03-17 23:55:38,291 - INFO - Got description for 'Machine Learning Engineer, Reliability Engineering' (20 chars)\n",
      "2025-03-17 23:55:38,350 - INFO - Getting description for job 5/13: Machine Learning Hardware Performance Engineer, Self-Driving Hardware\n",
      "2025-03-17 23:55:43,702 - INFO - Got description for 'Machine Learning Hardware Performance Engineer, Self-Driving Hardware' (20 chars)\n",
      "2025-03-17 23:55:43,761 - INFO - Getting description for job 6/13: Sr. Machine Learning Engineer, Autobidder\n",
      "2025-03-17 23:55:49,012 - INFO - Got description for 'Sr. Machine Learning Engineer, Autobidder' (20 chars)\n",
      "2025-03-17 23:55:49,071 - INFO - Getting description for job 7/13: Sr. Site Reliability Engineer, Machine Learning Operations, Infrastructure\n",
      "2025-03-17 23:55:54,522 - INFO - Got description for 'Sr. Site Reliability Engineer, Machine Learning Operations, Infrastructure' (20 chars)\n",
      "2025-03-17 23:55:54,583 - INFO - Getting description for job 8/13: Fullstack Software Engineer, Machine Learning Platform, AI Infrastructure\n",
      "2025-03-17 23:55:59,892 - INFO - Got description for 'Fullstack Software Engineer, Machine Learning Platform, AI Infrastructure' (20 chars)\n",
      "2025-03-17 23:55:59,952 - INFO - Getting description for job 9/13: Backend Software Engineer, Machine Learning Platform, AI Infrastructure\n",
      "2025-03-17 23:56:05,403 - INFO - Got description for 'Backend Software Engineer, Machine Learning Platform, AI Infrastructure' (20 chars)\n",
      "2025-03-17 23:56:05,467 - INFO - Getting description for job 10/13: Software Engineer, AI Networking, Machine Learning Infrastructure\n",
      "2025-03-17 23:56:10,988 - INFO - Got description for 'Software Engineer, AI Networking, Machine Learning Infrastructure' (20 chars)\n",
      "2025-03-17 23:56:11,045 - INFO - Getting description for job 11/13: Machine Learning Engineer, Integration, AI Platforms\n",
      "2025-03-17 23:56:16,450 - INFO - Got description for 'Machine Learning Engineer, Integration, AI Platforms' (20 chars)\n",
      "2025-03-17 23:56:16,510 - INFO - Getting description for job 12/13: Staff Machine Learning Engineer, Autobidder\n",
      "2025-03-17 23:56:22,392 - INFO - Got description for 'Staff Machine Learning Engineer, Autobidder' (20 chars)\n",
      "2025-03-17 23:56:22,452 - INFO - Getting description for job 13/13: Sr. Machine Learning Infrastructure Engineer, Optimus\n",
      "2025-03-17 23:56:28,082 - INFO - Got description for 'Sr. Machine Learning Infrastructure Engineer, Optimus' (20 chars)\n",
      "2025-03-17 23:56:30,509 - INFO - Completed fetching descriptions for 13 jobs\n",
      "2025-03-17 23:56:30,512 - INFO - Saving 13 jobs found in search results\n",
      "2025-03-17 23:56:30,792 - INFO - Detailed jobs saved to 'tesla_jobs_detailed_Machine_Learning_20250317_235630.csv'\n",
      "2025-03-17 23:56:30,799 - INFO - Simplified jobs list saved to 'tesla_jobs_simple_Machine_Learning_20250317_235630.csv'\n",
      "2025-03-17 23:56:30,805 - INFO - Completed in 95.76 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Tesla Job Scraping Results:\n",
      "Total jobs found: 13\n",
      "Unique locations: 1\n",
      "Sample jobs:\n",
      "                                               Title       Location\n",
      "0  Machine Learning Engineer, Motion Planning, Se...  Not specified\n",
      "1  Machine Learning Engineer, Geometric Vision, S...  Not specified\n",
      "2  Sr. Machine Learning Engineer, Navigation, Opt...  Not specified\n",
      "3  Machine Learning Engineer, Reliability Enginee...  Not specified\n",
      "4  Machine Learning Hardware Performance Engineer...  Not specified\n",
      "\n",
      "Top locations:\n",
      "Location\n",
      "Not specified    13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Results saved to:\n",
      "- tesla_jobs_detailed_Machine_Learning_20250317_235630.csv\n",
      "- tesla_jobs_simple_Machine_Learning_20250317_235630.csv\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(f\"tesla_job_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to scroll and load all jobs\n",
    "def scroll_to_load_all(driver, max_scrolls=20, wait_time=1.5):\n",
    "    \"\"\"\n",
    "    Scroll the page to load all content with a maximum number of scrolls\n",
    "    For Tesla career site\n",
    "    \"\"\"\n",
    "    scrolls = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    consecutive_no_change = 0\n",
    "    \n",
    "    logger.info(\"Starting to scroll to load all content...\")\n",
    "    \n",
    "    while scrolls < max_scrolls:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(wait_time)  # Wait time for content to load\n",
    "        \n",
    "        # Take screenshot for debugging\n",
    "        driver.save_screenshot(f\"screenshots/tesla_scroll_{scrolls+1}.png\")\n",
    "        \n",
    "        # Try to find \"Load More\" or similar buttons and click them\n",
    "        try:\n",
    "            load_more_buttons = driver.find_elements(By.XPATH, \n",
    "                \"//button[contains(text(), 'Load More') or contains(text(), 'View More') or contains(text(), 'Show More')]\")\n",
    "            if load_more_buttons:\n",
    "                for button in load_more_buttons:\n",
    "                    if button.is_displayed() and button.is_enabled():\n",
    "                        driver.execute_script(\"arguments[0].click();\", button)\n",
    "                        logger.info(\"Clicked 'Load More' button\")\n",
    "                        time.sleep(wait_time + 1)  # Extra wait for new content\n",
    "        except Exception as e:\n",
    "            logger.info(f\"No 'Load More' button found or error clicking it: {e}\")\n",
    "        \n",
    "        # Check if we've reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        logger.info(f\"Scroll {scrolls+1}: Height {last_height} → {new_height}\")\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            consecutive_no_change += 1\n",
    "            logger.info(f\"No change detected ({consecutive_no_change}/3)\")\n",
    "            if consecutive_no_change >= 3:  # If no change for 3 consecutive scrolls\n",
    "                logger.info(\"No more content loading after multiple scrolls. Stopping scroll operation.\")\n",
    "                break\n",
    "        else:\n",
    "            consecutive_no_change = 0\n",
    "            \n",
    "        last_height = new_height\n",
    "        scrolls += 1\n",
    "    \n",
    "    logger.info(f\"Completed scrolling after {scrolls} scrolls.\")\n",
    "    return True\n",
    "\n",
    "# Function to handle category navigation on Tesla career page\n",
    "def handle_job_categories(driver):\n",
    "    \"\"\"\n",
    "    Navigate through job categories on Tesla careers page\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    \n",
    "    logger.info(\"Finding and processing job categories...\")\n",
    "    \n",
    "    # Take screenshot before category processing\n",
    "    driver.save_screenshot(\"screenshots/tesla_before_categories.png\")\n",
    "    \n",
    "    # Find job category sections (based on the screenshot, they have card-like elements)\n",
    "    # The specific selectors might need adjustment based on the actual HTML structure\n",
    "    try:\n",
    "        job_categories = driver.find_elements(By.CSS_SELECTOR, \".tcl-section, [id^='tdsgrid-']\")\n",
    "        logger.info(f\"Found {len(job_categories)} potential job category sections\")\n",
    "        \n",
    "        if len(job_categories) == 0:\n",
    "            # If no specific sections found, we'll try to get all job listings from current page\n",
    "            logger.info(\"No specific job categories found. Getting all visible jobs.\")\n",
    "            return extract_job_listings_tesla(driver)\n",
    "            \n",
    "        # Process \"See Opportunities\" links within categories\n",
    "        opportunity_links = driver.find_elements(By.XPATH, \"//a[contains(text(), 'See Opportunities')]\")\n",
    "        logger.info(f\"Found {len(opportunity_links)} 'See Opportunities' links\")\n",
    "        \n",
    "        # If no opportunity links found, try to get jobs from current page\n",
    "        if len(opportunity_links) == 0:\n",
    "            logger.info(\"No 'See Opportunities' links found. Getting jobs from current page.\")\n",
    "            return extract_job_listings_tesla(driver)\n",
    "        \n",
    "        # Store the current window handle\n",
    "        original_window = driver.current_window_handle\n",
    "        \n",
    "        # Process each category\n",
    "        for index, link in enumerate(opportunity_links):\n",
    "            try:\n",
    "                # Get category name - usually in a heading element near the link\n",
    "                category_name = \"Unknown Category\"\n",
    "                try:\n",
    "                    # Try to find the closest heading to this link\n",
    "                    parent_section = link.find_element(By.XPATH, \"./ancestor::section[1]\") \n",
    "                    headings = parent_section.find_elements(By.XPATH, \".//h2 | .//h3 | .//h4\")\n",
    "                    if headings:\n",
    "                        category_name = headings[0].text.strip()\n",
    "                except:\n",
    "                    # If we can't find a heading, use the link's parent text\n",
    "                    try:\n",
    "                        parent_text = link.find_element(By.XPATH, \"./..\").text.strip()\n",
    "                        # Extract first line as category name\n",
    "                        category_name = parent_text.split('\\n')[0].strip()\n",
    "                    except:\n",
    "                        logger.warning(f\"Could not determine category name for link {index+1}\")\n",
    "                \n",
    "                logger.info(f\"Processing category: {category_name} ({index+1}/{len(opportunity_links)})\")\n",
    "                \n",
    "                # Take screenshot before clicking\n",
    "                driver.save_screenshot(f\"screenshots/tesla_category_{index+1}_before.png\")\n",
    "                \n",
    "                # Open in new tab to preserve main page navigation\n",
    "                driver.execute_script(\"window.open('about:blank', '_blank');\")\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "                \n",
    "                # Get the href value and navigate to it\n",
    "                href = link.get_attribute('href')\n",
    "                driver.get(href)\n",
    "                time.sleep(3)  # Wait for page to load\n",
    "                \n",
    "                # Take screenshot after navigation\n",
    "                driver.save_screenshot(f\"screenshots/tesla_category_{index+1}_page.png\")\n",
    "                \n",
    "                # Scroll to load all jobs in this category\n",
    "                scroll_to_load_all(driver)\n",
    "                \n",
    "                # Extract jobs for this category\n",
    "                category_jobs = extract_job_listings_tesla(driver)\n",
    "                \n",
    "                # Add category to each job\n",
    "                for job in category_jobs:\n",
    "                    job['Category'] = category_name\n",
    "                \n",
    "                all_jobs.extend(category_jobs)\n",
    "                logger.info(f\"Added {len(category_jobs)} jobs from '{category_name}' category\")\n",
    "                \n",
    "                # Close tab and switch back to original window\n",
    "                driver.close()\n",
    "                driver.switch_to.window(original_window)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing category {index+1}: {e}\")\n",
    "                # Make sure we're back to the original window\n",
    "                try:\n",
    "                    if driver.current_window_handle != original_window:\n",
    "                        driver.close()\n",
    "                        driver.switch_to.window(original_window)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "        return all_jobs\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error finding job categories: {e}\")\n",
    "        driver.save_screenshot(\"screenshots/tesla_categories_error.png\")\n",
    "        \n",
    "        # Fallback to extracting jobs from current page\n",
    "        return extract_job_listings_tesla(driver)\n",
    "\n",
    "# Extract job information from Tesla page\n",
    "def extract_job_listings_tesla(driver):\n",
    "    \"\"\"\n",
    "    Extract job listings from Tesla career page\n",
    "    \"\"\"\n",
    "    jobs_data = []\n",
    "    \n",
    "    # Take screenshot before extraction\n",
    "    driver.save_screenshot(\"screenshots/tesla_before_extraction.png\")\n",
    "    \n",
    "    try:\n",
    "        # First try to identify if we're on a job listings page with a search form\n",
    "        search_input = None\n",
    "        try:\n",
    "            search_input = driver.find_element(By.XPATH, \"//input[@type='search' or @placeholder='Search by role or keyword']\")\n",
    "            logger.info(\"Found search input on job listings page\")\n",
    "        except:\n",
    "            logger.info(\"No search input found. May not be on standard listings page.\")\n",
    "        \n",
    "        # Look for job listing elements\n",
    "        # Based on the screenshots, we need to identify the correct job card elements\n",
    "        job_cards = []\n",
    "        \n",
    "        # Try different selectors that might contain job listings\n",
    "        for selector in [\n",
    "            \"[id^='job-'] a\", \n",
    "            \".job-card\", \n",
    "            \"a[href*='/careers/'][href*='/apply']\",\n",
    "            \"[id^='tdsgrid-'] a[href*='/careers/']\",\n",
    "            \".opening a\",  # Similar to Greenhouse job board\n",
    "            \".positions-list a\",\n",
    "            \"section a[href*='/careers/']\"\n",
    "        ]:\n",
    "            try:\n",
    "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if elements:\n",
    "                    logger.info(f\"Found {len(elements)} potential job elements with selector: {selector}\")\n",
    "                    job_cards.extend(elements)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Remove duplicates by href\n",
    "        unique_hrefs = set()\n",
    "        unique_cards = []\n",
    "        \n",
    "        for card in job_cards:\n",
    "            href = card.get_attribute(\"href\")\n",
    "            if href and href not in unique_hrefs and '/careers/' in href:\n",
    "                unique_hrefs.add(href)\n",
    "                unique_cards.append(card)\n",
    "        \n",
    "        logger.info(f\"Found {len(unique_cards)} unique job listings after deduplication\")\n",
    "        \n",
    "        if len(unique_cards) > 0:\n",
    "            # Highlight first job for screenshot\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].style.border='3px solid red'\", unique_cards[0])\n",
    "                driver.save_screenshot(\"screenshots/tesla_job_elements_found.png\")\n",
    "                driver.execute_script(\"arguments[0].style.border=''\", unique_cards[0])\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            logger.warning(\"No job cards found using primary selectors\")\n",
    "            driver.save_screenshot(\"screenshots/tesla_no_jobs_found.png\")\n",
    "            \n",
    "            # Try alternative approach - look for any links containing '/careers/' and job-related keywords\n",
    "            try:\n",
    "                all_links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in all_links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and '/careers/' in href and href not in unique_hrefs:\n",
    "                        # Check if link or its parent contains job-related text\n",
    "                        link_text = link.text.lower()\n",
    "                        if any(keyword in link_text for keyword in [\"engineer\", \"manager\", \"specialist\", \"analyst\", \"developer\", \"job\", \"position\", \"apply\"]):\n",
    "                            unique_hrefs.add(href)\n",
    "                            unique_cards.append(link)\n",
    "                \n",
    "                logger.info(f\"Found {len(unique_cards)} jobs using alternative detection\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during alternative job detection: {e}\")\n",
    "                \n",
    "        # Process each job card\n",
    "        for index, job_card in enumerate(unique_cards):\n",
    "            try:\n",
    "                # Get job link\n",
    "                link = job_card.get_attribute(\"href\")\n",
    "                \n",
    "                # Get job title\n",
    "                title = job_card.text.strip()\n",
    "                \n",
    "                # If the text contains multiple lines, the first line is likely the title\n",
    "                if \"\\n\" in title:\n",
    "                    title = title.split(\"\\n\")[0].strip()\n",
    "                \n",
    "                # If title is empty, try to find title element inside\n",
    "                if not title:\n",
    "                    try:\n",
    "                        title_elem = job_card.find_element(By.TAG_NAME, \"h3\") or job_card.find_element(By.TAG_NAME, \"h4\")\n",
    "                        title = title_elem.text.strip()\n",
    "                    except:\n",
    "                        # Try to use last part of URL as fallback title\n",
    "                        if link:\n",
    "                            url_parts = link.rstrip('/').split('/')\n",
    "                            title = url_parts[-1].replace('-', ' ').title()\n",
    "                \n",
    "                # Get location - often in the job card but might need specific targeting\n",
    "                location = \"Not specified\"\n",
    "                try:\n",
    "                    # Try various ways to find location text\n",
    "                    location_elem = None\n",
    "                    try:\n",
    "                        # Try to find location by class or common patterns\n",
    "                        location_elem = job_card.find_element(By.CSS_SELECTOR, \n",
    "                            \".location, [class*='location'], span:not(:first-child)\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    if not location_elem:\n",
    "                        # Try to get full text and parse location from it\n",
    "                        full_text = job_card.text\n",
    "                        text_parts = full_text.split('\\n')\n",
    "                        if len(text_parts) > 1:\n",
    "                            location = text_parts[1].strip()\n",
    "                    else:\n",
    "                        location = location_elem.text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Add job if we have title and link\n",
    "                if title and title.strip() and link and link.strip():\n",
    "                    # Check for duplicate before adding\n",
    "                    is_duplicate = False\n",
    "                    for existing_job in jobs_data:\n",
    "                        if existing_job[\"Title\"] == title and existing_job[\"Link\"] == link:\n",
    "                            is_duplicate = True\n",
    "                            break\n",
    "                    \n",
    "                    if not is_duplicate:\n",
    "                        jobs_data.append({\n",
    "                            \"Title\": title,\n",
    "                            \"Location\": location,\n",
    "                            \"Department\": \"Not specified\",  # We'll set this with category later\n",
    "                            \"Link\": link,\n",
    "                            \"Description\": \"\",  # We'll get descriptions later\n",
    "                            \"Company\": \"Tesla\"\n",
    "                        })\n",
    "                        logger.info(f\"Added job: {title} at {location}\")\n",
    "                \n",
    "            except (StaleElementReferenceException, Exception) as e:\n",
    "                logger.error(f\"Error extracting job details for job {index+1}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error finding job elements: {e}\")\n",
    "        driver.save_screenshot(\"screenshots/tesla_extract_error.png\")\n",
    "    \n",
    "    return jobs_data\n",
    "\n",
    "# Function to get job descriptions\n",
    "def get_job_descriptions(driver, jobs_data, max_descriptions=100):\n",
    "    \"\"\"\n",
    "    Get job descriptions for Tesla jobs by visiting their individual pages\n",
    "    \"\"\"\n",
    "    logger.info(f\"Getting descriptions for {len(jobs_data)} jobs (up to {max_descriptions})\")\n",
    "    \n",
    "    # Create a dedicated folder for job description screenshots\n",
    "    job_desc_folder = 'tesla_job_description_screenshots'\n",
    "    os.makedirs(job_desc_folder, exist_ok=True)\n",
    "    \n",
    "    # Store current URL to return to afterward\n",
    "    original_url = driver.current_url\n",
    "    original_window = driver.current_window_handle\n",
    "    \n",
    "    # Process up to max_descriptions\n",
    "    for i, job in enumerate(jobs_data[:max_descriptions]):\n",
    "        if not job.get(\"Link\"):\n",
    "            continue\n",
    "            \n",
    "        logger.info(f\"Getting description for job {i+1}/{min(len(jobs_data), max_descriptions)}: {job['Title']}\")\n",
    "        \n",
    "        # Create a clean filename from the job title\n",
    "        clean_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", job['Title'])\n",
    "        clean_title = re.sub(r'\\s+', \"_\", clean_title)\n",
    "        clean_title = clean_title[:100] if len(clean_title) > 100 else clean_title\n",
    "        \n",
    "        # Create a new tab for each job\n",
    "        try:\n",
    "            # Open new tab\n",
    "            driver.execute_script(\"window.open('about:blank', '_blank');\")\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            \n",
    "            # Navigate to job details page\n",
    "            driver.get(job[\"Link\"])\n",
    "            time.sleep(4)  # Wait for page to load\n",
    "            \n",
    "            # Take a screenshot\n",
    "            screenshot_file = f\"{job_desc_folder}/job_{i+1}_{clean_title}.png\"\n",
    "            driver.save_screenshot(screenshot_file)\n",
    "            \n",
    "            # Extract description - try various selectors\n",
    "            description = \"\"\n",
    "            desc_selectors = [\n",
    "                \".job-description\", \"#job-description\", \"[id*='job-description']\",\n",
    "                \".description\", \"#description\",\n",
    "                \".job-details\", \"#job-details\",\n",
    "                \"main\", \"article\", \n",
    "                \"[role='main']\", \"[class*='content']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in desc_selectors:\n",
    "                try:\n",
    "                    desc_elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if desc_elements:\n",
    "                        description = desc_elements[0].text.strip()\n",
    "                        if description:\n",
    "                            logger.info(f\"Got description for '{job['Title']}' ({len(description)} chars)\")\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Selector {selector} failed: {e}\")\n",
    "            \n",
    "            # Update the job with the description\n",
    "            if description:\n",
    "                job[\"Description\"] = description\n",
    "            \n",
    "            # Close tab\n",
    "            driver.close()\n",
    "            driver.switch_to.window(original_window)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting description for {job['Title']}: {e}\")\n",
    "            # Make sure we're back to the original window\n",
    "            try:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(original_window)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Return to original page\n",
    "    try:\n",
    "        driver.get(original_url)\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    logger.info(f\"Completed fetching descriptions for {min(len(jobs_data), max_descriptions)} jobs\")\n",
    "    return jobs_data\n",
    "\n",
    "# Function to handle different types of popups\n",
    "def handle_popups(driver):\n",
    "    try:\n",
    "        # Common buttons for accepting cookies, terms, etc.\n",
    "        popup_selectors = [\n",
    "            \"//button[contains(text(), 'Accept')]\", \n",
    "            \"//button[contains(text(), 'I agree')]\",\n",
    "            \"//button[contains(@id, 'accept')]\",\n",
    "            \"//button[contains(@class, 'accept')]\",\n",
    "            \"//button[contains(text(), 'Continue')]\",\n",
    "            \"//button[contains(text(), 'Got it')]\",\n",
    "            \"//button[contains(text(), 'Close')]\",\n",
    "            \"//button[@aria-label='Close']\",\n",
    "            \"//div[contains(@class, 'cookie')]//button\",\n",
    "            \"//div[contains(@id, 'consent')]//button\"\n",
    "        ]\n",
    "        \n",
    "        for xpath in popup_selectors:\n",
    "            try:\n",
    "                buttons = driver.find_elements(By.XPATH, xpath)\n",
    "                for button in buttons:\n",
    "                    if button.is_displayed():\n",
    "                        button.click()\n",
    "                        logger.info(f\"Clicked popup/cookie button with xpath: {xpath}\")\n",
    "                        time.sleep(1)\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "        # Handle alerts\n",
    "        try:\n",
    "            alert = driver.switch_to.alert\n",
    "            alert.accept()\n",
    "            logger.info(\"Accepted alert popup\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error handling popups: {e}\")\n",
    "\n",
    "# Function to validate URL\n",
    "def is_valid_link(url):\n",
    "    if not url or not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
    "        return response.status_code < 400  # Accept any non-error status\n",
    "    except requests.RequestException:\n",
    "        logger.warning(f\"Invalid link: {url}\")\n",
    "        return False\n",
    "\n",
    "# Main Tesla job scraper function\n",
    "def scrape_tesla_jobs(search_keyword=\"\", max_pages=10, headless=False):\n",
    "    \"\"\"\n",
    "    Scrape Tesla jobs from the careers page\n",
    "    \n",
    "    Parameters:\n",
    "    search_keyword (str): Keyword to filter jobs by title\n",
    "    max_pages (int): Maximum number of pages to scrape (used for pagination if present)\n",
    "    headless (bool): Whether to run in headless mode\n",
    "    \n",
    "    Returns:\n",
    "    list: List of all job dictionaries found\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "        \n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36')\n",
    "    \n",
    "    # Create directories for debugging\n",
    "    os.makedirs('screenshots', exist_ok=True)\n",
    "    \n",
    "    driver = None\n",
    "    jobs_data = []\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        \n",
    "        # Tesla's careers URL\n",
    "        base_url = \"https://www.tesla.com/careers\"\n",
    "        logger.info(f\"Scraping jobs from Tesla\" + (f\", filtering by keyword '{search_keyword}'\" if search_keyword else \"\"))\n",
    "        \n",
    "        # Open the Tesla careers page\n",
    "        driver.get(base_url)\n",
    "        driver.save_screenshot(\"screenshots/tesla_initial.png\")\n",
    "        \n",
    "        # Handle popups\n",
    "        handle_popups(driver)\n",
    "        \n",
    "        # If there's a search box, use it\n",
    "        if search_keyword:\n",
    "            try:\n",
    "                search_box = driver.find_element(By.XPATH, \"//input[@type='search' or @placeholder='Search by role or keyword']\")\n",
    "                if search_box:\n",
    "                    search_box.clear()\n",
    "                    search_box.send_keys(search_keyword)\n",
    "                    search_box.send_keys(Keys.RETURN)\n",
    "                    time.sleep(3)  # Wait for results to load\n",
    "                    logger.info(f\"Searched for '{search_keyword}'\")\n",
    "                    driver.save_screenshot(\"screenshots/tesla_search_results.png\")\n",
    "                    \n",
    "                    # Wait for search results\n",
    "                    try:\n",
    "                        # Wait for page to load after search\n",
    "                        WebDriverWait(driver, 10).until(\n",
    "                            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                        )\n",
    "                    except TimeoutException:\n",
    "                        logger.warning(\"Timed out waiting for search results\")\n",
    "            except NoSuchElementException:\n",
    "                logger.info(\"No search box found. Will filter results after scraping.\")\n",
    "        \n",
    "        # Scroll to load all results on current page\n",
    "        scroll_to_load_all(driver)\n",
    "        driver.save_screenshot(\"screenshots/tesla_after_scroll.png\")\n",
    "        \n",
    "        # Handle job categories and collect all jobs\n",
    "        all_jobs = handle_job_categories(driver)\n",
    "        logger.info(f\"Collected {len(all_jobs)} total jobs\")\n",
    "        \n",
    "        # If search keyword was provided and we didn't use the search box, filter the results by job title\n",
    "        if search_keyword and all_jobs:\n",
    "            filtered_jobs = [job for job in all_jobs if search_keyword.lower() in job[\"Title\"].lower()]\n",
    "            logger.info(f\"Filtered from {len(all_jobs)} to {len(filtered_jobs)} jobs matching '{search_keyword}'\")\n",
    "            all_jobs = filtered_jobs\n",
    "        \n",
    "        # Get job descriptions for all collected jobs\n",
    "        jobs_with_descriptions = get_job_descriptions(driver, all_jobs, max_descriptions=100)\n",
    "        \n",
    "        # Save all jobs\n",
    "        jobs_data = jobs_with_descriptions\n",
    "        logger.info(f\"Saving {len(jobs_data)} jobs found in search results\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping from Tesla: {e}\")\n",
    "        if driver:\n",
    "            driver.save_screenshot(\"screenshots/tesla_error.png\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "    return jobs_data\n",
    "\n",
    "# Main function to scrape and save results to CSV\n",
    "def main(search_keyword=\"\", max_pages=10, headless=False):\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting Tesla job scraper at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Create screenshots directory\n",
    "    os.makedirs('screenshots', exist_ok=True)\n",
    "    \n",
    "    # Create job description screenshots directory\n",
    "    os.makedirs('tesla_job_description_screenshots', exist_ok=True)\n",
    "    \n",
    "    # Scrape Tesla jobs\n",
    "    jobs_data = scrape_tesla_jobs(search_keyword, max_pages, headless)\n",
    "    \n",
    "    # Generate filenames with timestamps\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    keyword_slug = search_keyword.replace(' ', '_') if search_keyword else 'all'\n",
    "    detailed_filename = f\"tesla_jobs_detailed_{keyword_slug}_{timestamp}.csv\"\n",
    "    simple_filename = f\"tesla_jobs_simple_{keyword_slug}_{timestamp}.csv\"\n",
    "    \n",
    "    # Save results\n",
    "    if jobs_data:\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(jobs_data)\n",
    "        \n",
    "        # Save detailed CSV with descriptions\n",
    "        df.to_csv(detailed_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Detailed jobs saved to '{detailed_filename}'\")\n",
    "        \n",
    "        # Create and save a simplified CSV without descriptions\n",
    "        simple_df = df[['Title', 'Location', 'Department', 'Link']].copy()\n",
    "        simple_df.to_csv(simple_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Simplified jobs list saved to '{simple_filename}'\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Tesla Job Scraping Results:\")\n",
    "        print(f\"Total jobs found: {len(df)}\")\n",
    "        print(f\"Unique locations: {len(df['Location'].unique())}\")\n",
    "        print(f\"Sample jobs:\")\n",
    "        print(df[['Title', 'Location']].head())\n",
    "        print(\"\\nTop locations:\")\n",
    "        print(df['Location'].value_counts().head())\n",
    "        print(f\"\\nResults saved to:\")\n",
    "        print(f\"- {detailed_filename}\")\n",
    "        print(f\"- {simple_filename}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Completed in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        logger.warning(f\"No jobs found with search keyword '{search_keyword}'\")\n",
    "        print(\"\\nNo jobs found to display.\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Process completed with no results in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Tesla Job Scraper\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get user input\n",
    "    job_title = input(\"Enter job title to search for (leave empty to get all jobs): \").strip()\n",
    "    \n",
    "    try:\n",
    "        max_pages = int(input(\"Maximum number of pages to scrape (default 10): \") or \"10\")\n",
    "    except ValueError:\n",
    "        max_pages = 10\n",
    "        print(\"Invalid input. Using default of 10 pages.\")\n",
    "    \n",
    "    headless_mode = input(\"Run in headless mode? (y/n, default: n): \").strip().lower() == 'y'\n",
    "    \n",
    "    print(\"\\nStarting job scraper...\")\n",
    "    print(\"This may take several minutes depending on the number of jobs and pages.\")\n",
    "    print(\"Progress will be logged to the console and a log file.\")\n",
    "    \n",
    "    # Run the scraper\n",
    "    main(job_title, max_pages, headless_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662269e8-6bee-434f-8c58-4278ed42bfea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
