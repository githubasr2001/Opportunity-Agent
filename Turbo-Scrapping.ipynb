{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25101745-f77f-46b7-b89b-a46e9da30b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API key:  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\n",
      "Enter job title to search for (e.g., 'software engineer'):  Data Scientist\n",
      "Enter location (e.g., 'San Francisco, CA'):  San Francisco, CA\n",
      "Enter number of pages to scrape (1-5 recommended):  3\n",
      "Enter job source to scrape (indeed/linkedin/both):  both\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping Indeed jobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping page 1 jobs: 0it [00:00, ?it/s]\n",
      "Scraping page 2 jobs: 0it [00:00, ?it/s]\n",
      "Scraping page 3 jobs: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping LinkedIn jobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping LinkedIn page 1 jobs:   0%|                     | 0/60 [00:00<?, ?it/s]2025-03-22 10:23:00,594 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:   2%|▏            | 1/60 [00:04<04:48,  4.89s/it]2025-03-22 10:23:04,688 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:   3%|▍            | 2/60 [00:10<04:53,  5.05s/it]2025-03-22 10:23:10,442 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:   5%|▋            | 3/60 [00:14<04:20,  4.58s/it]2025-03-22 10:23:14,452 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:   7%|▊            | 4/60 [00:19<04:30,  4.82s/it]2025-03-22 10:23:20,148 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:   8%|█            | 5/60 [00:24<04:33,  4.97s/it]2025-03-22 10:23:24,759 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  10%|█▎           | 6/60 [00:28<04:18,  4.79s/it]2025-03-22 10:23:28,752 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  12%|█▌           | 7/60 [00:33<04:14,  4.80s/it]2025-03-22 10:23:34,109 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  13%|█▋           | 8/60 [00:38<04:16,  4.94s/it]2025-03-22 10:23:39,093 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  15%|█▉           | 9/60 [00:42<03:53,  4.59s/it]2025-03-22 10:23:42,574 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  17%|██          | 10/60 [00:46<03:42,  4.45s/it]2025-03-22 10:23:46,396 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  18%|██▏         | 11/60 [00:51<03:45,  4.60s/it]2025-03-22 10:23:51,586 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  20%|██▍         | 12/60 [00:55<03:31,  4.41s/it]2025-03-22 10:23:56,911 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  22%|██▌         | 13/60 [01:02<03:56,  5.02s/it]2025-03-22 10:24:02,184 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  23%|██▊         | 14/60 [01:07<03:54,  5.09s/it]2025-03-22 10:24:07,153 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  25%|███         | 15/60 [01:11<03:30,  4.68s/it]2025-03-22 10:24:11,379 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  27%|███▏        | 16/60 [01:15<03:19,  4.54s/it]2025-03-22 10:24:15,365 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  28%|███▍        | 17/60 [01:19<03:06,  4.34s/it]2025-03-22 10:24:19,152 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  30%|███▌        | 18/60 [01:22<02:51,  4.08s/it]2025-03-22 10:24:22,695 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  32%|███▊        | 19/60 [01:28<03:02,  4.45s/it]2025-03-22 10:24:28,782 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  33%|████        | 20/60 [01:33<03:09,  4.74s/it]2025-03-22 10:24:34,391 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  35%|████▏       | 21/60 [01:38<03:09,  4.87s/it]2025-03-22 10:24:38,487 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  37%|████▍       | 22/60 [01:43<03:03,  4.84s/it]2025-03-22 10:24:43,707 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  38%|████▌       | 23/60 [01:49<03:06,  5.04s/it]2025-03-22 10:24:50,158 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  40%|████▊       | 24/60 [01:54<03:05,  5.16s/it]2025-03-22 10:24:54,152 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  42%|█████       | 25/60 [01:59<03:00,  5.16s/it]2025-03-22 10:25:00,705 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  43%|█████▏      | 26/60 [02:04<02:52,  5.07s/it]2025-03-22 10:25:05,519 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  45%|█████▍      | 27/60 [02:10<02:52,  5.22s/it]2025-03-22 10:25:09,718 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  47%|█████▌      | 28/60 [02:14<02:41,  5.03s/it]2025-03-22 10:25:15,861 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  48%|█████▊      | 29/60 [02:21<02:50,  5.49s/it]2025-03-22 10:25:21,903 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  50%|██████      | 30/60 [02:26<02:45,  5.51s/it]2025-03-22 10:25:26,922 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  52%|██████▏     | 31/60 [02:32<02:40,  5.54s/it]2025-03-22 10:25:32,348 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  53%|██████▍     | 32/60 [02:36<02:24,  5.16s/it]2025-03-22 10:25:38,320 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  55%|██████▌     | 33/60 [02:41<02:20,  5.21s/it]2025-03-22 10:25:41,872 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  57%|██████▊     | 34/60 [02:46<02:12,  5.11s/it]2025-03-22 10:25:48,734 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  58%|███████     | 35/60 [02:53<02:21,  5.68s/it]2025-03-22 10:25:54,674 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  60%|███████▏    | 36/60 [02:59<02:18,  5.77s/it]2025-03-22 10:25:59,708 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  62%|███████▍    | 37/60 [03:05<02:08,  5.60s/it]2025-03-22 10:26:05,219 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  63%|███████▌    | 38/60 [03:10<02:01,  5.53s/it]2025-03-22 10:26:10,338 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  65%|███████▊    | 39/60 [03:14<01:49,  5.22s/it]2025-03-22 10:26:15,052 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  67%|████████    | 40/60 [03:20<01:44,  5.23s/it]2025-03-22 10:26:21,912 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  68%|████████▏   | 41/60 [03:26<01:45,  5.55s/it]2025-03-22 10:26:26,621 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  70%|████████▍   | 42/60 [03:31<01:37,  5.41s/it]2025-03-22 10:26:31,743 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  72%|████████▌   | 43/60 [03:35<01:26,  5.07s/it]2025-03-22 10:26:35,531 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  73%|████████▊   | 44/60 [03:40<01:21,  5.09s/it]2025-03-22 10:26:41,327 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  75%|█████████   | 45/60 [03:46<01:20,  5.34s/it]2025-03-22 10:26:47,584 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  77%|█████████▏  | 46/60 [03:51<01:11,  5.12s/it]2025-03-22 10:26:51,812 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  78%|█████████▍  | 47/60 [03:55<01:02,  4.80s/it]2025-03-22 10:26:55,805 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  80%|█████████▌  | 48/60 [03:59<00:56,  4.68s/it]2025-03-22 10:27:00,105 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  82%|█████████▊  | 49/60 [04:03<00:48,  4.44s/it]2025-03-22 10:27:03,587 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  83%|██████████  | 50/60 [04:07<00:42,  4.22s/it]2025-03-22 10:27:07,377 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  85%|██████████▏ | 51/60 [04:12<00:40,  4.55s/it]2025-03-22 10:27:12,802 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  87%|██████████▍ | 52/60 [04:16<00:34,  4.34s/it]2025-03-22 10:27:16,797 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  88%|██████████▌ | 53/60 [04:20<00:29,  4.21s/it]2025-03-22 10:27:20,688 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  90%|██████████▊ | 54/60 [04:24<00:24,  4.09s/it]2025-03-22 10:27:24,989 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  92%|███████████ | 55/60 [04:29<00:21,  4.38s/it]2025-03-22 10:27:29,392 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  93%|███████████▏| 56/60 [04:33<00:16,  4.24s/it]2025-03-22 10:27:34,000 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  95%|███████████▍| 57/60 [04:37<00:12,  4.29s/it]2025-03-22 10:27:42,092 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  97%|███████████▌| 58/60 [04:46<00:10,  5.48s/it]2025-03-22 10:27:45,942 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs:  98%|███████████▊| 59/60 [04:53<00:05,  5.98s/it]2025-03-22 10:27:54,303 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 1 jobs: 100%|████████████| 60/60 [04:59<00:00,  4.99s/it]\n",
      "Scraping LinkedIn page 2 jobs:   0%|                     | 0/60 [00:00<?, ?it/s]2025-03-22 10:28:05,540 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:   2%|▏            | 1/60 [00:05<05:14,  5.33s/it]2025-03-22 10:28:10,983 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:   3%|▍            | 2/60 [00:09<04:36,  4.77s/it]2025-03-22 10:28:16,384 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:   5%|▋            | 3/60 [00:14<04:41,  4.95s/it]2025-03-22 10:28:21,105 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:   7%|▊            | 4/60 [00:20<04:41,  5.03s/it]2025-03-22 10:28:26,532 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:   8%|█            | 5/60 [00:24<04:32,  4.95s/it]2025-03-22 10:28:30,892 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  10%|█▎           | 6/60 [00:29<04:26,  4.93s/it]2025-03-22 10:28:35,032 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  12%|█▌           | 7/60 [00:33<04:01,  4.56s/it]2025-03-22 10:28:39,639 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  13%|█▋           | 8/60 [00:38<03:57,  4.56s/it]2025-03-22 10:28:43,428 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  15%|█▉           | 9/60 [00:41<03:36,  4.25s/it]2025-03-22 10:28:47,731 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  17%|██          | 10/60 [00:45<03:32,  4.24s/it]2025-03-22 10:28:52,133 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  18%|██▏         | 11/60 [00:50<03:29,  4.27s/it]2025-03-22 10:28:56,024 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  20%|██▍         | 12/60 [00:55<03:33,  4.46s/it]2025-03-22 10:29:01,348 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  22%|██▌         | 13/60 [00:59<03:28,  4.43s/it]2025-03-22 10:29:05,240 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  23%|██▊         | 14/60 [01:04<03:37,  4.74s/it]2025-03-22 10:29:10,052 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  25%|███         | 15/60 [01:09<03:33,  4.74s/it]2025-03-22 10:29:15,070 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  27%|███▏        | 16/60 [01:13<03:13,  4.40s/it]2025-03-22 10:29:18,920 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  28%|███▍        | 17/60 [01:17<03:03,  4.26s/it]2025-03-22 10:29:22,492 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  30%|███▌        | 18/60 [01:22<03:10,  4.54s/it]2025-03-22 10:29:28,321 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  32%|███▊        | 19/60 [01:26<03:02,  4.44s/it]2025-03-22 10:29:32,346 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  33%|████        | 20/60 [01:31<03:07,  4.69s/it]2025-03-22 10:29:38,213 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  35%|████▏       | 21/60 [01:37<03:09,  4.86s/it]2025-03-22 10:29:42,780 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  37%|████▍       | 22/60 [01:41<03:02,  4.79s/it]2025-03-22 10:29:47,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  38%|████▌       | 23/60 [01:46<02:58,  4.81s/it]2025-03-22 10:29:53,368 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  40%|████▊       | 24/60 [01:52<03:04,  5.14s/it]2025-03-22 10:29:58,086 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  42%|█████       | 25/60 [01:57<03:02,  5.21s/it]2025-03-22 10:30:04,939 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  43%|█████▏      | 26/60 [02:04<03:13,  5.70s/it]2025-03-22 10:30:11,187 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  45%|█████▍      | 27/60 [02:09<02:58,  5.42s/it]2025-03-22 10:30:14,974 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  47%|█████▌      | 28/60 [02:13<02:42,  5.07s/it]2025-03-22 10:30:20,812 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  48%|█████▊      | 29/60 [02:19<02:39,  5.15s/it]2025-03-22 10:30:25,002 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  50%|██████      | 30/60 [02:23<02:26,  4.89s/it]2025-03-22 10:30:29,311 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  52%|██████▏     | 31/60 [02:29<02:29,  5.16s/it]2025-03-22 10:30:35,046 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  53%|██████▍     | 32/60 [02:34<02:27,  5.28s/it]2025-03-22 10:30:40,677 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  55%|██████▌     | 33/60 [02:39<02:20,  5.21s/it]2025-03-22 10:30:47,744 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  57%|██████▊     | 34/60 [02:47<02:35,  5.99s/it]2025-03-22 10:30:53,356 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  58%|███████     | 35/60 [02:51<02:13,  5.35s/it]2025-03-22 10:30:57,801 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  60%|███████▏    | 36/60 [02:55<02:02,  5.10s/it]2025-03-22 10:31:02,565 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  62%|███████▍    | 37/60 [03:01<01:57,  5.10s/it]2025-03-22 10:31:07,403 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  63%|███████▌    | 38/60 [03:07<01:58,  5.38s/it]2025-03-22 10:31:15,186 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  65%|███████▊    | 39/60 [03:14<02:06,  6.03s/it]2025-03-22 10:31:20,409 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  67%|████████    | 40/60 [03:19<01:54,  5.72s/it]2025-03-22 10:31:25,575 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  68%|████████▏   | 41/60 [03:24<01:43,  5.44s/it]2025-03-22 10:31:30,036 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  70%|████████▍   | 42/60 [03:28<01:31,  5.06s/it]2025-03-22 10:31:34,540 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  72%|████████▌   | 43/60 [03:33<01:25,  5.02s/it]2025-03-22 10:31:38,944 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  73%|████████▊   | 44/60 [03:36<01:12,  4.52s/it]2025-03-22 10:31:43,141 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  75%|█████████   | 45/60 [03:41<01:09,  4.65s/it]2025-03-22 10:31:47,955 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  77%|█████████▏  | 46/60 [03:47<01:07,  4.82s/it]2025-03-22 10:31:56,572 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  78%|█████████▍  | 47/60 [03:54<01:14,  5.71s/it]2025-03-22 10:32:00,960 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  80%|█████████▌  | 48/60 [04:00<01:08,  5.74s/it]2025-03-22 10:32:06,593 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  82%|█████████▊  | 49/60 [04:04<00:57,  5.21s/it]2025-03-22 10:32:10,076 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  83%|██████████  | 50/60 [04:09<00:51,  5.11s/it]2025-03-22 10:32:15,397 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  85%|██████████▏ | 51/60 [04:14<00:45,  5.05s/it]2025-03-22 10:32:20,074 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  87%|██████████▍ | 52/60 [04:18<00:38,  4.82s/it]2025-03-22 10:32:24,410 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  88%|██████████▌ | 53/60 [04:23<00:34,  4.92s/it]2025-03-22 10:32:30,041 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  90%|██████████▊ | 54/60 [04:29<00:30,  5.14s/it]2025-03-22 10:32:35,776 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  92%|███████████ | 55/60 [04:34<00:25,  5.08s/it]2025-03-22 10:32:40,075 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  93%|███████████▏| 56/60 [04:38<00:19,  4.86s/it]2025-03-22 10:32:43,967 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  95%|███████████▍| 57/60 [04:42<00:13,  4.59s/it]2025-03-22 10:32:48,678 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  97%|███████████▌| 58/60 [04:47<00:09,  4.73s/it]2025-03-22 10:32:54,107 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs:  98%|███████████▊| 59/60 [04:53<00:04,  4.90s/it]2025-03-22 10:32:58,509 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 2 jobs: 100%|████████████| 60/60 [04:58<00:00,  4.97s/it]\n",
      "Scraping LinkedIn page 3 jobs:   0%|                     | 0/60 [00:00<?, ?it/s]2025-03-22 10:33:08,239 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:   2%|▏            | 1/60 [00:02<02:24,  2.45s/it]2025-03-22 10:33:11,986 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:   3%|▍            | 2/60 [00:07<03:51,  3.98s/it]2025-03-22 10:33:17,863 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:   5%|▋            | 3/60 [00:12<04:17,  4.51s/it]2025-03-22 10:33:23,207 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:   7%|▊            | 4/60 [00:16<04:08,  4.44s/it]2025-03-22 10:33:28,647 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:   8%|█            | 5/60 [00:24<05:00,  5.46s/it]2025-03-22 10:33:35,068 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  10%|█▎           | 6/60 [00:30<05:10,  5.75s/it]2025-03-22 10:33:40,800 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  12%|█▌           | 7/60 [00:35<04:44,  5.38s/it]2025-03-22 10:33:45,526 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  13%|█▋           | 8/60 [00:40<04:40,  5.39s/it]2025-03-22 10:33:51,143 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  15%|█▉           | 9/60 [00:45<04:26,  5.23s/it]2025-03-22 10:33:54,726 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  17%|██          | 10/60 [00:49<04:01,  4.82s/it]2025-03-22 10:33:59,540 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  18%|██▏         | 11/60 [00:53<03:52,  4.75s/it]2025-03-22 10:34:04,568 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  20%|██▍         | 12/60 [00:59<04:03,  5.08s/it]2025-03-22 10:34:10,480 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  22%|██▌         | 13/60 [01:05<04:06,  5.25s/it]2025-03-22 10:34:16,321 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  23%|██▊         | 14/60 [01:10<03:58,  5.19s/it]2025-03-22 10:34:22,683 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  25%|███         | 15/60 [01:16<04:08,  5.53s/it]2025-03-22 10:34:27,530 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  27%|███▏        | 16/60 [01:22<04:11,  5.71s/it]2025-03-22 10:34:32,950 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  28%|███▍        | 17/60 [01:27<03:47,  5.29s/it]2025-03-22 10:34:37,632 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  30%|███▌        | 18/60 [01:32<03:37,  5.18s/it]2025-03-22 10:34:43,572 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  32%|███▊        | 19/60 [01:39<03:54,  5.73s/it]2025-03-22 10:34:48,816 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  33%|████        | 20/60 [01:43<03:32,  5.31s/it]2025-03-22 10:34:53,403 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  35%|████▏       | 21/60 [01:47<03:16,  5.04s/it]2025-03-22 10:34:57,704 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  37%|████▍       | 22/60 [01:51<02:53,  4.56s/it]2025-03-22 10:35:01,492 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  38%|████▌       | 23/60 [01:56<02:59,  4.84s/it]2025-03-22 10:35:07,330 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  40%|████▊       | 24/60 [02:01<02:50,  4.74s/it]2025-03-22 10:35:11,220 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  42%|█████       | 25/60 [02:05<02:36,  4.48s/it]2025-03-22 10:35:14,906 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  43%|█████▏      | 26/60 [02:09<02:33,  4.51s/it]2025-03-22 10:35:21,564 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  45%|█████▍      | 27/60 [02:16<02:47,  5.09s/it]2025-03-22 10:35:25,761 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  47%|█████▌      | 28/60 [02:19<02:26,  4.57s/it]2025-03-22 10:35:30,471 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  48%|█████▊      | 29/60 [02:24<02:27,  4.76s/it]2025-03-22 10:35:36,925 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  50%|██████      | 30/60 [02:30<02:33,  5.11s/it]2025-03-22 10:35:41,133 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  52%|██████▏     | 31/60 [02:36<02:32,  5.27s/it]2025-03-22 10:35:45,730 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  53%|██████▍     | 32/60 [02:40<02:14,  4.81s/it]2025-03-22 10:35:49,927 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  55%|██████▌     | 33/60 [02:44<02:06,  4.68s/it]2025-03-22 10:35:54,335 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  57%|██████▊     | 34/60 [02:49<02:05,  4.81s/it]2025-03-22 10:36:01,569 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  58%|███████     | 35/60 [02:55<02:08,  5.13s/it]2025-03-22 10:36:06,108 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  60%|███████▏    | 36/60 [03:01<02:06,  5.26s/it]2025-03-22 10:36:11,432 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  62%|███████▍    | 37/60 [03:06<02:00,  5.24s/it]2025-03-22 10:36:17,577 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  63%|███████▌    | 38/60 [03:12<02:02,  5.58s/it]2025-03-22 10:36:23,926 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  65%|███████▊    | 39/60 [03:19<02:02,  5.83s/it]2025-03-22 10:36:29,806 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  67%|████████    | 40/60 [03:23<01:49,  5.50s/it]2025-03-22 10:36:33,755 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  68%|████████▏   | 41/60 [03:27<01:35,  5.05s/it]2025-03-22 10:36:38,365 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  70%|████████▍   | 42/60 [03:33<01:33,  5.21s/it]2025-03-22 10:36:43,688 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  72%|████████▌   | 43/60 [03:38<01:26,  5.10s/it]2025-03-22 10:36:48,295 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  73%|████████▊   | 44/60 [03:42<01:18,  4.91s/it]2025-03-22 10:36:50,863 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  75%|█████████   | 45/60 [03:45<01:02,  4.19s/it]2025-03-22 10:36:54,952 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  77%|█████████▏  | 46/60 [03:48<00:55,  3.96s/it]2025-03-22 10:36:59,050 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  78%|█████████▍  | 47/60 [03:53<00:53,  4.13s/it]2025-03-22 10:37:03,553 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  80%|█████████▌  | 48/60 [03:58<00:55,  4.62s/it]2025-03-22 10:37:09,186 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  82%|█████████▊  | 49/60 [04:04<00:52,  4.79s/it]2025-03-22 10:37:14,305 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  83%|██████████  | 50/60 [04:08<00:45,  4.55s/it]2025-03-22 10:37:18,379 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  85%|██████████▏ | 51/60 [04:12<00:40,  4.54s/it]2025-03-22 10:37:22,601 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  87%|██████████▍ | 52/60 [04:17<00:37,  4.70s/it]2025-03-22 10:37:28,643 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  88%|██████████▌ | 53/60 [04:23<00:36,  5.15s/it]2025-03-22 10:37:33,885 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  90%|██████████▊ | 54/60 [04:28<00:30,  5.01s/it]2025-03-22 10:37:38,427 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  92%|███████████ | 55/60 [04:32<00:24,  4.84s/it]2025-03-22 10:37:43,081 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  93%|███████████▏| 56/60 [04:37<00:18,  4.72s/it]2025-03-22 10:37:48,610 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  95%|███████████▍| 57/60 [04:43<00:15,  5.22s/it]2025-03-22 10:37:53,118 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  97%|███████████▌| 58/60 [04:47<00:09,  4.91s/it]2025-03-22 10:37:58,441 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs:  98%|███████████▊| 59/60 [04:53<00:05,  5.03s/it]2025-03-22 10:38:03,970 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scraping LinkedIn page 3 jobs: 100%|████████████| 60/60 [04:59<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180 jobs on LinkedIn\n",
      "\n",
      "Total jobs found: 180\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Save as CSV or Excel? (csv/excel):  CSV\n",
      "Enter filename (default: job_data.csv):  Data_Scientist.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Data_Scientist.csv\n",
      "\n",
      "Job data preview:\n",
      "                            job_title company                        location  \\\n",
      "0               Growth Data Scientist  Notion      Notion (in person company)   \n",
      "1              Product Data Scientist  Notion  San Francisco or New York City   \n",
      "2  Data Scientist - Product Analytics    Brex                   San Francisco   \n",
      "3           Data Scientist II, Credit    Brex                   San Francisco   \n",
      "4                      AI/ML Engineer  Ikigai                     Ikigai Labs   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.linkedin.com/jobs/view/data-scient...  \n",
      "1  https://www.linkedin.com/jobs/view/data-scient...  \n",
      "2  https://www.linkedin.com/jobs/view/data-scient...  \n",
      "3  https://www.linkedin.com/jobs/view/data-scient...  \n",
      "4  https://www.linkedin.com/jobs/view/machine-lea...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class JobScraper:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "    def extract_job_details_with_llm(self, job_description):\n",
    "        \"\"\"\n",
    "        Use OpenAI to extract structured data from job descriptions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts job information from text. Extract the following fields: job title, location, preferred qualifications, and minimum qualifications.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Extract the job details from the following job description. Return a JSON object with these fields: 'job_title', 'location', 'preferred_qualifications' (as a list), 'minimum_qualifications' (as a list).\\n\\nJob Description:\\n{job_description}\"}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Try to extract the JSON part if the AI added explanations\n",
    "            import json\n",
    "            try:\n",
    "                # First try to parse directly\n",
    "                parsed_data = json.loads(result)\n",
    "                return parsed_data\n",
    "            except:\n",
    "                # If direct parsing fails, try to extract JSON part with regex\n",
    "                json_pattern = r'```json\\n(.*?)\\n```'\n",
    "                json_match = re.search(json_pattern, result, re.DOTALL)\n",
    "                \n",
    "                if json_match:\n",
    "                    json_str = json_match.group(1)\n",
    "                    return json.loads(json_str)\n",
    "                \n",
    "                # If that fails, try another approach\n",
    "                json_pattern = r'\\{[\\s\\S]*\\}'\n",
    "                json_match = re.search(json_pattern, result)\n",
    "                \n",
    "                if json_match:\n",
    "                    json_str = json_match.group(0)\n",
    "                    return json.loads(json_str)\n",
    "                \n",
    "                # If all parsing attempts fail\n",
    "                return {\n",
    "                    \"job_title\": \"Parsing Error\",\n",
    "                    \"location\": \"Parsing Error\", \n",
    "                    \"preferred_qualifications\": [\"Parsing Error\"],\n",
    "                    \"minimum_qualifications\": [\"Parsing Error\"]\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with OpenAI API: {e}\")\n",
    "            return {\n",
    "                \"job_title\": \"API Error\",\n",
    "                \"location\": \"API Error\", \n",
    "                \"preferred_qualifications\": [\"API Error\"],\n",
    "                \"minimum_qualifications\": [\"API Error\"]\n",
    "            }\n",
    "    \n",
    "    def scrape_indeed(self, query, location, num_pages=1):\n",
    "        \"\"\"\n",
    "        Scrape job listings from Indeed\n",
    "        \"\"\"\n",
    "        jobs_data = []\n",
    "        \n",
    "        for page in range(num_pages):\n",
    "            start = page * 10  # Indeed uses increments of 10 for pagination\n",
    "            url = f\"https://www.indeed.com/jobs?q={query.replace(' ', '+')}&l={location.replace(' ', '+')}&start={start}\"\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                job_cards = soup.find_all('div', class_='job_seen_beacon')\n",
    "                \n",
    "                for job in tqdm(job_cards, desc=f\"Scraping page {page+1} jobs\"):\n",
    "                    try:\n",
    "                        # Extract job title\n",
    "                        title_elem = job.find('h2', class_='jobTitle')\n",
    "                        if title_elem:\n",
    "                            job_title = title_elem.get_text().strip()\n",
    "                        else:\n",
    "                            continue  # Skip if no title found\n",
    "                        \n",
    "                        # Extract job URL\n",
    "                        job_link_elem = job.find('a', id=lambda x: x and x.startswith('job_'))\n",
    "                        if job_link_elem:\n",
    "                            job_url = \"https://www.indeed.com\" + job_link_elem.get('href', '')\n",
    "                        else:\n",
    "                            job_url = \"\"\n",
    "                        \n",
    "                        # Extract company\n",
    "                        company_elem = job.find('span', class_='companyName')\n",
    "                        company = company_elem.get_text().strip() if company_elem else \"Not specified\"\n",
    "                        \n",
    "                        # Extract location\n",
    "                        location_elem = job.find('div', class_='companyLocation')\n",
    "                        job_location = location_elem.get_text().strip() if location_elem else \"Not specified\"\n",
    "                        \n",
    "                        # Get full job description\n",
    "                        job_desc = self.get_job_description(job_url)\n",
    "                        \n",
    "                        # Extract detailed information using LLM\n",
    "                        details = self.extract_job_details_with_llm(job_desc)\n",
    "                        \n",
    "                        # Combine all job information\n",
    "                        job_data = {\n",
    "                            'job_title': details.get('job_title', job_title),\n",
    "                            'company': company,\n",
    "                            'location': details.get('location', job_location),\n",
    "                            'preferred_qualifications': details.get('preferred_qualifications', []),\n",
    "                            'minimum_qualifications': details.get('minimum_qualifications', []),\n",
    "                            'url': job_url\n",
    "                        }\n",
    "                        \n",
    "                        jobs_data.append(job_data)\n",
    "                        \n",
    "                        # Sleep to avoid rate limiting\n",
    "                        time.sleep(random.uniform(1, 3))\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing job listing: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Sleep between pages\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping page {page+1}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return pd.DataFrame(jobs_data)\n",
    "    \n",
    "    def scrape_linkedin(self, query, location, num_pages=1):\n",
    "        \"\"\"\n",
    "        Scrape job listings from LinkedIn\n",
    "        \"\"\"\n",
    "        jobs_data = []\n",
    "        \n",
    "        for page in range(num_pages):\n",
    "            start = page * 25  # LinkedIn uses increments of 25 for pagination\n",
    "            url = f\"https://www.linkedin.com/jobs/search/?keywords={query.replace(' ', '%20')}&location={location.replace(' ', '%20')}&start={start}\"\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                job_cards = soup.find_all('div', class_='base-card')\n",
    "                \n",
    "                for job in tqdm(job_cards, desc=f\"Scraping LinkedIn page {page+1} jobs\"):\n",
    "                    try:\n",
    "                        # Extract job title\n",
    "                        title_elem = job.find('h3', class_='base-search-card__title')\n",
    "                        if title_elem:\n",
    "                            job_title = title_elem.get_text().strip()\n",
    "                        else:\n",
    "                            continue  # Skip if no title found\n",
    "                        \n",
    "                        # Extract job URL\n",
    "                        job_link_elem = job.find('a', class_='base-card__full-link')\n",
    "                        if job_link_elem:\n",
    "                            job_url = job_link_elem.get('href', '').split('?')[0]  # Remove tracking parameters\n",
    "                        else:\n",
    "                            job_url = \"\"\n",
    "                        \n",
    "                        # Extract company\n",
    "                        company_elem = job.find('h4', class_='base-search-card__subtitle')\n",
    "                        company = company_elem.get_text().strip() if company_elem else \"Not specified\"\n",
    "                        \n",
    "                        # Extract location\n",
    "                        location_elem = job.find('span', class_='job-search-card__location')\n",
    "                        job_location = location_elem.get_text().strip() if location_elem else \"Not specified\"\n",
    "                        \n",
    "                        # Get full job description\n",
    "                        job_desc = self.get_job_description(job_url)\n",
    "                        \n",
    "                        # Extract detailed information using LLM\n",
    "                        details = self.extract_job_details_with_llm(job_desc)\n",
    "                        \n",
    "                        # Combine all job information\n",
    "                        job_data = {\n",
    "                            'job_title': details.get('job_title', job_title),\n",
    "                            'company': company,\n",
    "                            'location': details.get('location', job_location),\n",
    "                            'preferred_qualifications': details.get('preferred_qualifications', []),\n",
    "                            'minimum_qualifications': details.get('minimum_qualifications', []),\n",
    "                            'url': job_url\n",
    "                        }\n",
    "                        \n",
    "                        jobs_data.append(job_data)\n",
    "                        \n",
    "                        # Sleep to avoid rate limiting\n",
    "                        time.sleep(random.uniform(1, 3))\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing LinkedIn job listing: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Sleep between pages\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping LinkedIn page {page+1}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return pd.DataFrame(jobs_data)\n",
    "    \n",
    "    def get_job_description(self, url):\n",
    "        \"\"\"\n",
    "        Fetch the full job description from the job URL\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Try different selectors for job descriptions (varies by site)\n",
    "            job_desc_elem = None\n",
    "            \n",
    "            # Indeed\n",
    "            if 'indeed.com' in url:\n",
    "                job_desc_elem = soup.find('div', id='jobDescriptionText')\n",
    "            # LinkedIn\n",
    "            elif 'linkedin.com' in url:\n",
    "                job_desc_elem = soup.find('div', class_='show-more-less-html__markup')\n",
    "            # Generic attempt if specific selectors fail\n",
    "            if not job_desc_elem:\n",
    "                job_desc_elem = (\n",
    "                    soup.find('div', class_=lambda x: x and ('description' in x.lower() or 'details' in x.lower())) or\n",
    "                    soup.find('section', class_=lambda x: x and ('description' in x.lower() or 'details' in x.lower()))\n",
    "                )\n",
    "            \n",
    "            if job_desc_elem:\n",
    "                return job_desc_elem.get_text(separator='\\n').strip()\n",
    "            else:\n",
    "                # Try to get all text from the page as a fallback\n",
    "                main_content = soup.find('main') or soup.find('div', id='main-content') or soup.body\n",
    "                if main_content:\n",
    "                    return main_content.get_text(separator='\\n').strip()\n",
    "                return \"No job description found\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching job description: {e}\")\n",
    "            return \"Error fetching job description\"\n",
    "    \n",
    "    def save_to_csv(self, df, filename='job_data.csv'):\n",
    "        \"\"\"\n",
    "        Save the DataFrame to a CSV file\n",
    "        \"\"\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    \n",
    "    def save_to_excel(self, df, filename='job_data.xlsx'):\n",
    "        \"\"\"\n",
    "        Save the DataFrame to an Excel file\n",
    "        \"\"\"\n",
    "        df.to_excel(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get OpenAI API key\n",
    "    api_key = input(\"Please enter your OpenAI API key: \")\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = JobScraper(api_key)\n",
    "    \n",
    "    # Get search parameters\n",
    "    job_title = input(\"Enter job title to search for (e.g., 'software engineer'): \")\n",
    "    location = input(\"Enter location (e.g., 'San Francisco, CA'): \")\n",
    "    num_pages = int(input(\"Enter number of pages to scrape (1-5 recommended): \"))\n",
    "    source = input(\"Enter job source to scrape (indeed/linkedin/both): \").lower()\n",
    "    \n",
    "    all_jobs = pd.DataFrame()\n",
    "    \n",
    "    # Scrape Indeed\n",
    "    if source in ['indeed', 'both']:\n",
    "        print(\"\\nScraping Indeed jobs...\")\n",
    "        indeed_jobs = scraper.scrape_indeed(job_title, location, num_pages)\n",
    "        if not indeed_jobs.empty:\n",
    "            indeed_jobs['source'] = 'Indeed'\n",
    "            all_jobs = pd.concat([all_jobs, indeed_jobs])\n",
    "            print(f\"Found {len(indeed_jobs)} jobs on Indeed\")\n",
    "    \n",
    "    # Scrape LinkedIn\n",
    "    if source in ['linkedin', 'both']:\n",
    "        print(\"\\nScraping LinkedIn jobs...\")\n",
    "        linkedin_jobs = scraper.scrape_linkedin(job_title, location, num_pages)\n",
    "        if not linkedin_jobs.empty:\n",
    "            linkedin_jobs['source'] = 'LinkedIn'\n",
    "            all_jobs = pd.concat([all_jobs, linkedin_jobs])\n",
    "            print(f\"Found {len(linkedin_jobs)} jobs on LinkedIn\")\n",
    "    \n",
    "    # Save results\n",
    "    if not all_jobs.empty:\n",
    "        print(f\"\\nTotal jobs found: {len(all_jobs)}\")\n",
    "        save_format = input(\"Save as CSV or Excel? (csv/excel): \").lower()\n",
    "        \n",
    "        if save_format == 'csv':\n",
    "            filename = input(\"Enter filename (default: job_data.csv): \") or \"job_data.csv\"\n",
    "            scraper.save_to_csv(all_jobs, filename)\n",
    "        else:\n",
    "            filename = input(\"Enter filename (default: job_data.xlsx): \") or \"job_data.xlsx\"\n",
    "            scraper.save_to_excel(all_jobs, filename)\n",
    "            \n",
    "        print(\"\\nJob data preview:\")\n",
    "        print(all_jobs[['job_title', 'company', 'location', 'url']].head())\n",
    "    else:\n",
    "        print(\"No jobs found matching your criteria.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d97333-57c8-4d24-90f7-f23aeaeb66e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>preferred_qualifications</th>\n",
       "      <th>minimum_qualifications</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Growth Data Scientist</td>\n",
       "      <td>Notion</td>\n",
       "      <td>Notion (in person company)</td>\n",
       "      <td>['Experience working on a growth team', 'Exper...</td>\n",
       "      <td>['Meaningful experience as a data scientist pa...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-scient...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product Data Scientist</td>\n",
       "      <td>Notion</td>\n",
       "      <td>San Francisco or New York City</td>\n",
       "      <td>['Expertise in SQL and at least one scripting ...</td>\n",
       "      <td>['Meaningful experience as a data scientist pa...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-scient...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Product Analytics</td>\n",
       "      <td>Brex</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>['Experience working in B2B SaaS or fintech', ...</td>\n",
       "      <td>['Master’s degree or Ph.D. in Statistics, Comp...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-scient...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist II, Credit</td>\n",
       "      <td>Brex</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>['Experience working in B2B SaaS or fintech, p...</td>\n",
       "      <td>['Master’s degree or Ph.D. in Statistics, Econ...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-scient...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI/ML Engineer</td>\n",
       "      <td>Ikigai</td>\n",
       "      <td>Ikigai Labs</td>\n",
       "      <td>[\"Master's degree in Computer Science, Math, E...</td>\n",
       "      <td>['Bachelor’s degree in Computer Science, Math,...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/machine-lea...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>ML Solutions Engineer</td>\n",
       "      <td>SuperAnnotate</td>\n",
       "      <td>Partial remote working</td>\n",
       "      <td>[\"Master's degree in Computer Science, Machine...</td>\n",
       "      <td>['1+ years of customer-facing experience', 'Fa...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/ml-solution...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Data Scientist - Financial Forecasting</td>\n",
       "      <td>Brex</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>['Experience building and maintaining financia...</td>\n",
       "      <td>[\"Master's degree or Ph.D. in Finance, Statist...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Autonomy Pipeline Designer and Builder</td>\n",
       "      <td>Weave Robotics</td>\n",
       "      <td>Weave</td>\n",
       "      <td>['Experience in vision language models, reinfo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/ml-research...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Pinterest</td>\n",
       "      <td>US based applicants only</td>\n",
       "      <td>['Experience using machine learning and deep l...</td>\n",
       "      <td>['4+ years of experience analyzing data in a f...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-scient...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Machine Learning Research Intern</td>\n",
       "      <td>Cognitiv</td>\n",
       "      <td>San Mateo, CA</td>\n",
       "      <td>['Experience with LLMs and fine-tuning transfo...</td>\n",
       "      <td>['Pursuing or recently completed a Master’s or...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/machine-lea...</td>\n",
       "      <td>LinkedIn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  job_title         company  \\\n",
       "0                     Growth Data Scientist          Notion   \n",
       "1                    Product Data Scientist          Notion   \n",
       "2        Data Scientist - Product Analytics            Brex   \n",
       "3                 Data Scientist II, Credit            Brex   \n",
       "4                            AI/ML Engineer          Ikigai   \n",
       "..                                      ...             ...   \n",
       "175                   ML Solutions Engineer   SuperAnnotate   \n",
       "176  Data Scientist - Financial Forecasting            Brex   \n",
       "177  Autonomy Pipeline Designer and Builder  Weave Robotics   \n",
       "178                          Data Scientist       Pinterest   \n",
       "179        Machine Learning Research Intern        Cognitiv   \n",
       "\n",
       "                           location  \\\n",
       "0        Notion (in person company)   \n",
       "1    San Francisco or New York City   \n",
       "2                     San Francisco   \n",
       "3                     San Francisco   \n",
       "4                       Ikigai Labs   \n",
       "..                              ...   \n",
       "175          Partial remote working   \n",
       "176                   San Francisco   \n",
       "177                           Weave   \n",
       "178        US based applicants only   \n",
       "179                   San Mateo, CA   \n",
       "\n",
       "                              preferred_qualifications  \\\n",
       "0    ['Experience working on a growth team', 'Exper...   \n",
       "1    ['Expertise in SQL and at least one scripting ...   \n",
       "2    ['Experience working in B2B SaaS or fintech', ...   \n",
       "3    ['Experience working in B2B SaaS or fintech, p...   \n",
       "4    [\"Master's degree in Computer Science, Math, E...   \n",
       "..                                                 ...   \n",
       "175  [\"Master's degree in Computer Science, Machine...   \n",
       "176  ['Experience building and maintaining financia...   \n",
       "177  ['Experience in vision language models, reinfo...   \n",
       "178  ['Experience using machine learning and deep l...   \n",
       "179  ['Experience with LLMs and fine-tuning transfo...   \n",
       "\n",
       "                                minimum_qualifications  \\\n",
       "0    ['Meaningful experience as a data scientist pa...   \n",
       "1    ['Meaningful experience as a data scientist pa...   \n",
       "2    ['Master’s degree or Ph.D. in Statistics, Comp...   \n",
       "3    ['Master’s degree or Ph.D. in Statistics, Econ...   \n",
       "4    ['Bachelor’s degree in Computer Science, Math,...   \n",
       "..                                                 ...   \n",
       "175  ['1+ years of customer-facing experience', 'Fa...   \n",
       "176  [\"Master's degree or Ph.D. in Finance, Statist...   \n",
       "177                                                 []   \n",
       "178  ['4+ years of experience analyzing data in a f...   \n",
       "179  ['Pursuing or recently completed a Master’s or...   \n",
       "\n",
       "                                                   url    source  \n",
       "0    https://www.linkedin.com/jobs/view/data-scient...  LinkedIn  \n",
       "1    https://www.linkedin.com/jobs/view/data-scient...  LinkedIn  \n",
       "2    https://www.linkedin.com/jobs/view/data-scient...  LinkedIn  \n",
       "3    https://www.linkedin.com/jobs/view/data-scient...  LinkedIn  \n",
       "4    https://www.linkedin.com/jobs/view/machine-lea...  LinkedIn  \n",
       "..                                                 ...       ...  \n",
       "175  https://www.linkedin.com/jobs/view/ml-solution...  LinkedIn  \n",
       "176  https://www.linkedin.com/jobs/view/senior-data...  LinkedIn  \n",
       "177  https://www.linkedin.com/jobs/view/ml-research...  LinkedIn  \n",
       "178  https://www.linkedin.com/jobs/view/data-scient...  LinkedIn  \n",
       "179  https://www.linkedin.com/jobs/view/machine-lea...  LinkedIn  \n",
       "\n",
       "[180 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df=pd.read_csv('Data_Scientist.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519bfa3-9555-49f3-b854-921a879686a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
